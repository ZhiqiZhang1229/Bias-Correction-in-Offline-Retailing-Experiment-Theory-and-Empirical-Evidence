{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27192f",
   "metadata": {
    "id": "6c27192f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14efbb6",
   "metadata": {
    "id": "f14efbb6"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56819b49-b3bb-4d32-ad88-a458e7ee142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ff42b4-0dd7-4555-bfbc-c1d6a7c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Product = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42efd20-8c8f-4675-aa1a-9a2534ee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_percentage = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce9a174-68e6-47f3-ad2f-966cc2698e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc24047e-fa23-40ae-b761-5f0d1fbe595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338e9f4b-a5a7-47a9-b06d-5216d077594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083308b",
   "metadata": {
    "id": "9083308b"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "USER_Cont_FEATURES = 2*user_continuous_feature_multiplier\n",
    "USER_Dicr_FEATURES = 3\n",
    "\n",
    "Product_Cont_FEATURES = 3*prod_continuous_feature_multiplier\n",
    "Product_Dicr_FEATURES = 2\n",
    "OUTSIDE_OPTION_UTILITY = 0\n",
    "utilities = torch.zeros(NUM_USER, NUM_Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442d9dd4",
   "metadata": {
    "id": "442d9dd4"
   },
   "outputs": [],
   "source": [
    "def generate_features(N, C, D):\n",
    "    continuous_features = np.zeros((N, C))\n",
    "    for i in range(C):\n",
    "        continuous_features[:, i] = np.random.uniform(0,1,size=N)\n",
    "    binary_features = np.random.randint(0, 2, (N, D))\n",
    "    return np.hstack((continuous_features, binary_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb84b940",
   "metadata": {
    "id": "cb84b940"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class UtilityDNN(nn.Module):\n",
    "    def __init__(self, user_features, product_features):\n",
    "        super(UtilityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features + product_features, 1)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.uniform_(self.fc1.weight, a=-0.0, b=0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class PriceSensitivityDNN(nn.Module):\n",
    "    def __init__(self, user_features):\n",
    "        super(PriceSensitivityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features,1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "\n",
    "        nn.init.constant_(self.fc1.weight, 0)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.weight, 0)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.abs(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e26f6da",
   "metadata": {
    "id": "0e26f6da"
   },
   "outputs": [],
   "source": [
    "def utility_model(x_user, X_product, price, user_randomization, prod_randomization,pair_utility_model,price_sensitivity_model,gumbel_noise):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    \n",
    "\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    for i in range(num_users):\n",
    "\n",
    "        for j in range(num_products):\n",
    "            # Determine if the user and product are in the treatment group\n",
    "            is_user_treated = (user_randomization[i] == 1)\n",
    "            is_product_treated = (prod_randomization[j] == 1)\n",
    "\n",
    "            # Adjust price based on the experiment conditions\n",
    "            adjusted_price = price[j] * discount if is_user_treated or is_product_treated else price[j]\n",
    "            combined_features = torch.cat((x_user[i], X_product[j]), 0)\n",
    "            utility_from_dnn = pair_utility_model(combined_features)\n",
    "            price_effect = price_sensitivities[i] * adjusted_price\n",
    "\n",
    "            utilities[i, j] = utility_from_dnn - price_effect + gumbel_noise[i,j]\n",
    "\n",
    "    return utility_from_dnn,price_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22bf01fa",
   "metadata": {
    "id": "22bf01fa"
   },
   "outputs": [],
   "source": [
    "def make_decision(utilities):\n",
    "    num_users = utilities.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  \n",
    "    for i in range(num_users):\n",
    "        max_utility, chosen_product = torch.max(utilities[i], dim=0)\n",
    "\n",
    "        # Compare the maximum utility with the outside option (utility = 0)\n",
    "        if max_utility <= 0:\n",
    "            decisions[i] = -1 \n",
    "        else:\n",
    "            decisions[i] = chosen_product \n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b9061",
   "metadata": {
    "id": "3a7b9061"
   },
   "outputs": [],
   "source": [
    "def calculate_revenue(decisions, prices):\n",
    "    total_revenue = 0.0\n",
    "\n",
    "    # Iterate over each decision and add the corresponding product price to total revenue\n",
    "    for i, decision in enumerate(decisions):\n",
    "        if decision != -1:  # Check if the decision is not the outside option\n",
    "            total_revenue += prices[decision].item()  # Add the price of the chosen product\n",
    "\n",
    "    return total_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be800ea9",
   "metadata": {
    "id": "be800ea9"
   },
   "outputs": [],
   "source": [
    "X_user = generate_features(NUM_USER,USER_Cont_FEATURES, USER_Dicr_FEATURES)\n",
    "X_product = generate_features(NUM_Product, Product_Cont_FEATURES, Product_Dicr_FEATURES)\n",
    "price = np.random.uniform(0.5 ,1, NUM_Product)\n",
    "\n",
    "X_user = torch.from_numpy(X_user).float()\n",
    "X_product = torch.from_numpy(X_product).float()\n",
    "price = torch.from_numpy(price).float()\n",
    "gumbel_dist = torch.distributions.Gumbel(0, 1)\n",
    "gumbel_noise = gumbel_dist.sample((NUM_USER, NUM_Product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b15fbba-7cea-4bbe-a6b4-5decb6f40ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9095, 0.9339, 0.9683, 0.5530, 0.5794, 0.5476, 0.6965, 0.8665, 0.8784,\n",
       "        0.7379])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b1ab05",
   "metadata": {
    "id": "13b1ab05"
   },
   "outputs": [],
   "source": [
    "pair_utility_model = UtilityDNN(X_user.shape[1], X_product.shape[1])\n",
    "price_sensitivity_model = PriceSensitivityDNN(X_user.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "590d5d95-5c75-43e6-ae03-eddddbfd3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.1622, 0.2144, 0.4507, 0.2663, 0.1491, 0.0533, 0.0337, 0.1800, 0.1470,\n",
      "         0.1712]])\n",
      "Biases: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = pair_utility_model.fc1.weight.data\n",
    "layer_biases = pair_utility_model.fc1.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbb32ec0-62e7-4760-8bda-c816621ef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_biases = torch.from_numpy(np.array(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "084c7cb6-2ebc-4cf1-92ea-d229b229bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new weights and biases\n",
    "with torch.no_grad():  # Avoid tracking this operation in the computation graph\n",
    "    price_sensitivity_model.fc2.bias.copy_(new_biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e280f46-89ba-422d-9bd1-6dc50dab0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.]])\n",
      "Biases: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = price_sensitivity_model.fc2.weight.data\n",
    "layer_biases = price_sensitivity_model.fc2.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a148e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def utility_model_batched(x_user, X_product, price, user_randomization, prod_randomization, pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  # Initialize decision array\n",
    "\n",
    "    # Convert numpy arrays to tensors if necessary\n",
    "    if isinstance(user_randomization, np.ndarray):\n",
    "        user_randomization = torch.from_numpy(user_randomization).to(torch.bool)\n",
    "    if isinstance(prod_randomization, np.ndarray):\n",
    "        prod_randomization = torch.from_numpy(prod_randomization).to(torch.bool)\n",
    "    if isinstance(price, np.ndarray):\n",
    "        price = torch.from_numpy(price)\n",
    "\n",
    "    # Compute price sensitivities outside the batch loop\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    # Iterate over users in batches\n",
    "    for i in range(0, num_users, batch_size):\n",
    "        batch_end = min(i + batch_size, num_users)  # Define the end of the batch\n",
    "        batch_indices = slice(i, batch_end)  # Slice for batch indexing\n",
    "\n",
    "        # Repeat the product features and price for each user in the batch\n",
    "        batch_user_features = x_user[batch_indices].unsqueeze(1).expand(-1, num_products, -1)\n",
    "        batch_prod_features = X_product.unsqueeze(0).expand(batch_end - i, -1, -1)\n",
    "        batch_price = price.unsqueeze(0).expand(batch_end - i, -1)\n",
    "\n",
    "        # Handle treatment adjustments in batch\n",
    "        batch_user_treatment = user_randomization[batch_indices].unsqueeze(1).expand(-1, num_products) == 1\n",
    "        batch_prod_treatment = prod_randomization.unsqueeze(0).expand(batch_end - i, -1) == 1\n",
    "        batch_adjusted_price = torch.where(batch_user_treatment | batch_prod_treatment, batch_price * discount, batch_price)\n",
    "\n",
    "        # Combine user and product features\n",
    "        combined_features = torch.cat((batch_user_features, batch_prod_features), dim=2)\n",
    "\n",
    "        # Compute utilities using the neural network in a batch\n",
    "        utility_from_dnn = pair_utility_model(combined_features.view(-1, combined_features.shape[-1])).view(batch_end - i, num_products)\n",
    "        price_effect = price_sensitivities[batch_indices] * batch_adjusted_price\n",
    "        batch_utilities = utility_from_dnn - price_effect + gumbel_noise[batch_indices]\n",
    "        max_utilities, chosen_products = torch.max(batch_utilities, dim=1)\n",
    "\n",
    "        # Compare the maximum utility with the outside option (utility = 0)\n",
    "        outside_option = -1 * torch.ones_like(chosen_products, dtype=torch.long)  # Match dimension and dtype\n",
    "        decisions[batch_indices] = torch.where(max_utilities > 0, chosen_products, outside_option)\n",
    "\n",
    "    return decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97381ec",
   "metadata": {
    "id": "c97381ec"
   },
   "source": [
    "# GTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c9652",
   "metadata": {
    "id": "c74c9652"
   },
   "source": [
    "## All treated scenario: all products are discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8620e2f7",
   "metadata": {
    "id": "8620e2f7"
   },
   "outputs": [],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a69429d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a69429d2",
    "outputId": "da77f021-0faf-424d-91be-088437608bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([5, 5, 4,  ..., 1, 2, 7])\n"
     ]
    }
   ],
   "source": [
    "decisions_all_treat=utility_model_batched(X_user, X_product,price, user_randomization, prod_randomization, \n",
    "                                          pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b7df52-fdc0-46a9-ad64-9119957d6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "all_num_unique = torch.unique(decisions_all_treat).numel()\n",
    "print(all_num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8748518-f99d-47ae-a031-7b89fbbfa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(935)\n",
      "tensor(961)\n",
      "tensor(1116)\n",
      "tensor(1082)\n",
      "tensor(1182)\n",
      "tensor(888)\n",
      "tensor(849)\n",
      "tensor(981)\n",
      "tensor(771)\n",
      "tensor(1235)\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1,10):\n",
    "    print(torch.sum(decisions_all_treat==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f59995c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "f59995c9",
    "outputId": "b6b193fc-3ab4-4e76-8096-49dfeae0a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue from sales when all products are discounted: $1525.56\n"
     ]
    }
   ],
   "source": [
    "total_revenue_all_treated = calculate_revenue(decisions_all_treat, price*discount)\n",
    "print(f\"Total revenue from sales when all products are discounted: ${total_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86abbc5",
   "metadata": {},
   "source": [
    "## All control scenario: all products remain the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34e8b6df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "34e8b6df",
    "outputId": "846dbf7a-920f-4c54-d79f-1b527ec86ff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([5, 5, 4,  ..., 1, 6, 7])\n",
      "Total Revenue from Sales: $7430.04\n"
     ]
    }
   ],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "\n",
    "decisions_all_control =utility_model_batched(X_user, X_product,price, user_randomization, prod_randomization, \n",
    "                                          pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_control)\n",
    "total_revenue_all_control = calculate_revenue(decisions_all_control, price)\n",
    "print(f\"Total Revenue from Sales: ${total_revenue_all_control:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e3c0837",
   "metadata": {
    "id": "2e3c0837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (ALLTreated - ALLControl): $-5904.48\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = total_revenue_all_treated - total_revenue_all_control\n",
    "print(f\"Revenue Difference (ALLTreated - ALLControl): ${revenue_difference:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/total_revenue_all_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Sp6U7JOV1vEi",
   "metadata": {
    "id": "Sp6U7JOV1vEi"
   },
   "outputs": [],
   "source": [
    "true = revenue_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec3bca",
   "metadata": {
    "id": "d0ec3bca"
   },
   "source": [
    "## product randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cbbd718",
   "metadata": {
    "id": "4cbbd718"
   },
   "outputs": [],
   "source": [
    "def calculate_product_revenue(decisions, prices, prod_randomization):\n",
    "    revenue_treated = 0.0\n",
    "    revenue_control = 0.0\n",
    "\n",
    "    # Iterate over each user's decision\n",
    "    for user_index, decision in enumerate(decisions):\n",
    "        if decision != -1:  # If the user chose a product\n",
    "            product_price = prices[decision].item()  # Get the price of the chosen product\n",
    "\n",
    "            # Check if the product was in the treatment or control group\n",
    "            if prod_randomization[decision] == 1:\n",
    "                revenue_treated += product_price\n",
    "            else:\n",
    "                revenue_control += product_price\n",
    "\n",
    "    return revenue_treated, revenue_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "402ee3f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402ee3f8",
    "outputId": "17e270ca-c5e5-4e92-fd2f-2c675eb2fbf8"
   },
   "outputs": [],
   "source": [
    "utilities = torch.zeros(NUM_USER, NUM_Product)\n",
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1-treatment_percentage, treatment_percentage])\n",
    "# prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, ])\n",
    "decisions_product_randomization =utility_model_batched(X_user, X_product,price, user_randomization, prod_randomization, \n",
    "                                          pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "022d9af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022d9af1",
    "outputId": "9f338893-e1f9-4c6a-945c-408a2e5b6f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue from Treated Products: $1081.14\n",
      "Revenue from Control Products: $2095.83\n",
      "Revenue Difference (Treated - Control) by naive DIM: $-2029.39\n"
     ]
    }
   ],
   "source": [
    "revenue_treated, revenue_control = calculate_product_revenue(decisions_product_randomization, price-price*(1-discount)*prod_randomization, prod_randomization)\n",
    "naive = revenue_treated/treatment_percentage - revenue_control/(1-treatment_percentage)\n",
    "print(f\"Revenue from Treated Products: ${revenue_treated:.2f}\")\n",
    "print(f\"Revenue from Control Products: ${revenue_control:.2f}\")\n",
    "print(f\"Revenue Difference (Treated - Control) by naive DIM: ${naive:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/revenue_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b9eb",
   "metadata": {},
   "source": [
    "## Prepare training and testing data given experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "wUgBFRaYHK7-",
   "metadata": {
    "id": "wUgBFRaYHK7-"
   },
   "outputs": [],
   "source": [
    "X_user_1, X_user_2, decision_1, decision_2 = train_test_split(\n",
    "X_user, decisions_product_randomization, test_size=1/2, random_state=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bd5975d-53cb-45c6-90e2-c36e313515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\n",
    "    'features': X_user_1,\n",
    "    'labels': decision_1\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    'features': X_user_2,\n",
    "    'labels': decision_2\n",
    "}\n",
    "\n",
    "# Flag to switch between training and test set\n",
    "use_train_set = False  # Set to False for the test set\n",
    "\n",
    "# Function to get the current active dataset\n",
    "def get_active_dataset(use_train):\n",
    "    return train_set if use_train else test_set\n",
    "def get_test_dataset(use_train):\n",
    "    return test_set if use_train else train_set\n",
    "# Retrieve the current dataset based on the flag\n",
    "current_dataset = get_active_dataset(use_train_set)\n",
    "X_user_train = current_dataset['features']\n",
    "decision_train = current_dataset['labels']\n",
    "X_user_test = get_test_dataset(use_train_set)['features']\n",
    "decision_test =  get_test_dataset(use_train_set)['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320887a",
   "metadata": {
    "id": "d320887a"
   },
   "source": [
    "# use simple MNL structural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b14cd096",
   "metadata": {
    "id": "b14cd096"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearMNLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearMNLModel, self).__init__()\n",
    "        # Initialize parameters for user and product features\n",
    "        self.beta_user = nn.Parameter(torch.randn(user_feature_dim))\n",
    "        self.beta_product = nn.Parameter(torch.randn(product_feature_dim))\n",
    "        self.beta_price = nn.Parameter(torch.tensor(-1.0)) \n",
    "\n",
    "    def forward(self, x_user, X_product, price, user_randomization, prod_randomization):\n",
    "        N, M = x_user.shape[0], X_product.shape[0]\n",
    "\n",
    "        # Expand user and product features to create a [N, M, F] shaped tensor for each\n",
    "        x_user_expanded = x_user.unsqueeze(1).expand(-1, M, -1).detach()\n",
    "        X_product_expanded = X_product.unsqueeze(0).expand(N, -1, -1).detach()\n",
    "\n",
    "\n",
    "        # Calculate linear utility from features\n",
    "        utility_user = torch.sum(x_user_expanded * self.beta_user, dim=2)\n",
    "        utility_product = torch.sum(X_product_expanded * self.beta_product, dim=2)\n",
    "\n",
    "        # Adjust prices based on randomization\n",
    "        adjusted_price = torch.where(\n",
    "             prod_randomization.unsqueeze(0),\n",
    "            price * discount,  \n",
    "            price\n",
    "        )\n",
    "\n",
    "        # Calculate utility from price, properly expanding its dimension\n",
    "        utility_price = adjusted_price * self.beta_price  # [M]\n",
    "        utility_price = utility_price.expand(N, M)  # [N, M]\n",
    "\n",
    "        # Total utility including features and price\n",
    "        total_utility = utility_user + utility_product + utility_price\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=total_utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities,total_utility), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d027617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89f614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearMNLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a74e297a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74e297a",
    "outputId": "d9b4de3b-2c2d-452e-b491-1b217ba2aaf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7480978965759277\n",
      "Epoch 1000, Loss: 2.2523162364959717\n",
      "Epoch 2000, Loss: 2.248289108276367\n",
      "Epoch 3000, Loss: 2.2470858097076416\n",
      "Epoch 4000, Loss: 2.246516704559326\n",
      "Epoch 5000, Loss: 2.246188163757324\n",
      "Epoch 6000, Loss: 2.245976209640503\n",
      "Epoch 7000, Loss: 2.245830774307251\n",
      "Epoch 8000, Loss: 2.2457263469696045\n",
      "Epoch 9000, Loss: 2.2456493377685547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    utilities = model(X_user_train, X_product, price, user_randomization, prod_randomization)\n",
    "    choice_probabilities = nn.functional.log_softmax(utilities, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train+1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da3dbeb5-c326-49f2-95d1-948ed3fbb443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.0155,  -2.2636,  -2.9214,  ...,  -2.8097,  -2.2555,  -2.4124],\n",
       "        [-13.4486,  -2.2636,  -2.9213,  ...,  -2.8096,  -2.2554,  -2.4124],\n",
       "        [-11.2641,  -2.2636,  -2.9214,  ...,  -2.8097,  -2.2554,  -2.4124],\n",
       "        ...,\n",
       "        [-22.2856,  -2.2636,  -2.9213,  ...,  -2.8096,  -2.2554,  -2.4124],\n",
       "        [-13.3430,  -2.2636,  -2.9213,  ...,  -2.8096,  -2.2554,  -2.4124],\n",
       "        [-18.0846,  -2.2636,  -2.9213,  ...,  -2.8096,  -2.2554,  -2.4124]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f78d4b21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78d4b21",
    "outputId": "6078e9c0-ce87-4f8e-dd3d-88c3cfb2a616"
   },
   "outputs": [],
   "source": [
    "beta_price_est = model.beta_price.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ac97d98-d381-4a84-82c7-aa6821c3aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0846511\n"
     ]
    }
   ],
   "source": [
    "print(beta_price_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e4df3c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e4df3c1",
    "outputId": "709d1125-67b8-459d-d512-a95764796126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([10.8519,  7.3817,  4.4586,  3.6662,  3.9259], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1783523",
   "metadata": {
    "id": "a1783523"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17eb38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17eb38d3",
    "outputId": "df4ab9ed-52f4-4538-87a5-66932ad5f9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue: $3712.32\n",
      "Expected Revenue: $764.05\n"
     ]
    }
   ],
   "source": [
    "all_product_control = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "all_product_treated = np.random.choice([0,1], NUM_Product, p=[0, 1])\n",
    "all_product_control = torch.from_numpy(all_product_control).to(X_user_train.device).bool()\n",
    "all_product_treated = torch.from_numpy(all_product_treated).to(X_user_train.device).bool()\n",
    "\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_control)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "expected_revenue = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue.item():.2f}\")\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_treated)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)*discount\n",
    "expected_revenue_treated = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue_treated.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "P7Z_BF2C1kdj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7Z_BF2C1kdj",
    "outputId": "6998bb85-85b3-4cf8-da34-4ecded076114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (Treated - Control) by Linear MNL: $-5896.54\n",
      "Absolute Percentage Estimation Error of Linear MNL:  -0.13%\n"
     ]
    }
   ],
   "source": [
    "linear = (expected_revenue_treated-expected_revenue).cpu().detach().numpy()\n",
    "linear = linear*2\n",
    "print(f\"Revenue Difference (Treated - Control) by Linear MNL: ${linear:.2f}\")\n",
    "print(f\"Absolute Percentage Estimation Error of Linear MNL:  {100*np.abs(linear-revenue_difference)/revenue_difference:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309183f7",
   "metadata": {},
   "source": [
    "# Use NMNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c36b1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "707121c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25b8e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNestedMNL(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearNestedMNL, self).__init__()\n",
    "        \n",
    "  \n",
    "        total_feature_dim = user_feature_dim + product_feature_dim + 1 \n",
    "        self.utility_linear = nn.Linear(total_feature_dim, 1)\n",
    "        self.raw_lambda = nn.Parameter(torch.tensor(2.0)) \n",
    "\n",
    "    def forward(self, x_user, x_product, prices):\n",
    " \n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "\n",
    "        prices_expanded = prices.view(1, M, 1).expand(N, -1, -1)\n",
    "        \n",
    "      \n",
    "        combined_features = torch.cat((\n",
    "            x_user.unsqueeze(1).expand(-1, M, -1),       # (N, M, U_dim)\n",
    "            x_product.unsqueeze(0).expand(N, -1, -1),    # (N, M, P_dim)\n",
    "            prices_expanded                              # (N, M, 1)\n",
    "        ), dim=2)\n",
    "\n",
    "\n",
    "        utilities = self.utility_linear(combined_features).squeeze(-1)\n",
    "\n",
    "       \n",
    "        lam = torch.sigmoid(self.raw_lambda) \n",
    "\n",
    "\n",
    "        scaled_utilities = utilities / lam\n",
    "        inclusive_value_log = torch.logsumexp(scaled_utilities, dim=1, keepdim=True) \n",
    "        \n",
    "        v_buy = lam * inclusive_value_log\n",
    "        v_outside = torch.zeros(N, 1, device=x_user.device)\n",
    "        \n",
    "        # Log Softmax over the two Nests\n",
    "        nest_logits = torch.cat([v_buy, v_outside], dim=1)\n",
    "        nest_log_probs = F.log_softmax(nest_logits, dim=1) \n",
    "        \n",
    "        log_prob_buy_nest = nest_log_probs[:, 0].unsqueeze(1)\n",
    "        log_prob_outside = nest_log_probs[:, 1].unsqueeze(1)\n",
    "\n",
    "        log_prob_item_given_buy = scaled_utilities - inclusive_value_log\n",
    "        final_log_probs_products = log_prob_item_given_buy + log_prob_buy_nest\n",
    "        \n",
    "        # Return: [Log P(Outside), Log P(Prod 1), ..., Log P(Prod M)]\n",
    "        return torch.cat([log_prob_outside, final_log_probs_products], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44ae9fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a4a9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 2.5305, Validation Loss: 2.5223\n",
      "Epoch 100, Training Loss: 2.2731, Validation Loss: 2.2675\n",
      "Epoch 200, Training Loss: 2.2599, Validation Loss: 2.2542\n",
      "Epoch 300, Training Loss: 2.2544, Validation Loss: 2.2491\n",
      "Epoch 400, Training Loss: 2.2517, Validation Loss: 2.2466\n",
      "Epoch 500, Training Loss: 2.2502, Validation Loss: 2.2451\n",
      "Epoch 600, Training Loss: 2.2492, Validation Loss: 2.2442\n",
      "Epoch 700, Training Loss: 2.2486, Validation Loss: 2.2436\n",
      "Epoch 800, Training Loss: 2.2481, Validation Loss: 2.2431\n",
      "Epoch 900, Training Loss: 2.2477, Validation Loss: 2.2428\n",
      "Final Lambda (Nesting Parameter): 0.9620\n",
      "Expected Revenue (Control): $3706.08\n",
      "Expected Revenue (Treated): $763.23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LinearNestedMNL(\n",
    "    user_feature_dim=X_user_train1.shape[1], \n",
    "    product_feature_dim=X_product.shape[1]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    log_probs = model(user_features, product_features, prices)\n",
    "    loss = -torch.mean(log_probs[torch.arange(log_probs.shape[0]), decision_train1 + 1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_log_probs = model(X_user_val, product_features, prices)\n",
    "        val_loss = -torch.mean(val_log_probs[torch.arange(val_log_probs.shape[0]), decision_val + 1])\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0 \n",
    "    else:\n",
    "        patience_counter += 1 \n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at Epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Lambda (Nesting Parameter): {torch.sigmoid(model.raw_lambda).item():.4f}\")\n",
    "\n",
    "\n",
    "def calculate_expected_revenue(model, user_features, product_features, prices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        log_probs = model(user_features, product_features, prices)\n",
    "        probabilities = torch.exp(log_probs)\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device), prices), dim=0)\n",
    "        \n",
    "        total_expected_revenue = (probabilities * price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()\n",
    "\n",
    "\n",
    "X_user_test = X_user_test.to(device)\n",
    "X_product = X_product.to(device)\n",
    "price = price.to(device)\n",
    "\n",
    "\n",
    "user_features_test, product_features_test, prices_control, _ = prepare_data(X_user_test, X_product, price)\n",
    "\n",
    "expected_revenue_control = calculate_expected_revenue(\n",
    "    model, user_features_test, product_features_test, prices_control\n",
    ")\n",
    "print(f\"Expected Revenue (Control): ${expected_revenue_control:.2f}\")\n",
    "\n",
    "all_treated_price = price * discount\n",
    "user_features_test, product_features_test, prices_treated, _ = prepare_data(X_user_test, X_product, all_treated_price)\n",
    "\n",
    "expected_revenue_treated = calculate_expected_revenue(\n",
    "    model, user_features_test, product_features_test, prices_treated\n",
    ")\n",
    "print(f\"Expected Revenue (Treated): ${expected_revenue_treated:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "252c6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference: $-5885.71\n",
      "Absolute Percentage Estimation Error of PDL:  -0.32%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Revenue Difference: ${2*(expected_revenue_treated - expected_revenue_control):.2f}\")\n",
    "nmnl = 2*(expected_revenue_treated - expected_revenue_control)\n",
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(nmnl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e9445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab4aae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2899bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "134acfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39864359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(PDLModel, self).__init__()\n",
    "        # Combined feature dimension includes product features, price, and user features, as well as other products' features and prices\n",
    "        total_feature_dim = user_feature_dim + 2*product_feature_dim + 1  # +1 for price\n",
    "\n",
    "        # Single neural network to process the combined features\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1) \n",
    "        )\n",
    "            # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_user, x_product, x_other_products,prices):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                        x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        aggregated_other_features.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        prices.view(1, -1, 1).expand(N, -1, -1)),\n",
    "                                        dim=2)\n",
    "   \n",
    "\n",
    "        # Compute utility for each combined feature set\n",
    "        utilities = self.network(combined_features).squeeze(-1)\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=utilities.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utilities), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a160035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdlmodel = PDLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1ceb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.4031901359558105, Validation Loss: 2.402212381362915\n",
      "Epoch 2, Training Loss: 2.402205228805542, Validation Loss: 2.4012296199798584\n",
      "Epoch 3, Training Loss: 2.4012317657470703, Validation Loss: 2.400289535522461\n",
      "Epoch 4, Training Loss: 2.400249481201172, Validation Loss: 2.399254083633423\n",
      "Epoch 5, Training Loss: 2.3992366790771484, Validation Loss: 2.3982481956481934\n",
      "Epoch 6, Training Loss: 2.3982439041137695, Validation Loss: 2.3972465991973877\n",
      "Epoch 7, Training Loss: 2.397242546081543, Validation Loss: 2.3961853981018066\n",
      "Epoch 8, Training Loss: 2.396186113357544, Validation Loss: 2.3950695991516113\n",
      "Epoch 9, Training Loss: 2.3950984477996826, Validation Loss: 2.3939170837402344\n",
      "Epoch 10, Training Loss: 2.3939545154571533, Validation Loss: 2.392739772796631\n",
      "Epoch 11, Training Loss: 2.392758369445801, Validation Loss: 2.3914601802825928\n",
      "Epoch 12, Training Loss: 2.3914871215820312, Validation Loss: 2.39019513130188\n",
      "Epoch 13, Training Loss: 2.3901662826538086, Validation Loss: 2.3888094425201416\n",
      "Epoch 14, Training Loss: 2.388805627822876, Validation Loss: 2.387387275695801\n",
      "Epoch 15, Training Loss: 2.387410879135132, Validation Loss: 2.3860485553741455\n",
      "Epoch 16, Training Loss: 2.386054039001465, Validation Loss: 2.384669065475464\n",
      "Epoch 17, Training Loss: 2.384693145751953, Validation Loss: 2.383225679397583\n",
      "Epoch 18, Training Loss: 2.383315324783325, Validation Loss: 2.3818321228027344\n",
      "Epoch 19, Training Loss: 2.381962299346924, Validation Loss: 2.3804986476898193\n",
      "Epoch 20, Training Loss: 2.380624532699585, Validation Loss: 2.379124164581299\n",
      "Epoch 21, Training Loss: 2.3792800903320312, Validation Loss: 2.3777549266815186\n",
      "Epoch 22, Training Loss: 2.377911329269409, Validation Loss: 2.376392126083374\n",
      "Epoch 23, Training Loss: 2.376516819000244, Validation Loss: 2.375001907348633\n",
      "Epoch 24, Training Loss: 2.3751072883605957, Validation Loss: 2.3736178874969482\n",
      "Epoch 25, Training Loss: 2.373671770095825, Validation Loss: 2.3721461296081543\n",
      "Epoch 26, Training Loss: 2.3721940517425537, Validation Loss: 2.3706700801849365\n",
      "Epoch 27, Training Loss: 2.370708703994751, Validation Loss: 2.369443416595459\n",
      "Epoch 28, Training Loss: 2.3693573474884033, Validation Loss: 2.36775803565979\n",
      "Epoch 29, Training Loss: 2.367684841156006, Validation Loss: 2.366269111633301\n",
      "Epoch 30, Training Loss: 2.3662965297698975, Validation Loss: 2.364849805831909\n",
      "Epoch 31, Training Loss: 2.364654064178467, Validation Loss: 2.3635270595550537\n",
      "Epoch 32, Training Loss: 2.363203525543213, Validation Loss: 2.362046718597412\n",
      "Epoch 33, Training Loss: 2.36165714263916, Validation Loss: 2.360396146774292\n",
      "Epoch 34, Training Loss: 2.360016107559204, Validation Loss: 2.3587160110473633\n",
      "Epoch 35, Training Loss: 2.3583757877349854, Validation Loss: 2.357149839401245\n",
      "Epoch 36, Training Loss: 2.356959342956543, Validation Loss: 2.355527877807617\n",
      "Epoch 37, Training Loss: 2.3552305698394775, Validation Loss: 2.354043483734131\n",
      "Epoch 38, Training Loss: 2.3536605834960938, Validation Loss: 2.3525376319885254\n",
      "Epoch 39, Training Loss: 2.3521170616149902, Validation Loss: 2.3509140014648438\n",
      "Epoch 40, Training Loss: 2.350552797317505, Validation Loss: 2.3492484092712402\n",
      "Epoch 41, Training Loss: 2.3489718437194824, Validation Loss: 2.347618579864502\n",
      "Epoch 42, Training Loss: 2.347444534301758, Validation Loss: 2.3460781574249268\n",
      "Epoch 43, Training Loss: 2.345933198928833, Validation Loss: 2.3446056842803955\n",
      "Epoch 44, Training Loss: 2.344395399093628, Validation Loss: 2.3434526920318604\n",
      "Epoch 45, Training Loss: 2.3429274559020996, Validation Loss: 2.341946840286255\n",
      "Epoch 46, Training Loss: 2.3414382934570312, Validation Loss: 2.340355157852173\n",
      "Epoch 47, Training Loss: 2.3399763107299805, Validation Loss: 2.3389298915863037\n",
      "Epoch 48, Training Loss: 2.3385560512542725, Validation Loss: 2.3377106189727783\n",
      "Epoch 49, Training Loss: 2.3371427059173584, Validation Loss: 2.3367412090301514\n",
      "Epoch 50, Training Loss: 2.33579683303833, Validation Loss: 2.3351845741271973\n",
      "Epoch 51, Training Loss: 2.334425926208496, Validation Loss: 2.3339624404907227\n",
      "Epoch 52, Training Loss: 2.3331215381622314, Validation Loss: 2.3329339027404785\n",
      "Epoch 53, Training Loss: 2.331827402114868, Validation Loss: 2.3320181369781494\n",
      "Epoch 54, Training Loss: 2.330587148666382, Validation Loss: 2.330911159515381\n",
      "Epoch 55, Training Loss: 2.3293776512145996, Validation Loss: 2.3299477100372314\n",
      "Epoch 56, Training Loss: 2.328193187713623, Validation Loss: 2.328947067260742\n",
      "Epoch 57, Training Loss: 2.3269951343536377, Validation Loss: 2.328026533126831\n",
      "Epoch 58, Training Loss: 2.325840473175049, Validation Loss: 2.3262012004852295\n",
      "Epoch 59, Training Loss: 2.324643611907959, Validation Loss: 2.3246254920959473\n",
      "Epoch 60, Training Loss: 2.3236446380615234, Validation Loss: 2.326704978942871\n",
      "Epoch 61, Training Loss: 2.323110342025757, Validation Loss: 2.3230292797088623\n",
      "Epoch 62, Training Loss: 2.3213050365448, Validation Loss: 2.3218307495117188\n",
      "Epoch 63, Training Loss: 2.3215274810791016, Validation Loss: 2.3210573196411133\n",
      "Epoch 64, Training Loss: 2.319556951522827, Validation Loss: 2.3267393112182617\n",
      "Epoch 65, Training Loss: 2.32086443901062, Validation Loss: 2.3193612098693848\n",
      "Epoch 66, Training Loss: 2.3182127475738525, Validation Loss: 2.319021701812744\n",
      "Epoch 67, Training Loss: 2.319169282913208, Validation Loss: 2.3181519508361816\n",
      "Epoch 68, Training Loss: 2.3180925846099854, Validation Loss: 2.318112850189209\n",
      "Epoch 69, Training Loss: 2.3167293071746826, Validation Loss: 2.319232940673828\n",
      "Epoch 70, Training Loss: 2.316852569580078, Validation Loss: 2.3187379837036133\n",
      "Epoch 71, Training Loss: 2.3163082599639893, Validation Loss: 2.3166897296905518\n",
      "Epoch 72, Training Loss: 2.3151142597198486, Validation Loss: 2.314932346343994\n",
      "Epoch 73, Training Loss: 2.314640760421753, Validation Loss: 2.314607858657837\n",
      "Epoch 74, Training Loss: 2.314518451690674, Validation Loss: 2.313839912414551\n",
      "Epoch 75, Training Loss: 2.313382863998413, Validation Loss: 2.3150522708892822\n",
      "Epoch 76, Training Loss: 2.312479257583618, Validation Loss: 2.3151957988739014\n",
      "Epoch 77, Training Loss: 2.3120148181915283, Validation Loss: 2.3166627883911133\n",
      "Epoch 78, Training Loss: 2.3116295337677, Validation Loss: 2.313267946243286\n",
      "Epoch 79, Training Loss: 2.310224771499634, Validation Loss: 2.3116071224212646\n",
      "Epoch 80, Training Loss: 2.3101651668548584, Validation Loss: 2.312609910964966\n",
      "Epoch 81, Training Loss: 2.3095972537994385, Validation Loss: 2.3125739097595215\n",
      "Epoch 82, Training Loss: 2.3089683055877686, Validation Loss: 2.3115358352661133\n",
      "Epoch 83, Training Loss: 2.3080482482910156, Validation Loss: 2.3109889030456543\n",
      "Epoch 84, Training Loss: 2.3075239658355713, Validation Loss: 2.3111259937286377\n",
      "Epoch 85, Training Loss: 2.3072593212127686, Validation Loss: 2.310574531555176\n",
      "Epoch 86, Training Loss: 2.306680917739868, Validation Loss: 2.3094160556793213\n",
      "Epoch 87, Training Loss: 2.305915117263794, Validation Loss: 2.309030055999756\n",
      "Epoch 88, Training Loss: 2.3054397106170654, Validation Loss: 2.3092031478881836\n",
      "Epoch 89, Training Loss: 2.305119276046753, Validation Loss: 2.3083882331848145\n",
      "Epoch 90, Training Loss: 2.3045847415924072, Validation Loss: 2.307499647140503\n",
      "Epoch 91, Training Loss: 2.3040218353271484, Validation Loss: 2.3075454235076904\n",
      "Epoch 92, Training Loss: 2.303480386734009, Validation Loss: 2.3070068359375\n",
      "Epoch 93, Training Loss: 2.303081512451172, Validation Loss: 2.306163787841797\n",
      "Epoch 94, Training Loss: 2.302612543106079, Validation Loss: 2.305462121963501\n",
      "Epoch 95, Training Loss: 2.3021316528320312, Validation Loss: 2.305220603942871\n",
      "Epoch 96, Training Loss: 2.3015856742858887, Validation Loss: 2.3054349422454834\n",
      "Epoch 97, Training Loss: 2.3012335300445557, Validation Loss: 2.3043973445892334\n",
      "Epoch 98, Training Loss: 2.300705909729004, Validation Loss: 2.303856372833252\n",
      "Epoch 99, Training Loss: 2.3003008365631104, Validation Loss: 2.3037397861480713\n",
      "Epoch 100, Training Loss: 2.299872398376465, Validation Loss: 2.3029725551605225\n",
      "Epoch 101, Training Loss: 2.29939603805542, Validation Loss: 2.3025290966033936\n",
      "Epoch 102, Training Loss: 2.2989988327026367, Validation Loss: 2.302208662033081\n",
      "Epoch 103, Training Loss: 2.2985873222351074, Validation Loss: 2.302021026611328\n",
      "Epoch 104, Training Loss: 2.298227071762085, Validation Loss: 2.30086350440979\n",
      "Epoch 105, Training Loss: 2.2978005409240723, Validation Loss: 2.300478219985962\n",
      "Epoch 106, Training Loss: 2.297391891479492, Validation Loss: 2.300598382949829\n",
      "Epoch 107, Training Loss: 2.297011137008667, Validation Loss: 2.300244092941284\n",
      "Epoch 108, Training Loss: 2.2966020107269287, Validation Loss: 2.2994205951690674\n",
      "Epoch 109, Training Loss: 2.2962825298309326, Validation Loss: 2.2992238998413086\n",
      "Epoch 110, Training Loss: 2.295781373977661, Validation Loss: 2.2992372512817383\n",
      "Epoch 111, Training Loss: 2.295516014099121, Validation Loss: 2.2981960773468018\n",
      "Epoch 112, Training Loss: 2.2950799465179443, Validation Loss: 2.2978148460388184\n",
      "Epoch 113, Training Loss: 2.2946975231170654, Validation Loss: 2.298039674758911\n",
      "Epoch 114, Training Loss: 2.2944014072418213, Validation Loss: 2.297093152999878\n",
      "Epoch 115, Training Loss: 2.2939746379852295, Validation Loss: 2.296741247177124\n",
      "Epoch 116, Training Loss: 2.293642282485962, Validation Loss: 2.296947479248047\n",
      "Epoch 117, Training Loss: 2.2932698726654053, Validation Loss: 2.2964677810668945\n",
      "Epoch 118, Training Loss: 2.2928788661956787, Validation Loss: 2.296128034591675\n",
      "Epoch 119, Training Loss: 2.292541742324829, Validation Loss: 2.296069383621216\n",
      "Epoch 120, Training Loss: 2.292203903198242, Validation Loss: 2.2955715656280518\n",
      "Epoch 121, Training Loss: 2.2918481826782227, Validation Loss: 2.295036792755127\n",
      "Epoch 122, Training Loss: 2.291520118713379, Validation Loss: 2.2948906421661377\n",
      "Epoch 123, Training Loss: 2.2912120819091797, Validation Loss: 2.2942726612091064\n",
      "Epoch 124, Training Loss: 2.290874719619751, Validation Loss: 2.2942867279052734\n",
      "Epoch 125, Training Loss: 2.2905266284942627, Validation Loss: 2.293940544128418\n",
      "Epoch 126, Training Loss: 2.290203094482422, Validation Loss: 2.293820858001709\n",
      "Epoch 127, Training Loss: 2.28987979888916, Validation Loss: 2.293649435043335\n",
      "Epoch 128, Training Loss: 2.28956937789917, Validation Loss: 2.2935352325439453\n",
      "Epoch 129, Training Loss: 2.28926682472229, Validation Loss: 2.2927794456481934\n",
      "Epoch 130, Training Loss: 2.2889838218688965, Validation Loss: 2.292855978012085\n",
      "Epoch 131, Training Loss: 2.288717269897461, Validation Loss: 2.2920830249786377\n",
      "Epoch 132, Training Loss: 2.2883734703063965, Validation Loss: 2.292236328125\n",
      "Epoch 133, Training Loss: 2.2880611419677734, Validation Loss: 2.291996717453003\n",
      "Epoch 134, Training Loss: 2.2877554893493652, Validation Loss: 2.291879892349243\n",
      "Epoch 135, Training Loss: 2.2874631881713867, Validation Loss: 2.2917544841766357\n",
      "Epoch 136, Training Loss: 2.287179708480835, Validation Loss: 2.291257381439209\n",
      "Epoch 137, Training Loss: 2.286914110183716, Validation Loss: 2.2914249897003174\n",
      "Epoch 138, Training Loss: 2.2866647243499756, Validation Loss: 2.290703773498535\n",
      "Epoch 139, Training Loss: 2.286332130432129, Validation Loss: 2.290064573287964\n",
      "Epoch 140, Training Loss: 2.286102771759033, Validation Loss: 2.2906956672668457\n",
      "Epoch 141, Training Loss: 2.2859046459198, Validation Loss: 2.2898764610290527\n",
      "Epoch 142, Training Loss: 2.285494089126587, Validation Loss: 2.2894937992095947\n",
      "Epoch 143, Training Loss: 2.285322427749634, Validation Loss: 2.2902626991271973\n",
      "Epoch 144, Training Loss: 2.2850594520568848, Validation Loss: 2.2896740436553955\n",
      "Epoch 145, Training Loss: 2.284700870513916, Validation Loss: 2.288966417312622\n",
      "Epoch 146, Training Loss: 2.284630298614502, Validation Loss: 2.289397716522217\n",
      "Epoch 147, Training Loss: 2.2841956615448, Validation Loss: 2.2892351150512695\n",
      "Epoch 148, Training Loss: 2.283959150314331, Validation Loss: 2.2883048057556152\n",
      "Epoch 149, Training Loss: 2.2837436199188232, Validation Loss: 2.2884998321533203\n",
      "Epoch 150, Training Loss: 2.2834155559539795, Validation Loss: 2.288630962371826\n",
      "Epoch 151, Training Loss: 2.2832024097442627, Validation Loss: 2.287877082824707\n",
      "Epoch 152, Training Loss: 2.282926559448242, Validation Loss: 2.287734031677246\n",
      "Epoch 153, Training Loss: 2.282686710357666, Validation Loss: 2.2880442142486572\n",
      "Epoch 154, Training Loss: 2.282463550567627, Validation Loss: 2.287320852279663\n",
      "Epoch 155, Training Loss: 2.2821784019470215, Validation Loss: 2.2866616249084473\n",
      "Epoch 156, Training Loss: 2.2820212841033936, Validation Loss: 2.287351608276367\n",
      "Epoch 157, Training Loss: 2.281792640686035, Validation Loss: 2.2868144512176514\n",
      "Epoch 158, Training Loss: 2.2814748287200928, Validation Loss: 2.2861897945404053\n",
      "Epoch 159, Training Loss: 2.2814276218414307, Validation Loss: 2.2871413230895996\n",
      "Epoch 160, Training Loss: 2.281090259552002, Validation Loss: 2.287012815475464\n",
      "Epoch 161, Training Loss: 2.2808620929718018, Validation Loss: 2.2858500480651855\n",
      "Epoch 162, Training Loss: 2.2807090282440186, Validation Loss: 2.285781145095825\n",
      "Epoch 163, Training Loss: 2.2803499698638916, Validation Loss: 2.28619384765625\n",
      "Epoch 164, Training Loss: 2.280244827270508, Validation Loss: 2.2850444316864014\n",
      "Epoch 165, Training Loss: 2.279914617538452, Validation Loss: 2.2848782539367676\n",
      "Epoch 166, Training Loss: 2.2797110080718994, Validation Loss: 2.2858829498291016\n",
      "Epoch 167, Training Loss: 2.279531240463257, Validation Loss: 2.2852776050567627\n",
      "Epoch 168, Training Loss: 2.279175281524658, Validation Loss: 2.2847886085510254\n",
      "Epoch 169, Training Loss: 2.279083490371704, Validation Loss: 2.285865545272827\n",
      "Epoch 170, Training Loss: 2.2789316177368164, Validation Loss: 2.2848267555236816\n",
      "Epoch 171, Training Loss: 2.2785754203796387, Validation Loss: 2.284083604812622\n",
      "Epoch 172, Training Loss: 2.2784790992736816, Validation Loss: 2.2845675945281982\n",
      "Epoch 173, Training Loss: 2.2782061100006104, Validation Loss: 2.2839956283569336\n",
      "Epoch 174, Training Loss: 2.277952194213867, Validation Loss: 2.2829761505126953\n",
      "Epoch 175, Training Loss: 2.2779054641723633, Validation Loss: 2.2836217880249023\n",
      "Epoch 176, Training Loss: 2.2775659561157227, Validation Loss: 2.2837648391723633\n",
      "Epoch 177, Training Loss: 2.2774319648742676, Validation Loss: 2.282686233520508\n",
      "Epoch 178, Training Loss: 2.277280807495117, Validation Loss: 2.282938003540039\n",
      "Epoch 179, Training Loss: 2.276982307434082, Validation Loss: 2.2837212085723877\n",
      "Epoch 180, Training Loss: 2.2769808769226074, Validation Loss: 2.2821731567382812\n",
      "Epoch 181, Training Loss: 2.2766993045806885, Validation Loss: 2.282125234603882\n",
      "Epoch 182, Training Loss: 2.2764317989349365, Validation Loss: 2.2832512855529785\n",
      "Epoch 183, Training Loss: 2.276482343673706, Validation Loss: 2.281632900238037\n",
      "Epoch 184, Training Loss: 2.2761189937591553, Validation Loss: 2.281802177429199\n",
      "Epoch 185, Training Loss: 2.2758419513702393, Validation Loss: 2.2825992107391357\n",
      "Epoch 186, Training Loss: 2.275759220123291, Validation Loss: 2.2817251682281494\n",
      "Epoch 187, Training Loss: 2.2754452228546143, Validation Loss: 2.28102707862854\n",
      "Epoch 188, Training Loss: 2.2754111289978027, Validation Loss: 2.281554698944092\n",
      "Epoch 189, Training Loss: 2.2751176357269287, Validation Loss: 2.281707525253296\n",
      "Epoch 190, Training Loss: 2.275029420852661, Validation Loss: 2.2805469036102295\n",
      "Epoch 191, Training Loss: 2.274872303009033, Validation Loss: 2.280731201171875\n",
      "Epoch 192, Training Loss: 2.2745773792266846, Validation Loss: 2.2813332080841064\n",
      "Epoch 193, Training Loss: 2.2744996547698975, Validation Loss: 2.2804760932922363\n",
      "Epoch 194, Training Loss: 2.2742292881011963, Validation Loss: 2.2798962593078613\n",
      "Epoch 195, Training Loss: 2.274127960205078, Validation Loss: 2.2803308963775635\n",
      "Epoch 196, Training Loss: 2.2739102840423584, Validation Loss: 2.2801713943481445\n",
      "Epoch 197, Training Loss: 2.2737205028533936, Validation Loss: 2.2799041271209717\n",
      "Epoch 198, Training Loss: 2.273648738861084, Validation Loss: 2.2802717685699463\n",
      "Epoch 199, Training Loss: 2.2734463214874268, Validation Loss: 2.2797691822052\n",
      "Epoch 200, Training Loss: 2.273218870162964, Validation Loss: 2.2791054248809814\n",
      "Epoch 201, Training Loss: 2.27311110496521, Validation Loss: 2.279449224472046\n",
      "Epoch 202, Training Loss: 2.272958755493164, Validation Loss: 2.2791225910186768\n",
      "Epoch 203, Training Loss: 2.272768497467041, Validation Loss: 2.278416156768799\n",
      "Epoch 204, Training Loss: 2.272674560546875, Validation Loss: 2.2789182662963867\n",
      "Epoch 205, Training Loss: 2.272437334060669, Validation Loss: 2.2796518802642822\n",
      "Epoch 206, Training Loss: 2.2724878787994385, Validation Loss: 2.2785489559173584\n",
      "Epoch 207, Training Loss: 2.2722513675689697, Validation Loss: 2.2781543731689453\n",
      "Epoch 208, Training Loss: 2.2720603942871094, Validation Loss: 2.2789225578308105\n",
      "Epoch 209, Training Loss: 2.2720816135406494, Validation Loss: 2.277834177017212\n",
      "Epoch 210, Training Loss: 2.2717392444610596, Validation Loss: 2.277250051498413\n",
      "Epoch 211, Training Loss: 2.2717037200927734, Validation Loss: 2.278534173965454\n",
      "Epoch 212, Training Loss: 2.271493673324585, Validation Loss: 2.278480052947998\n",
      "Epoch 213, Training Loss: 2.271341323852539, Validation Loss: 2.277461051940918\n",
      "Epoch 214, Training Loss: 2.27128529548645, Validation Loss: 2.27775502204895\n",
      "Epoch 215, Training Loss: 2.2709543704986572, Validation Loss: 2.27801775932312\n",
      "Epoch 216, Training Loss: 2.2709405422210693, Validation Loss: 2.2770919799804688\n",
      "Epoch 217, Training Loss: 2.2707173824310303, Validation Loss: 2.276671886444092\n",
      "Epoch 218, Training Loss: 2.2707321643829346, Validation Loss: 2.27742600440979\n",
      "Epoch 219, Training Loss: 2.270439386367798, Validation Loss: 2.2776284217834473\n",
      "Epoch 220, Training Loss: 2.270376682281494, Validation Loss: 2.276298999786377\n",
      "Epoch 221, Training Loss: 2.2703545093536377, Validation Loss: 2.276766777038574\n",
      "Epoch 222, Training Loss: 2.2700212001800537, Validation Loss: 2.276609182357788\n",
      "Epoch 223, Training Loss: 2.269941568374634, Validation Loss: 2.275831937789917\n",
      "Epoch 224, Training Loss: 2.269801378250122, Validation Loss: 2.2758798599243164\n",
      "Epoch 225, Training Loss: 2.2696075439453125, Validation Loss: 2.276578664779663\n",
      "Epoch 226, Training Loss: 2.2694554328918457, Validation Loss: 2.2762467861175537\n",
      "Epoch 227, Training Loss: 2.2693545818328857, Validation Loss: 2.275866985321045\n",
      "Epoch 228, Training Loss: 2.2691311836242676, Validation Loss: 2.2755537033081055\n",
      "Epoch 229, Training Loss: 2.2690317630767822, Validation Loss: 2.274782180786133\n",
      "Epoch 230, Training Loss: 2.268932819366455, Validation Loss: 2.2745840549468994\n",
      "Epoch 231, Training Loss: 2.268777370452881, Validation Loss: 2.2751028537750244\n",
      "Epoch 232, Training Loss: 2.2685658931732178, Validation Loss: 2.275226593017578\n",
      "Epoch 233, Training Loss: 2.268467903137207, Validation Loss: 2.2751944065093994\n",
      "Epoch 234, Training Loss: 2.2683327198028564, Validation Loss: 2.275067090988159\n",
      "Epoch 235, Training Loss: 2.2681329250335693, Validation Loss: 2.2743866443634033\n",
      "Epoch 236, Training Loss: 2.268000841140747, Validation Loss: 2.2744293212890625\n",
      "Epoch 237, Training Loss: 2.267831563949585, Validation Loss: 2.2744312286376953\n",
      "Epoch 238, Training Loss: 2.2676351070404053, Validation Loss: 2.2741665840148926\n",
      "Epoch 239, Training Loss: 2.2674663066864014, Validation Loss: 2.2740612030029297\n",
      "Epoch 240, Training Loss: 2.267308235168457, Validation Loss: 2.2745115756988525\n",
      "Epoch 241, Training Loss: 2.2671358585357666, Validation Loss: 2.2735915184020996\n",
      "Epoch 242, Training Loss: 2.266981363296509, Validation Loss: 2.27364444732666\n",
      "Epoch 243, Training Loss: 2.2667336463928223, Validation Loss: 2.273716926574707\n",
      "Epoch 244, Training Loss: 2.2665820121765137, Validation Loss: 2.272634267807007\n",
      "Epoch 245, Training Loss: 2.2665154933929443, Validation Loss: 2.2731220722198486\n",
      "Epoch 246, Training Loss: 2.2663137912750244, Validation Loss: 2.272712469100952\n",
      "Epoch 247, Training Loss: 2.266129732131958, Validation Loss: 2.2721338272094727\n",
      "Epoch 248, Training Loss: 2.26599383354187, Validation Loss: 2.2723145484924316\n",
      "Epoch 249, Training Loss: 2.2658019065856934, Validation Loss: 2.2726070880889893\n",
      "Epoch 250, Training Loss: 2.265709638595581, Validation Loss: 2.272271156311035\n",
      "Epoch 251, Training Loss: 2.265561819076538, Validation Loss: 2.2722277641296387\n",
      "Epoch 252, Training Loss: 2.265430450439453, Validation Loss: 2.2722091674804688\n",
      "Epoch 253, Training Loss: 2.2652900218963623, Validation Loss: 2.2718780040740967\n",
      "Epoch 254, Training Loss: 2.26515531539917, Validation Loss: 2.2717857360839844\n",
      "Epoch 255, Training Loss: 2.265044689178467, Validation Loss: 2.2717103958129883\n",
      "Epoch 256, Training Loss: 2.264925479888916, Validation Loss: 2.2714874744415283\n",
      "Epoch 257, Training Loss: 2.264796495437622, Validation Loss: 2.271397113800049\n",
      "Epoch 258, Training Loss: 2.26466965675354, Validation Loss: 2.271599769592285\n",
      "Epoch 259, Training Loss: 2.2645509243011475, Validation Loss: 2.2714834213256836\n",
      "Epoch 260, Training Loss: 2.264446258544922, Validation Loss: 2.271538734436035\n",
      "Epoch 261, Training Loss: 2.264342784881592, Validation Loss: 2.2708730697631836\n",
      "Epoch 262, Training Loss: 2.2642834186553955, Validation Loss: 2.27165150642395\n",
      "Epoch 263, Training Loss: 2.2641913890838623, Validation Loss: 2.2711856365203857\n",
      "Epoch 264, Training Loss: 2.264028549194336, Validation Loss: 2.2707393169403076\n",
      "Epoch 265, Training Loss: 2.26399302482605, Validation Loss: 2.2717151641845703\n",
      "Epoch 266, Training Loss: 2.263946294784546, Validation Loss: 2.2704150676727295\n",
      "Epoch 267, Training Loss: 2.263734817504883, Validation Loss: 2.270207166671753\n",
      "Epoch 268, Training Loss: 2.263659715652466, Validation Loss: 2.271008014678955\n",
      "Epoch 269, Training Loss: 2.2634904384613037, Validation Loss: 2.270848274230957\n",
      "Epoch 270, Training Loss: 2.2633492946624756, Validation Loss: 2.2703869342803955\n",
      "Epoch 271, Training Loss: 2.2633023262023926, Validation Loss: 2.2713468074798584\n",
      "Epoch 272, Training Loss: 2.263200044631958, Validation Loss: 2.2700767517089844\n",
      "Epoch 273, Training Loss: 2.263145923614502, Validation Loss: 2.2706589698791504\n",
      "Epoch 274, Training Loss: 2.2629761695861816, Validation Loss: 2.2707066535949707\n",
      "Epoch 275, Training Loss: 2.2628438472747803, Validation Loss: 2.2696802616119385\n",
      "Epoch 276, Training Loss: 2.262861728668213, Validation Loss: 2.270444393157959\n",
      "Epoch 277, Training Loss: 2.262659788131714, Validation Loss: 2.2701656818389893\n",
      "Epoch 278, Training Loss: 2.262528657913208, Validation Loss: 2.2691650390625\n",
      "Epoch 279, Training Loss: 2.2625107765197754, Validation Loss: 2.2702393531799316\n",
      "Epoch 280, Training Loss: 2.2624151706695557, Validation Loss: 2.27034592628479\n",
      "Epoch 281, Training Loss: 2.262268304824829, Validation Loss: 2.269287109375\n",
      "Epoch 282, Training Loss: 2.262273073196411, Validation Loss: 2.2707760334014893\n",
      "Epoch 283, Training Loss: 2.262251377105713, Validation Loss: 2.2691919803619385\n",
      "Epoch 284, Training Loss: 2.2620441913604736, Validation Loss: 2.269716739654541\n",
      "Epoch 285, Training Loss: 2.2618086338043213, Validation Loss: 2.269981861114502\n",
      "Epoch 286, Training Loss: 2.2617735862731934, Validation Loss: 2.2691359519958496\n",
      "Epoch 287, Training Loss: 2.261653423309326, Validation Loss: 2.2696003913879395\n",
      "Epoch 288, Training Loss: 2.261517286300659, Validation Loss: 2.269768714904785\n",
      "Epoch 289, Training Loss: 2.261457920074463, Validation Loss: 2.268433094024658\n",
      "Epoch 290, Training Loss: 2.2615087032318115, Validation Loss: 2.269998073577881\n",
      "Epoch 291, Training Loss: 2.261410713195801, Validation Loss: 2.2689361572265625\n",
      "Epoch 292, Training Loss: 2.261202096939087, Validation Loss: 2.2689523696899414\n",
      "Epoch 293, Training Loss: 2.261089324951172, Validation Loss: 2.2697715759277344\n",
      "Epoch 294, Training Loss: 2.2610602378845215, Validation Loss: 2.268378496170044\n",
      "Epoch 295, Training Loss: 2.2611565589904785, Validation Loss: 2.269505500793457\n",
      "Epoch 296, Training Loss: 2.2609140872955322, Validation Loss: 2.2685818672180176\n",
      "Epoch 297, Training Loss: 2.260765314102173, Validation Loss: 2.2691094875335693\n",
      "Epoch 298, Training Loss: 2.2607364654541016, Validation Loss: 2.2680118083953857\n",
      "Epoch 299, Training Loss: 2.260650634765625, Validation Loss: 2.2687275409698486\n",
      "Epoch 300, Training Loss: 2.2604801654815674, Validation Loss: 2.2686212062835693\n",
      "Epoch 301, Training Loss: 2.260441780090332, Validation Loss: 2.2688980102539062\n",
      "Epoch 302, Training Loss: 2.260309934616089, Validation Loss: 2.268014430999756\n",
      "Epoch 303, Training Loss: 2.260262966156006, Validation Loss: 2.269591808319092\n",
      "Epoch 304, Training Loss: 2.2603843212127686, Validation Loss: 2.267580509185791\n",
      "Epoch 305, Training Loss: 2.26019024848938, Validation Loss: 2.268270969390869\n",
      "Epoch 306, Training Loss: 2.2600111961364746, Validation Loss: 2.2691843509674072\n",
      "Epoch 307, Training Loss: 2.260068655014038, Validation Loss: 2.267092227935791\n",
      "Epoch 308, Training Loss: 2.2599382400512695, Validation Loss: 2.2686927318573\n",
      "Epoch 309, Training Loss: 2.2599496841430664, Validation Loss: 2.2674005031585693\n",
      "Epoch 310, Training Loss: 2.2596638202667236, Validation Loss: 2.2675940990448\n",
      "Epoch 311, Training Loss: 2.259491443634033, Validation Loss: 2.2677853107452393\n",
      "Epoch 312, Training Loss: 2.259472131729126, Validation Loss: 2.266831159591675\n",
      "Epoch 313, Training Loss: 2.259387254714966, Validation Loss: 2.2678093910217285\n",
      "Epoch 314, Training Loss: 2.259330987930298, Validation Loss: 2.266268253326416\n",
      "Epoch 315, Training Loss: 2.25925612449646, Validation Loss: 2.2681357860565186\n",
      "Epoch 316, Training Loss: 2.259416341781616, Validation Loss: 2.265805959701538\n",
      "Epoch 317, Training Loss: 2.2592287063598633, Validation Loss: 2.2676138877868652\n",
      "Epoch 318, Training Loss: 2.2591185569763184, Validation Loss: 2.2665724754333496\n",
      "Epoch 319, Training Loss: 2.2588725090026855, Validation Loss: 2.2662408351898193\n",
      "Epoch 320, Training Loss: 2.2588088512420654, Validation Loss: 2.267775058746338\n",
      "Epoch 321, Training Loss: 2.258984327316284, Validation Loss: 2.2653589248657227\n",
      "Epoch 322, Training Loss: 2.2589428424835205, Validation Loss: 2.267350673675537\n",
      "Epoch 323, Training Loss: 2.258788824081421, Validation Loss: 2.2661309242248535\n",
      "Epoch 324, Training Loss: 2.258444309234619, Validation Loss: 2.265904188156128\n",
      "Epoch 325, Training Loss: 2.2584123611450195, Validation Loss: 2.2671940326690674\n",
      "Epoch 326, Training Loss: 2.2585039138793945, Validation Loss: 2.265554666519165\n",
      "Epoch 327, Training Loss: 2.2583818435668945, Validation Loss: 2.2667341232299805\n",
      "Epoch 328, Training Loss: 2.258197069168091, Validation Loss: 2.266397476196289\n",
      "Epoch 329, Training Loss: 2.258030414581299, Validation Loss: 2.2660276889801025\n",
      "Epoch 330, Training Loss: 2.2580413818359375, Validation Loss: 2.2672104835510254\n",
      "Epoch 331, Training Loss: 2.2582600116729736, Validation Loss: 2.2650089263916016\n",
      "Epoch 332, Training Loss: 2.2580039501190186, Validation Loss: 2.265625\n",
      "Epoch 333, Training Loss: 2.257840394973755, Validation Loss: 2.2652838230133057\n",
      "Epoch 334, Training Loss: 2.2577126026153564, Validation Loss: 2.2654197216033936\n",
      "Epoch 335, Training Loss: 2.2576417922973633, Validation Loss: 2.265908718109131\n",
      "Epoch 336, Training Loss: 2.257594108581543, Validation Loss: 2.2654874324798584\n",
      "Epoch 337, Training Loss: 2.2574987411499023, Validation Loss: 2.265292167663574\n",
      "Epoch 338, Training Loss: 2.2574737071990967, Validation Loss: 2.2660510540008545\n",
      "Epoch 339, Training Loss: 2.2574634552001953, Validation Loss: 2.2651383876800537\n",
      "Epoch 340, Training Loss: 2.2573788166046143, Validation Loss: 2.265347719192505\n",
      "Epoch 341, Training Loss: 2.2572808265686035, Validation Loss: 2.2656655311584473\n",
      "Epoch 342, Training Loss: 2.2572245597839355, Validation Loss: 2.2651174068450928\n",
      "Epoch 343, Training Loss: 2.257215738296509, Validation Loss: 2.265186071395874\n",
      "Epoch 344, Training Loss: 2.257047414779663, Validation Loss: 2.2660250663757324\n",
      "Epoch 345, Training Loss: 2.2571539878845215, Validation Loss: 2.2644639015197754\n",
      "Epoch 346, Training Loss: 2.2571215629577637, Validation Loss: 2.2648837566375732\n",
      "Epoch 347, Training Loss: 2.256999969482422, Validation Loss: 2.264477252960205\n",
      "Epoch 348, Training Loss: 2.2568368911743164, Validation Loss: 2.265076160430908\n",
      "Epoch 349, Training Loss: 2.256727457046509, Validation Loss: 2.265756845474243\n",
      "Epoch 350, Training Loss: 2.256772756576538, Validation Loss: 2.2651193141937256\n",
      "Epoch 351, Training Loss: 2.256709337234497, Validation Loss: 2.2652223110198975\n",
      "Epoch 352, Training Loss: 2.2565455436706543, Validation Loss: 2.2653214931488037\n",
      "Epoch 353, Training Loss: 2.2565925121307373, Validation Loss: 2.2645537853240967\n",
      "Epoch 354, Training Loss: 2.2564609050750732, Validation Loss: 2.2644762992858887\n",
      "Epoch 355, Training Loss: 2.256402015686035, Validation Loss: 2.2647647857666016\n",
      "Epoch 356, Training Loss: 2.2564780712127686, Validation Loss: 2.2645444869995117\n",
      "Epoch 357, Training Loss: 2.256330966949463, Validation Loss: 2.263819932937622\n",
      "Epoch 358, Training Loss: 2.2563350200653076, Validation Loss: 2.2651305198669434\n",
      "Epoch 359, Training Loss: 2.256286144256592, Validation Loss: 2.2641265392303467\n",
      "Epoch 360, Training Loss: 2.2561562061309814, Validation Loss: 2.2645323276519775\n",
      "Epoch 361, Training Loss: 2.256024122238159, Validation Loss: 2.264751672744751\n",
      "Epoch 362, Training Loss: 2.2561275959014893, Validation Loss: 2.263654947280884\n",
      "Epoch 363, Training Loss: 2.2561731338500977, Validation Loss: 2.2641005516052246\n",
      "Epoch 364, Training Loss: 2.2560083866119385, Validation Loss: 2.2652528285980225\n",
      "Epoch 365, Training Loss: 2.256385326385498, Validation Loss: 2.263949394226074\n",
      "Epoch 366, Training Loss: 2.2558088302612305, Validation Loss: 2.2636656761169434\n",
      "Epoch 367, Training Loss: 2.2558016777038574, Validation Loss: 2.2647461891174316\n",
      "Epoch 368, Training Loss: 2.2564637660980225, Validation Loss: 2.2640469074249268\n",
      "Epoch 369, Training Loss: 2.256110191345215, Validation Loss: 2.2624106407165527\n",
      "Epoch 370, Training Loss: 2.256148099899292, Validation Loss: 2.262868642807007\n",
      "Epoch 371, Training Loss: 2.2556943893432617, Validation Loss: 2.26464581489563\n",
      "Epoch 372, Training Loss: 2.2562100887298584, Validation Loss: 2.264446973800659\n",
      "Epoch 373, Training Loss: 2.2559196949005127, Validation Loss: 2.2625811100006104\n",
      "Epoch 374, Training Loss: 2.255720853805542, Validation Loss: 2.262868642807007\n",
      "Epoch 375, Training Loss: 2.255476236343384, Validation Loss: 2.263774871826172\n",
      "Epoch 376, Training Loss: 2.255333423614502, Validation Loss: 2.262784481048584\n",
      "Epoch 377, Training Loss: 2.2558681964874268, Validation Loss: 2.263956308364868\n",
      "Epoch 378, Training Loss: 2.255391836166382, Validation Loss: 2.263550281524658\n",
      "Epoch 379, Training Loss: 2.2553958892822266, Validation Loss: 2.26214599609375\n",
      "Epoch 380, Training Loss: 2.255167007446289, Validation Loss: 2.261741876602173\n",
      "Epoch 381, Training Loss: 2.2552154064178467, Validation Loss: 2.263460636138916\n",
      "Epoch 382, Training Loss: 2.255145788192749, Validation Loss: 2.262920618057251\n",
      "Epoch 383, Training Loss: 2.254889488220215, Validation Loss: 2.2622053623199463\n",
      "Epoch 384, Training Loss: 2.25490665435791, Validation Loss: 2.263101816177368\n",
      "Epoch 385, Training Loss: 2.254926919937134, Validation Loss: 2.262587785720825\n",
      "Epoch 386, Training Loss: 2.25469708442688, Validation Loss: 2.262174367904663\n",
      "Epoch 387, Training Loss: 2.2547755241394043, Validation Loss: 2.2635304927825928\n",
      "Epoch 388, Training Loss: 2.254812240600586, Validation Loss: 2.263230085372925\n",
      "Epoch 389, Training Loss: 2.2545886039733887, Validation Loss: 2.261347770690918\n",
      "Epoch 390, Training Loss: 2.254848003387451, Validation Loss: 2.2618248462677\n",
      "Epoch 391, Training Loss: 2.254422664642334, Validation Loss: 2.2622811794281006\n",
      "Epoch 392, Training Loss: 2.254574775695801, Validation Loss: 2.261927366256714\n",
      "Epoch 393, Training Loss: 2.254331588745117, Validation Loss: 2.2621898651123047\n",
      "Epoch 394, Training Loss: 2.2542173862457275, Validation Loss: 2.262449026107788\n",
      "Epoch 395, Training Loss: 2.25435209274292, Validation Loss: 2.261676073074341\n",
      "Epoch 396, Training Loss: 2.25423002243042, Validation Loss: 2.261737823486328\n",
      "Epoch 397, Training Loss: 2.2541086673736572, Validation Loss: 2.2618696689605713\n",
      "Epoch 398, Training Loss: 2.254035234451294, Validation Loss: 2.2623085975646973\n",
      "Epoch 399, Training Loss: 2.2541704177856445, Validation Loss: 2.2607061862945557\n",
      "Epoch 400, Training Loss: 2.2542972564697266, Validation Loss: 2.2612738609313965\n",
      "Epoch 401, Training Loss: 2.2541167736053467, Validation Loss: 2.26283597946167\n",
      "Epoch 402, Training Loss: 2.25403094291687, Validation Loss: 2.2623982429504395\n",
      "Epoch 403, Training Loss: 2.253844976425171, Validation Loss: 2.261789560317993\n",
      "Epoch 404, Training Loss: 2.25373911857605, Validation Loss: 2.2613518238067627\n",
      "Epoch 405, Training Loss: 2.2539169788360596, Validation Loss: 2.262432098388672\n",
      "Epoch 406, Training Loss: 2.2538766860961914, Validation Loss: 2.2617461681365967\n",
      "Epoch 407, Training Loss: 2.2537808418273926, Validation Loss: 2.2613840103149414\n",
      "Epoch 408, Training Loss: 2.2537195682525635, Validation Loss: 2.261741876602173\n",
      "Epoch 409, Training Loss: 2.2535603046417236, Validation Loss: 2.261044979095459\n",
      "Epoch 410, Training Loss: 2.2534615993499756, Validation Loss: 2.260758638381958\n",
      "Epoch 411, Training Loss: 2.2534968852996826, Validation Loss: 2.261517286300659\n",
      "Epoch 412, Training Loss: 2.25359845161438, Validation Loss: 2.2613773345947266\n",
      "Epoch 413, Training Loss: 2.253413200378418, Validation Loss: 2.260916233062744\n",
      "Epoch 414, Training Loss: 2.2535274028778076, Validation Loss: 2.2617435455322266\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(pdlmodel.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pdlmodel.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = pdlmodel(user_features, product_features, all_x_other_products,prices)\n",
    "    choice_probabilities = F.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]),decision_train1+1])\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    pdlmodel.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = pdlmodel(X_user_val,  product_features, all_x_other_products,prices)\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        # torch.save(pdlmodel.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b5374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c915a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3665.69\n",
      "Expected Revenue all treated: $739.75\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17cb1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = (expected_revenue_all_treated-expected_revenue_all_control)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a4df632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of PDL:  -0.89%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(pdl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c900cf",
   "metadata": {
    "id": "63c900cf"
   },
   "source": [
    "# use dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ecd9bca0-6191-4297-8452-7f8af22d5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityEstimator(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(UtilityEstimator, self).__init__()\n",
    "        \n",
    "        # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "        self.theta0 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        # Output layer for Theta1 (takes xi, zj, z-j, p-j)\n",
    "        self.theta1 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_user, x_product, x_other_products,price):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "    \n",
    "\n",
    "        # Combine features for Theta0\n",
    "        \n",
    "        combined_features_theta =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                               x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                               aggregated_other_features.unsqueeze(0).expand(N, -1, -1)),\n",
    "                                                 dim=2)\n",
    "        theta0_output = self.theta0(combined_features_theta).squeeze(-1)\n",
    "        theta1_output = self.theta1(combined_features_theta).squeeze(-1)\n",
    "        \n",
    "        price = price.unsqueeze(-1)  \n",
    "        utility = theta0_output + theta1_output * price.squeeze(-1)\n",
    "\n",
    "        # Include the outside option (utility = 0)\n",
    "        zero_utilities = torch.zeros(x_user.shape[0], 1, device=utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utility), dim=1)\n",
    "        \n",
    "        return utilities_with_outside,theta0_output,theta1_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf77eeb6",
   "metadata": {
    "id": "cf77eeb6"
   },
   "outputs": [],
   "source": [
    "dml_model = UtilityEstimator(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "QT_wrrh3rIws",
   "metadata": {
    "id": "QT_wrrh3rIws"
   },
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31b522ed-0c36-4199-a309-74f71aece365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0ff32e5-5c64-49ad-bbf0-f6aa2a53d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b386e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b386e12",
    "outputId": "aab71afc-5217-4c73-eba6-26cb31bed335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.4104995727539062, Validation Loss: 2.4049038887023926\n",
      "Epoch 2, Training Loss: 2.404656410217285, Validation Loss: 2.400033712387085\n",
      "Epoch 3, Training Loss: 2.39985728263855, Validation Loss: 2.3957762718200684\n",
      "Epoch 4, Training Loss: 2.395529270172119, Validation Loss: 2.391464948654175\n",
      "Epoch 5, Training Loss: 2.3912882804870605, Validation Loss: 2.386776924133301\n",
      "Epoch 6, Training Loss: 2.3867173194885254, Validation Loss: 2.381265640258789\n",
      "Epoch 7, Training Loss: 2.3815460205078125, Validation Loss: 2.374903917312622\n",
      "Epoch 8, Training Loss: 2.3757407665252686, Validation Loss: 2.36799693107605\n",
      "Epoch 9, Training Loss: 2.3691470623016357, Validation Loss: 2.360262870788574\n",
      "Epoch 10, Training Loss: 2.3618290424346924, Validation Loss: 2.3522448539733887\n",
      "Epoch 11, Training Loss: 2.353926181793213, Validation Loss: 2.343076705932617\n",
      "Epoch 12, Training Loss: 2.3449535369873047, Validation Loss: 2.3323538303375244\n",
      "Epoch 13, Training Loss: 2.334913730621338, Validation Loss: 2.3209450244903564\n",
      "Epoch 14, Training Loss: 2.3243136405944824, Validation Loss: 2.3098831176757812\n",
      "Epoch 15, Training Loss: 2.3140969276428223, Validation Loss: 2.300044059753418\n",
      "Epoch 16, Training Loss: 2.305182456970215, Validation Loss: 2.2905356884002686\n",
      "Epoch 17, Training Loss: 2.296407461166382, Validation Loss: 2.2810561656951904\n",
      "Epoch 18, Training Loss: 2.2872872352600098, Validation Loss: 2.2723875045776367\n",
      "Epoch 19, Training Loss: 2.278524875640869, Validation Loss: 2.265352964401245\n",
      "Epoch 20, Training Loss: 2.2711198329925537, Validation Loss: 2.260239362716675\n",
      "Epoch 21, Training Loss: 2.265503406524658, Validation Loss: 2.2568180561065674\n",
      "Epoch 22, Training Loss: 2.2616279125213623, Validation Loss: 2.2546603679656982\n",
      "Epoch 23, Training Loss: 2.258892774581909, Validation Loss: 2.2531168460845947\n",
      "Epoch 24, Training Loss: 2.2568392753601074, Validation Loss: 2.2518229484558105\n",
      "Epoch 25, Training Loss: 2.254967451095581, Validation Loss: 2.2505435943603516\n",
      "Epoch 26, Training Loss: 2.2533953189849854, Validation Loss: 2.2495272159576416\n",
      "Epoch 27, Training Loss: 2.252216339111328, Validation Loss: 2.249011278152466\n",
      "Epoch 28, Training Loss: 2.251553773880005, Validation Loss: 2.2488484382629395\n",
      "Epoch 29, Training Loss: 2.2512528896331787, Validation Loss: 2.2487051486968994\n",
      "Epoch 30, Training Loss: 2.2510173320770264, Validation Loss: 2.2482213973999023\n",
      "Epoch 31, Training Loss: 2.250462055206299, Validation Loss: 2.2471792697906494\n",
      "Epoch 32, Training Loss: 2.2494056224823, Validation Loss: 2.2457199096679688\n",
      "Epoch 33, Training Loss: 2.2479968070983887, Validation Loss: 2.2442729473114014\n",
      "Epoch 34, Training Loss: 2.2466623783111572, Validation Loss: 2.243325710296631\n",
      "Epoch 35, Training Loss: 2.245868682861328, Validation Loss: 2.2430973052978516\n",
      "Epoch 36, Training Loss: 2.2457942962646484, Validation Loss: 2.2433557510375977\n",
      "Epoch 37, Training Loss: 2.2461354732513428, Validation Loss: 2.243642568588257\n",
      "Epoch 38, Training Loss: 2.2463715076446533, Validation Loss: 2.2437222003936768\n",
      "Epoch 39, Training Loss: 2.2462830543518066, Validation Loss: 2.2437047958374023\n",
      "Epoch 40, Training Loss: 2.246058702468872, Validation Loss: 2.2438247203826904\n",
      "Epoch 41, Training Loss: 2.245995044708252, Validation Loss: 2.244107484817505\n",
      "Epoch 42, Training Loss: 2.2461676597595215, Validation Loss: 2.244331121444702\n",
      "Epoch 43, Training Loss: 2.2463619709014893, Validation Loss: 2.2441930770874023\n",
      "Epoch 44, Training Loss: 2.2463173866271973, Validation Loss: 2.243633508682251\n",
      "Epoch 45, Training Loss: 2.245957851409912, Validation Loss: 2.2428383827209473\n",
      "Epoch 46, Training Loss: 2.24544358253479, Validation Loss: 2.2420969009399414\n",
      "Epoch 47, Training Loss: 2.2450199127197266, Validation Loss: 2.241610527038574\n",
      "Epoch 48, Training Loss: 2.24483323097229, Validation Loss: 2.241373062133789\n",
      "Epoch 49, Training Loss: 2.2448365688323975, Validation Loss: 2.2412757873535156\n",
      "Epoch 50, Training Loss: 2.244866132736206, Validation Loss: 2.2411956787109375\n",
      "Epoch 51, Training Loss: 2.244816780090332, Validation Loss: 2.2412140369415283\n",
      "Epoch 52, Training Loss: 2.244729518890381, Validation Loss: 2.2413136959075928\n",
      "Epoch 53, Training Loss: 2.2447009086608887, Validation Loss: 2.2415084838867188\n",
      "Epoch 54, Training Loss: 2.24476957321167, Validation Loss: 2.2417144775390625\n",
      "Epoch 55, Training Loss: 2.244891405105591, Validation Loss: 2.2418878078460693\n",
      "Epoch 56, Training Loss: 2.244955062866211, Validation Loss: 2.2419233322143555\n",
      "Epoch 57, Training Loss: 2.2449166774749756, Validation Loss: 2.241913080215454\n",
      "Epoch 58, Training Loss: 2.2448086738586426, Validation Loss: 2.241802453994751\n",
      "Epoch 59, Training Loss: 2.244711399078369, Validation Loss: 2.241673231124878\n",
      "Epoch 60, Training Loss: 2.2446441650390625, Validation Loss: 2.2415120601654053\n",
      "Epoch 61, Training Loss: 2.2446155548095703, Validation Loss: 2.241407632827759\n",
      "Epoch 62, Training Loss: 2.244574785232544, Validation Loss: 2.2413697242736816\n",
      "Epoch 63, Training Loss: 2.244516134262085, Validation Loss: 2.241306781768799\n",
      "Epoch 64, Training Loss: 2.244476795196533, Validation Loss: 2.2412407398223877\n",
      "Epoch 65, Training Loss: 2.244479179382324, Validation Loss: 2.241267442703247\n",
      "Epoch 66, Training Loss: 2.2445199489593506, Validation Loss: 2.2413320541381836\n",
      "Epoch 67, Training Loss: 2.244563341140747, Validation Loss: 2.2414166927337646\n",
      "Epoch 68, Training Loss: 2.2445733547210693, Validation Loss: 2.2415285110473633\n",
      "Epoch 69, Training Loss: 2.244555711746216, Validation Loss: 2.2415454387664795\n",
      "Epoch 70, Training Loss: 2.24452543258667, Validation Loss: 2.241549253463745\n",
      "Epoch 71, Training Loss: 2.2444875240325928, Validation Loss: 2.2415552139282227\n",
      "Epoch 72, Training Loss: 2.244474172592163, Validation Loss: 2.2415611743927\n",
      "Epoch 73, Training Loss: 2.24446439743042, Validation Loss: 2.241593837738037\n",
      "Epoch 74, Training Loss: 2.244447708129883, Validation Loss: 2.2416765689849854\n",
      "Epoch 75, Training Loss: 2.244417667388916, Validation Loss: 2.241767644882202\n",
      "Epoch 76, Training Loss: 2.2444026470184326, Validation Loss: 2.2417805194854736\n",
      "Epoch 77, Training Loss: 2.244401693344116, Validation Loss: 2.241757869720459\n",
      "Epoch 78, Training Loss: 2.244408369064331, Validation Loss: 2.241666555404663\n",
      "Epoch 79, Training Loss: 2.2444093227386475, Validation Loss: 2.2415809631347656\n",
      "Epoch 80, Training Loss: 2.244410514831543, Validation Loss: 2.2415499687194824\n",
      "Epoch 81, Training Loss: 2.244398832321167, Validation Loss: 2.241651773452759\n",
      "Epoch 82, Training Loss: 2.2443971633911133, Validation Loss: 2.241647481918335\n",
      "Epoch 83, Training Loss: 2.2443952560424805, Validation Loss: 2.2415525913238525\n",
      "Epoch 84, Training Loss: 2.244378089904785, Validation Loss: 2.2415387630462646\n",
      "Epoch 85, Training Loss: 2.244410991668701, Validation Loss: 2.241682291030884\n",
      "Epoch 86, Training Loss: 2.2443575859069824, Validation Loss: 2.2417683601379395\n",
      "Epoch 87, Training Loss: 2.2443792819976807, Validation Loss: 2.2417852878570557\n",
      "Epoch 88, Training Loss: 2.244385004043579, Validation Loss: 2.241821050643921\n",
      "Epoch 89, Training Loss: 2.244385242462158, Validation Loss: 2.2418394088745117\n",
      "Epoch 90, Training Loss: 2.244382619857788, Validation Loss: 2.241842031478882\n",
      "Epoch 91, Training Loss: 2.2443785667419434, Validation Loss: 2.241835594177246\n",
      "Epoch 92, Training Loss: 2.244375467300415, Validation Loss: 2.2418265342712402\n",
      "Epoch 93, Training Loss: 2.2443735599517822, Validation Loss: 2.241818428039551\n",
      "Epoch 94, Training Loss: 2.244371175765991, Validation Loss: 2.2418148517608643\n",
      "Epoch 95, Training Loss: 2.2443668842315674, Validation Loss: 2.241816520690918\n",
      "Epoch 96, Training Loss: 2.2443625926971436, Validation Loss: 2.24182391166687\n",
      "Epoch 97, Training Loss: 2.244359016418457, Validation Loss: 2.2418339252471924\n",
      "Epoch 98, Training Loss: 2.244356393814087, Validation Loss: 2.2418417930603027\n",
      "Epoch 99, Training Loss: 2.2443532943725586, Validation Loss: 2.2418453693389893\n",
      "Epoch 100, Training Loss: 2.244349479675293, Validation Loss: 2.241844892501831\n",
      "Epoch 101, Training Loss: 2.2443463802337646, Validation Loss: 2.2418410778045654\n",
      "Epoch 102, Training Loss: 2.2443439960479736, Validation Loss: 2.241835594177246\n",
      "Epoch 103, Training Loss: 2.244342565536499, Validation Loss: 2.2418277263641357\n",
      "Epoch 104, Training Loss: 2.244340181350708, Validation Loss: 2.241818428039551\n",
      "Epoch 105, Training Loss: 2.2443368434906006, Validation Loss: 2.241809844970703\n",
      "Epoch 106, Training Loss: 2.2443337440490723, Validation Loss: 2.241802930831909\n",
      "Epoch 107, Training Loss: 2.244331121444702, Validation Loss: 2.2417969703674316\n",
      "Epoch 108, Training Loss: 2.244328498840332, Validation Loss: 2.241793632507324\n",
      "Epoch 109, Training Loss: 2.244325876235962, Validation Loss: 2.241793632507324\n",
      "Epoch 110, Training Loss: 2.2443227767944336, Validation Loss: 2.241797924041748\n",
      "Epoch 111, Training Loss: 2.2443199157714844, Validation Loss: 2.241807222366333\n",
      "Epoch 112, Training Loss: 2.2443177700042725, Validation Loss: 2.2418203353881836\n",
      "Epoch 113, Training Loss: 2.2443156242370605, Validation Loss: 2.2418346405029297\n",
      "Epoch 114, Training Loss: 2.2443130016326904, Validation Loss: 2.241847276687622\n",
      "Epoch 115, Training Loss: 2.2443108558654785, Validation Loss: 2.241856098175049\n",
      "Epoch 116, Training Loss: 2.2443087100982666, Validation Loss: 2.241858959197998\n",
      "Epoch 117, Training Loss: 2.2443063259124756, Validation Loss: 2.2418551445007324\n",
      "Epoch 118, Training Loss: 2.2443037033081055, Validation Loss: 2.2418465614318848\n",
      "Epoch 119, Training Loss: 2.2443013191223145, Validation Loss: 2.241835117340088\n",
      "Epoch 120, Training Loss: 2.2442984580993652, Validation Loss: 2.2418243885040283\n",
      "Epoch 121, Training Loss: 2.244296073913574, Validation Loss: 2.241816759109497\n",
      "Epoch 122, Training Loss: 2.2442944049835205, Validation Loss: 2.2418131828308105\n",
      "Epoch 123, Training Loss: 2.2442917823791504, Validation Loss: 2.241814136505127\n",
      "Epoch 124, Training Loss: 2.2442896366119385, Validation Loss: 2.241818428039551\n",
      "Epoch 125, Training Loss: 2.2442872524261475, Validation Loss: 2.2418246269226074\n",
      "Epoch 126, Training Loss: 2.2442851066589355, Validation Loss: 2.241831064224243\n",
      "Epoch 127, Training Loss: 2.2442829608917236, Validation Loss: 2.2418372631073\n",
      "Epoch 128, Training Loss: 2.2442805767059326, Validation Loss: 2.241842746734619\n",
      "Epoch 129, Training Loss: 2.2442781925201416, Validation Loss: 2.241847276687622\n",
      "Epoch 130, Training Loss: 2.2442760467529297, Validation Loss: 2.241851806640625\n",
      "Epoch 131, Training Loss: 2.2442739009857178, Validation Loss: 2.241856098175049\n",
      "Epoch 132, Training Loss: 2.2442715167999268, Validation Loss: 2.241860866546631\n",
      "Epoch 133, Training Loss: 2.244269609451294, Validation Loss: 2.241865873336792\n",
      "Epoch 134, Training Loss: 2.244267463684082, Validation Loss: 2.241870403289795\n",
      "Epoch 135, Training Loss: 2.24426531791687, Validation Loss: 2.241875171661377\n",
      "Epoch 136, Training Loss: 2.244262933731079, Validation Loss: 2.241879463195801\n",
      "Epoch 137, Training Loss: 2.244260549545288, Validation Loss: 2.241884469985962\n",
      "Epoch 138, Training Loss: 2.244258403778076, Validation Loss: 2.2418882846832275\n",
      "Epoch 139, Training Loss: 2.2442564964294434, Validation Loss: 2.2418925762176514\n",
      "Epoch 140, Training Loss: 2.2442543506622314, Validation Loss: 2.241896390914917\n",
      "Epoch 141, Training Loss: 2.2442524433135986, Validation Loss: 2.2418994903564453\n",
      "Epoch 142, Training Loss: 2.2442500591278076, Validation Loss: 2.2419025897979736\n",
      "Epoch 143, Training Loss: 2.244248151779175, Validation Loss: 2.241905689239502\n",
      "Epoch 144, Training Loss: 2.244245767593384, Validation Loss: 2.2419087886810303\n",
      "Epoch 145, Training Loss: 2.244243860244751, Validation Loss: 2.241913080215454\n",
      "Epoch 146, Training Loss: 2.244241714477539, Validation Loss: 2.2419190406799316\n",
      "Epoch 147, Training Loss: 2.244239568710327, Validation Loss: 2.2419261932373047\n",
      "Epoch 148, Training Loss: 2.2442376613616943, Validation Loss: 2.241933584213257\n",
      "Epoch 149, Training Loss: 2.2442352771759033, Validation Loss: 2.241941452026367\n",
      "Epoch 150, Training Loss: 2.2442333698272705, Validation Loss: 2.241948366165161\n",
      "Epoch 151, Training Loss: 2.2442314624786377, Validation Loss: 2.2419543266296387\n",
      "Epoch 152, Training Loss: 2.2442290782928467, Validation Loss: 2.2419588565826416\n",
      "Epoch 153, Training Loss: 2.2442269325256348, Validation Loss: 2.241962432861328\n",
      "Epoch 154, Training Loss: 2.244225025177002, Validation Loss: 2.2419655323028564\n",
      "Epoch 155, Training Loss: 2.244223117828369, Validation Loss: 2.2419686317443848\n",
      "Epoch 156, Training Loss: 2.2442209720611572, Validation Loss: 2.2419722080230713\n",
      "Epoch 157, Training Loss: 2.2442190647125244, Validation Loss: 2.241976737976074\n",
      "Epoch 158, Training Loss: 2.2442171573638916, Validation Loss: 2.2419815063476562\n",
      "Epoch 159, Training Loss: 2.2442147731781006, Validation Loss: 2.2419869899749756\n",
      "Epoch 160, Training Loss: 2.2442128658294678, Validation Loss: 2.241992235183716\n",
      "Epoch 161, Training Loss: 2.244210958480835, Validation Loss: 2.2419981956481934\n",
      "Epoch 162, Training Loss: 2.244208812713623, Validation Loss: 2.242003917694092\n",
      "Epoch 163, Training Loss: 2.2442071437835693, Validation Loss: 2.242009401321411\n",
      "Epoch 164, Training Loss: 2.2442049980163574, Validation Loss: 2.2420146465301514\n",
      "Epoch 165, Training Loss: 2.2442026138305664, Validation Loss: 2.2420201301574707\n",
      "Epoch 166, Training Loss: 2.2442004680633545, Validation Loss: 2.24202561378479\n",
      "Epoch 167, Training Loss: 2.244198799133301, Validation Loss: 2.242030620574951\n",
      "Epoch 168, Training Loss: 2.244196891784668, Validation Loss: 2.2420358657836914\n",
      "Epoch 169, Training Loss: 2.244194984436035, Validation Loss: 2.24204158782959\n",
      "Epoch 170, Training Loss: 2.2441928386688232, Validation Loss: 2.2420475482940674\n",
      "Epoch 171, Training Loss: 2.2441909313201904, Validation Loss: 2.242053270339966\n",
      "Epoch 172, Training Loss: 2.2441887855529785, Validation Loss: 2.2420592308044434\n",
      "Epoch 173, Training Loss: 2.244187116622925, Validation Loss: 2.242064952850342\n",
      "Epoch 174, Training Loss: 2.244185209274292, Validation Loss: 2.2420709133148193\n",
      "Epoch 175, Training Loss: 2.244183301925659, Validation Loss: 2.2420766353607178\n",
      "Epoch 176, Training Loss: 2.2441811561584473, Validation Loss: 2.242082118988037\n",
      "Epoch 177, Training Loss: 2.2441790103912354, Validation Loss: 2.2420880794525146\n",
      "Epoch 178, Training Loss: 2.2441773414611816, Validation Loss: 2.242093801498413\n",
      "Epoch 179, Training Loss: 2.244175672531128, Validation Loss: 2.242100238800049\n",
      "Epoch 180, Training Loss: 2.244173526763916, Validation Loss: 2.2421071529388428\n",
      "Epoch 181, Training Loss: 2.244171380996704, Validation Loss: 2.2421133518218994\n",
      "Epoch 182, Training Loss: 2.2441699504852295, Validation Loss: 2.2421202659606934\n",
      "Epoch 183, Training Loss: 2.2441680431365967, Validation Loss: 2.24212646484375\n",
      "Epoch 184, Training Loss: 2.244166135787964, Validation Loss: 2.2421329021453857\n",
      "Epoch 185, Training Loss: 2.244163990020752, Validation Loss: 2.2421393394470215\n",
      "Epoch 186, Training Loss: 2.2441623210906982, Validation Loss: 2.242145299911499\n",
      "Epoch 187, Training Loss: 2.2441604137420654, Validation Loss: 2.2421517372131348\n",
      "Epoch 188, Training Loss: 2.2441585063934326, Validation Loss: 2.2421581745147705\n",
      "Epoch 189, Training Loss: 2.2441565990448, Validation Loss: 2.2421648502349854\n",
      "Epoch 190, Training Loss: 2.244154930114746, Validation Loss: 2.2421717643737793\n",
      "Epoch 191, Training Loss: 2.2441530227661133, Validation Loss: 2.2421786785125732\n",
      "Epoch 192, Training Loss: 2.2441511154174805, Validation Loss: 2.242185592651367\n",
      "Epoch 193, Training Loss: 2.2441494464874268, Validation Loss: 2.2421927452087402\n",
      "Epoch 194, Training Loss: 2.244147539138794, Validation Loss: 2.2421998977661133\n",
      "Epoch 195, Training Loss: 2.2441461086273193, Validation Loss: 2.242206573486328\n",
      "Epoch 196, Training Loss: 2.2441442012786865, Validation Loss: 2.2422139644622803\n",
      "Epoch 197, Training Loss: 2.2441422939300537, Validation Loss: 2.242220878601074\n",
      "Epoch 198, Training Loss: 2.244140625, Validation Loss: 2.242227554321289\n",
      "Epoch 199, Training Loss: 2.2441389560699463, Validation Loss: 2.242234706878662\n",
      "Epoch 200, Training Loss: 2.2441372871398926, Validation Loss: 2.2422420978546143\n",
      "Epoch 201, Training Loss: 2.2441353797912598, Validation Loss: 2.2422497272491455\n",
      "Epoch 202, Training Loss: 2.244133710861206, Validation Loss: 2.2422568798065186\n",
      "Epoch 203, Training Loss: 2.2441320419311523, Validation Loss: 2.2422642707824707\n",
      "Epoch 204, Training Loss: 2.2441301345825195, Validation Loss: 2.242271661758423\n",
      "Epoch 205, Training Loss: 2.244128465652466, Validation Loss: 2.242279052734375\n",
      "Epoch 206, Training Loss: 2.244127035140991, Validation Loss: 2.2422866821289062\n",
      "Epoch 207, Training Loss: 2.2441253662109375, Validation Loss: 2.2422940731048584\n",
      "Epoch 208, Training Loss: 2.244123697280884, Validation Loss: 2.2423019409179688\n",
      "Epoch 209, Training Loss: 2.24412202835083, Validation Loss: 2.242309093475342\n",
      "Epoch 210, Training Loss: 2.2441205978393555, Validation Loss: 2.242316961288452\n",
      "Epoch 211, Training Loss: 2.2441189289093018, Validation Loss: 2.2423248291015625\n",
      "Epoch 212, Training Loss: 2.244117498397827, Validation Loss: 2.2423324584960938\n",
      "Epoch 213, Training Loss: 2.2441158294677734, Validation Loss: 2.242339849472046\n",
      "Epoch 214, Training Loss: 2.2441141605377197, Validation Loss: 2.2423479557037354\n",
      "Epoch 215, Training Loss: 2.244112730026245, Validation Loss: 2.2423558235168457\n",
      "Epoch 216, Training Loss: 2.2441110610961914, Validation Loss: 2.242363691329956\n",
      "Epoch 217, Training Loss: 2.244109869003296, Validation Loss: 2.2423717975616455\n",
      "Epoch 218, Training Loss: 2.244108200073242, Validation Loss: 2.2423791885375977\n",
      "Epoch 219, Training Loss: 2.2441067695617676, Validation Loss: 2.242387294769287\n",
      "Epoch 220, Training Loss: 2.244105339050293, Validation Loss: 2.2423954010009766\n",
      "Epoch 221, Training Loss: 2.2441036701202393, Validation Loss: 2.242403507232666\n",
      "Epoch 222, Training Loss: 2.2441024780273438, Validation Loss: 2.2424111366271973\n",
      "Epoch 223, Training Loss: 2.244101047515869, Validation Loss: 2.242419481277466\n",
      "Epoch 224, Training Loss: 2.2440993785858154, Validation Loss: 2.242427349090576\n",
      "Epoch 225, Training Loss: 2.24409818649292, Validation Loss: 2.2424356937408447\n",
      "Epoch 226, Training Loss: 2.2440967559814453, Validation Loss: 2.242443561553955\n",
      "Epoch 227, Training Loss: 2.24409556388855, Validation Loss: 2.2424516677856445\n",
      "Epoch 228, Training Loss: 2.244094133377075, Validation Loss: 2.242459535598755\n",
      "Epoch 229, Training Loss: 2.2440929412841797, Validation Loss: 2.2424678802490234\n",
      "Epoch 230, Training Loss: 2.244091510772705, Validation Loss: 2.242475986480713\n",
      "Epoch 231, Training Loss: 2.2440903186798096, Validation Loss: 2.2424840927124023\n",
      "Epoch 232, Training Loss: 2.244088888168335, Validation Loss: 2.242492198944092\n",
      "Epoch 233, Training Loss: 2.2440876960754395, Validation Loss: 2.2425003051757812\n",
      "Epoch 234, Training Loss: 2.244086503982544, Validation Loss: 2.2425084114074707\n",
      "Epoch 235, Training Loss: 2.2440850734710693, Validation Loss: 2.2425167560577393\n",
      "Epoch 236, Training Loss: 2.244084119796753, Validation Loss: 2.2425248622894287\n",
      "Epoch 237, Training Loss: 2.2440829277038574, Validation Loss: 2.242532730102539\n",
      "Epoch 238, Training Loss: 2.244081735610962, Validation Loss: 2.2425408363342285\n",
      "Epoch 239, Training Loss: 2.2440805435180664, Validation Loss: 2.242549180984497\n",
      "Epoch 240, Training Loss: 2.244079351425171, Validation Loss: 2.2425572872161865\n",
      "Epoch 241, Training Loss: 2.2440783977508545, Validation Loss: 2.242565393447876\n",
      "Epoch 242, Training Loss: 2.244077205657959, Validation Loss: 2.2425732612609863\n",
      "Epoch 243, Training Loss: 2.2440762519836426, Validation Loss: 2.242581367492676\n",
      "Epoch 244, Training Loss: 2.244075059890747, Validation Loss: 2.2425894737243652\n",
      "Epoch 245, Training Loss: 2.2440743446350098, Validation Loss: 2.242597818374634\n",
      "Epoch 246, Training Loss: 2.244072914123535, Validation Loss: 2.242605686187744\n",
      "Epoch 247, Training Loss: 2.2440719604492188, Validation Loss: 2.2426133155822754\n",
      "Epoch 248, Training Loss: 2.2440712451934814, Validation Loss: 2.242621421813965\n",
      "Epoch 249, Training Loss: 2.244069814682007, Validation Loss: 2.2426297664642334\n",
      "Epoch 250, Training Loss: 2.2440690994262695, Validation Loss: 2.2426373958587646\n",
      "Epoch 251, Training Loss: 2.244068145751953, Validation Loss: 2.242645263671875\n",
      "Epoch 252, Training Loss: 2.2440671920776367, Validation Loss: 2.2426533699035645\n",
      "Epoch 253, Training Loss: 2.2440662384033203, Validation Loss: 2.242661237716675\n",
      "Epoch 254, Training Loss: 2.244065284729004, Validation Loss: 2.242669105529785\n",
      "Epoch 255, Training Loss: 2.2440645694732666, Validation Loss: 2.2426767349243164\n",
      "Epoch 256, Training Loss: 2.244063377380371, Validation Loss: 2.2426841259002686\n",
      "Epoch 257, Training Loss: 2.2440624237060547, Validation Loss: 2.242691993713379\n",
      "Epoch 258, Training Loss: 2.2440621852874756, Validation Loss: 2.24269962310791\n",
      "Epoch 259, Training Loss: 2.244061231613159, Validation Loss: 2.2427072525024414\n",
      "Epoch 260, Training Loss: 2.2440602779388428, Validation Loss: 2.2427148818969727\n",
      "Epoch 261, Training Loss: 2.2440595626831055, Validation Loss: 2.242722272872925\n",
      "Epoch 262, Training Loss: 2.244058609008789, Validation Loss: 2.242729663848877\n",
      "Epoch 263, Training Loss: 2.2440576553344727, Validation Loss: 2.242737054824829\n",
      "Epoch 264, Training Loss: 2.2440574169158936, Validation Loss: 2.2427446842193604\n",
      "Epoch 265, Training Loss: 2.244056463241577, Validation Loss: 2.2427518367767334\n",
      "Epoch 266, Training Loss: 2.2440555095672607, Validation Loss: 2.2427589893341064\n",
      "Epoch 267, Training Loss: 2.2440550327301025, Validation Loss: 2.2427661418914795\n",
      "Epoch 268, Training Loss: 2.2440543174743652, Validation Loss: 2.2427735328674316\n",
      "Epoch 269, Training Loss: 2.244053602218628, Validation Loss: 2.2427806854248047\n",
      "Epoch 270, Training Loss: 2.2440528869628906, Validation Loss: 2.2427875995635986\n",
      "Epoch 271, Training Loss: 2.2440524101257324, Validation Loss: 2.2427945137023926\n",
      "Epoch 272, Training Loss: 2.244051694869995, Validation Loss: 2.2428014278411865\n",
      "Epoch 273, Training Loss: 2.244050979614258, Validation Loss: 2.2428081035614014\n",
      "Epoch 274, Training Loss: 2.2440502643585205, Validation Loss: 2.2428150177001953\n",
      "Epoch 275, Training Loss: 2.2440500259399414, Validation Loss: 2.24282169342041\n",
      "Epoch 276, Training Loss: 2.244049310684204, Validation Loss: 2.242828369140625\n",
      "Epoch 277, Training Loss: 2.244048595428467, Validation Loss: 2.24283504486084\n",
      "Epoch 278, Training Loss: 2.2440478801727295, Validation Loss: 2.2428414821624756\n",
      "Epoch 279, Training Loss: 2.2440474033355713, Validation Loss: 2.2428481578826904\n",
      "Epoch 280, Training Loss: 2.244046926498413, Validation Loss: 2.242854356765747\n",
      "Epoch 281, Training Loss: 2.244046211242676, Validation Loss: 2.242861032485962\n",
      "Epoch 282, Training Loss: 2.2440457344055176, Validation Loss: 2.2428669929504395\n",
      "Epoch 283, Training Loss: 2.2440454959869385, Validation Loss: 2.242873430252075\n",
      "Epoch 284, Training Loss: 2.244044780731201, Validation Loss: 2.2428793907165527\n",
      "Epoch 285, Training Loss: 2.244044303894043, Validation Loss: 2.2428853511810303\n",
      "Epoch 286, Training Loss: 2.2440438270568848, Validation Loss: 2.242891788482666\n",
      "Epoch 287, Training Loss: 2.2440433502197266, Validation Loss: 2.2428975105285645\n",
      "Epoch 288, Training Loss: 2.2440428733825684, Validation Loss: 2.242903470993042\n",
      "Epoch 289, Training Loss: 2.2440426349639893, Validation Loss: 2.2429091930389404\n",
      "Epoch 290, Training Loss: 2.244041919708252, Validation Loss: 2.242914915084839\n",
      "Epoch 291, Training Loss: 2.2440414428710938, Validation Loss: 2.242920398712158\n",
      "Epoch 292, Training Loss: 2.2440407276153564, Validation Loss: 2.2429263591766357\n",
      "Epoch 293, Training Loss: 2.2440404891967773, Validation Loss: 2.242931842803955\n",
      "Epoch 294, Training Loss: 2.244040012359619, Validation Loss: 2.2429370880126953\n",
      "Epoch 295, Training Loss: 2.24403977394104, Validation Loss: 2.2429425716400146\n",
      "Epoch 296, Training Loss: 2.2440390586853027, Validation Loss: 2.242947816848755\n",
      "Epoch 297, Training Loss: 2.2440388202667236, Validation Loss: 2.242953300476074\n",
      "Epoch 298, Training Loss: 2.2440385818481445, Validation Loss: 2.2429580688476562\n",
      "Epoch 299, Training Loss: 2.2440381050109863, Validation Loss: 2.2429633140563965\n",
      "Epoch 300, Training Loss: 2.2440378665924072, Validation Loss: 2.2429683208465576\n",
      "Epoch 301, Training Loss: 2.244037389755249, Validation Loss: 2.2429733276367188\n",
      "Epoch 302, Training Loss: 2.244036912918091, Validation Loss: 2.242978096008301\n",
      "Epoch 303, Training Loss: 2.2440364360809326, Validation Loss: 2.242983102798462\n",
      "Epoch 304, Training Loss: 2.2440359592437744, Validation Loss: 2.242987871170044\n",
      "Epoch 305, Training Loss: 2.2440359592437744, Validation Loss: 2.242992877960205\n",
      "Epoch 306, Training Loss: 2.244035482406616, Validation Loss: 2.242997169494629\n",
      "Epoch 307, Training Loss: 2.244035243988037, Validation Loss: 2.24300217628479\n",
      "Epoch 308, Training Loss: 2.244034767150879, Validation Loss: 2.243006467819214\n",
      "Epoch 309, Training Loss: 2.244034767150879, Validation Loss: 2.2430107593536377\n",
      "Epoch 310, Training Loss: 2.2440340518951416, Validation Loss: 2.2430152893066406\n",
      "Epoch 311, Training Loss: 2.2440338134765625, Validation Loss: 2.2430195808410645\n",
      "Epoch 312, Training Loss: 2.2440335750579834, Validation Loss: 2.2430238723754883\n",
      "Epoch 313, Training Loss: 2.2440333366394043, Validation Loss: 2.243028163909912\n",
      "Epoch 314, Training Loss: 2.244033098220825, Validation Loss: 2.243032217025757\n",
      "Epoch 315, Training Loss: 2.244032621383667, Validation Loss: 2.2430362701416016\n",
      "Epoch 316, Training Loss: 2.244032144546509, Validation Loss: 2.2430403232574463\n",
      "Epoch 317, Training Loss: 2.244032144546509, Validation Loss: 2.24304461479187\n",
      "Epoch 318, Training Loss: 2.2440319061279297, Validation Loss: 2.2430484294891357\n",
      "Epoch 319, Training Loss: 2.2440316677093506, Validation Loss: 2.2430522441864014\n",
      "Epoch 320, Training Loss: 2.2440311908721924, Validation Loss: 2.243056058883667\n",
      "Epoch 321, Training Loss: 2.2440309524536133, Validation Loss: 2.2430596351623535\n",
      "Epoch 322, Training Loss: 2.244030714035034, Validation Loss: 2.2430636882781982\n",
      "Epoch 323, Training Loss: 2.244030475616455, Validation Loss: 2.2430672645568848\n",
      "Epoch 324, Training Loss: 2.244029998779297, Validation Loss: 2.243070602416992\n",
      "Epoch 325, Training Loss: 2.2440297603607178, Validation Loss: 2.243074417114258\n",
      "Epoch 326, Training Loss: 2.2440295219421387, Validation Loss: 2.2430777549743652\n",
      "Epoch 327, Training Loss: 2.2440292835235596, Validation Loss: 2.2430810928344727\n",
      "Epoch 328, Training Loss: 2.2440290451049805, Validation Loss: 2.243084669113159\n",
      "Epoch 329, Training Loss: 2.2440288066864014, Validation Loss: 2.2430880069732666\n",
      "Epoch 330, Training Loss: 2.2440285682678223, Validation Loss: 2.243091344833374\n",
      "Epoch 331, Training Loss: 2.2440285682678223, Validation Loss: 2.2430946826934814\n",
      "Epoch 332, Training Loss: 2.244028329849243, Validation Loss: 2.2430977821350098\n",
      "Epoch 333, Training Loss: 2.244027853012085, Validation Loss: 2.243101119995117\n",
      "Epoch 334, Training Loss: 2.244027853012085, Validation Loss: 2.2431042194366455\n",
      "Epoch 335, Training Loss: 2.244027614593506, Validation Loss: 2.2431070804595947\n",
      "Epoch 336, Training Loss: 2.2440273761749268, Validation Loss: 2.243110418319702\n",
      "Epoch 337, Training Loss: 2.2440271377563477, Validation Loss: 2.2431132793426514\n",
      "Epoch 338, Training Loss: 2.2440266609191895, Validation Loss: 2.2431163787841797\n",
      "Epoch 339, Training Loss: 2.2440266609191895, Validation Loss: 2.243119239807129\n",
      "Epoch 340, Training Loss: 2.2440261840820312, Validation Loss: 2.243122100830078\n",
      "Epoch 341, Training Loss: 2.2440261840820312, Validation Loss: 2.2431252002716064\n",
      "Epoch 342, Training Loss: 2.2440261840820312, Validation Loss: 2.2431275844573975\n",
      "Epoch 343, Training Loss: 2.244025945663452, Validation Loss: 2.2431304454803467\n",
      "Epoch 344, Training Loss: 2.244025707244873, Validation Loss: 2.243133306503296\n",
      "Epoch 345, Training Loss: 2.244025468826294, Validation Loss: 2.243135929107666\n",
      "Epoch 346, Training Loss: 2.244025230407715, Validation Loss: 2.2431387901306152\n",
      "Epoch 347, Training Loss: 2.2440249919891357, Validation Loss: 2.2431411743164062\n",
      "Epoch 348, Training Loss: 2.2440247535705566, Validation Loss: 2.2431435585021973\n",
      "Epoch 349, Training Loss: 2.2440247535705566, Validation Loss: 2.2431464195251465\n",
      "Epoch 350, Training Loss: 2.2440245151519775, Validation Loss: 2.2431490421295166\n",
      "Epoch 351, Training Loss: 2.2440242767333984, Validation Loss: 2.2431514263153076\n",
      "Epoch 352, Training Loss: 2.2440240383148193, Validation Loss: 2.2431538105010986\n",
      "Epoch 353, Training Loss: 2.2440237998962402, Validation Loss: 2.2431564331054688\n",
      "Epoch 354, Training Loss: 2.2440237998962402, Validation Loss: 2.2431588172912598\n",
      "Epoch 355, Training Loss: 2.244023561477661, Validation Loss: 2.243161201477051\n",
      "Epoch 356, Training Loss: 2.244023561477661, Validation Loss: 2.243163585662842\n",
      "Epoch 357, Training Loss: 2.244023561477661, Validation Loss: 2.243165969848633\n",
      "Epoch 358, Training Loss: 2.244023323059082, Validation Loss: 2.2431681156158447\n",
      "Epoch 359, Training Loss: 2.244023084640503, Validation Loss: 2.2431704998016357\n",
      "Epoch 360, Training Loss: 2.244023084640503, Validation Loss: 2.2431724071502686\n",
      "Epoch 361, Training Loss: 2.244022846221924, Validation Loss: 2.2431750297546387\n",
      "Epoch 362, Training Loss: 2.2440226078033447, Validation Loss: 2.2431769371032715\n",
      "Epoch 363, Training Loss: 2.2440223693847656, Validation Loss: 2.2431793212890625\n",
      "Epoch 364, Training Loss: 2.2440221309661865, Validation Loss: 2.2431812286376953\n",
      "Epoch 365, Training Loss: 2.2440221309661865, Validation Loss: 2.2431836128234863\n",
      "Epoch 366, Training Loss: 2.2440218925476074, Validation Loss: 2.243185520172119\n",
      "Epoch 367, Training Loss: 2.2440216541290283, Validation Loss: 2.243187665939331\n",
      "Epoch 368, Training Loss: 2.2440216541290283, Validation Loss: 2.243189573287964\n",
      "Epoch 369, Training Loss: 2.2440216541290283, Validation Loss: 2.2431914806365967\n",
      "Epoch 370, Training Loss: 2.24402117729187, Validation Loss: 2.2431933879852295\n",
      "Epoch 371, Training Loss: 2.24402117729187, Validation Loss: 2.2431957721710205\n",
      "Epoch 372, Training Loss: 2.24402117729187, Validation Loss: 2.243197441101074\n",
      "Epoch 373, Training Loss: 2.24402117729187, Validation Loss: 2.243199586868286\n",
      "Epoch 374, Training Loss: 2.244020938873291, Validation Loss: 2.243201494216919\n",
      "Epoch 375, Training Loss: 2.244020938873291, Validation Loss: 2.2432031631469727\n",
      "Epoch 376, Training Loss: 2.244020938873291, Validation Loss: 2.2432050704956055\n",
      "Epoch 377, Training Loss: 2.244020462036133, Validation Loss: 2.2432069778442383\n",
      "Epoch 378, Training Loss: 2.244020462036133, Validation Loss: 2.243208646774292\n",
      "Epoch 379, Training Loss: 2.2440202236175537, Validation Loss: 2.243210554122925\n",
      "Epoch 380, Training Loss: 2.244020462036133, Validation Loss: 2.2432124614715576\n",
      "Epoch 381, Training Loss: 2.2440199851989746, Validation Loss: 2.2432138919830322\n",
      "Epoch 382, Training Loss: 2.2440199851989746, Validation Loss: 2.243216037750244\n",
      "Epoch 383, Training Loss: 2.2440199851989746, Validation Loss: 2.2432174682617188\n",
      "Epoch 384, Training Loss: 2.2440199851989746, Validation Loss: 2.2432191371917725\n",
      "Epoch 385, Training Loss: 2.2440195083618164, Validation Loss: 2.243220806121826\n",
      "Epoch 386, Training Loss: 2.2440195083618164, Validation Loss: 2.24322247505188\n",
      "Epoch 387, Training Loss: 2.2440195083618164, Validation Loss: 2.2432241439819336\n",
      "Epoch 388, Training Loss: 2.2440192699432373, Validation Loss: 2.2432260513305664\n",
      "Epoch 389, Training Loss: 2.244019031524658, Validation Loss: 2.243227481842041\n",
      "Epoch 390, Training Loss: 2.244019031524658, Validation Loss: 2.2432291507720947\n",
      "Epoch 391, Training Loss: 2.244019031524658, Validation Loss: 2.2432305812835693\n",
      "Epoch 392, Training Loss: 2.244019031524658, Validation Loss: 2.243232011795044\n",
      "Epoch 393, Training Loss: 2.244018793106079, Validation Loss: 2.2432334423065186\n",
      "Epoch 394, Training Loss: 2.244018793106079, Validation Loss: 2.2432351112365723\n",
      "Epoch 395, Training Loss: 2.244018793106079, Validation Loss: 2.243236541748047\n",
      "Epoch 396, Training Loss: 2.244018793106079, Validation Loss: 2.2432382106781006\n",
      "Epoch 397, Training Loss: 2.244018793106079, Validation Loss: 2.243239641189575\n",
      "Epoch 398, Training Loss: 2.2440185546875, Validation Loss: 2.243241310119629\n",
      "Epoch 399, Training Loss: 2.244018316268921, Validation Loss: 2.2432427406311035\n",
      "Epoch 400, Training Loss: 2.244018316268921, Validation Loss: 2.243243932723999\n",
      "Epoch 401, Training Loss: 2.244018316268921, Validation Loss: 2.2432451248168945\n",
      "Epoch 402, Training Loss: 2.244018077850342, Validation Loss: 2.2432470321655273\n",
      "Epoch 403, Training Loss: 2.244018077850342, Validation Loss: 2.243248224258423\n",
      "Epoch 404, Training Loss: 2.244018077850342, Validation Loss: 2.2432496547698975\n",
      "Epoch 405, Training Loss: 2.2440178394317627, Validation Loss: 2.243251085281372\n",
      "Epoch 406, Training Loss: 2.2440178394317627, Validation Loss: 2.2432522773742676\n",
      "Epoch 407, Training Loss: 2.2440178394317627, Validation Loss: 2.243253707885742\n",
      "Epoch 408, Training Loss: 2.2440178394317627, Validation Loss: 2.2432548999786377\n",
      "Epoch 409, Training Loss: 2.2440173625946045, Validation Loss: 2.2432565689086914\n",
      "Epoch 410, Training Loss: 2.2440173625946045, Validation Loss: 2.243257522583008\n",
      "Epoch 411, Training Loss: 2.2440173625946045, Validation Loss: 2.2432589530944824\n",
      "Epoch 412, Training Loss: 2.2440173625946045, Validation Loss: 2.243260145187378\n",
      "Epoch 413, Training Loss: 2.2440173625946045, Validation Loss: 2.2432613372802734\n",
      "Epoch 414, Training Loss: 2.2440173625946045, Validation Loss: 2.243262529373169\n",
      "Epoch 415, Training Loss: 2.2440171241760254, Validation Loss: 2.2432637214660645\n",
      "Epoch 416, Training Loss: 2.2440171241760254, Validation Loss: 2.24326491355896\n",
      "Epoch 417, Training Loss: 2.2440168857574463, Validation Loss: 2.2432661056518555\n",
      "Epoch 418, Training Loss: 2.2440168857574463, Validation Loss: 2.24326753616333\n",
      "Epoch 419, Training Loss: 2.2440168857574463, Validation Loss: 2.2432687282562256\n",
      "Epoch 420, Training Loss: 2.2440168857574463, Validation Loss: 2.243269681930542\n",
      "Epoch 421, Training Loss: 2.244016647338867, Validation Loss: 2.2432706356048584\n",
      "Epoch 422, Training Loss: 2.244016647338867, Validation Loss: 2.243272066116333\n",
      "Epoch 423, Training Loss: 2.244016647338867, Validation Loss: 2.2432730197906494\n",
      "Epoch 424, Training Loss: 2.244016647338867, Validation Loss: 2.243273973464966\n",
      "Epoch 425, Training Loss: 2.244016647338867, Validation Loss: 2.2432751655578613\n",
      "Epoch 426, Training Loss: 2.244016408920288, Validation Loss: 2.243276357650757\n",
      "Epoch 427, Training Loss: 2.244016408920288, Validation Loss: 2.2432775497436523\n",
      "Epoch 428, Training Loss: 2.244016408920288, Validation Loss: 2.2432785034179688\n",
      "Epoch 429, Training Loss: 2.244016408920288, Validation Loss: 2.243279457092285\n",
      "Epoch 430, Training Loss: 2.244016408920288, Validation Loss: 2.2432806491851807\n",
      "Epoch 431, Training Loss: 2.244016408920288, Validation Loss: 2.243281602859497\n",
      "Epoch 432, Training Loss: 2.244016408920288, Validation Loss: 2.2432823181152344\n",
      "Epoch 433, Training Loss: 2.244016170501709, Validation Loss: 2.24328351020813\n",
      "Epoch 434, Training Loss: 2.244016170501709, Validation Loss: 2.2432844638824463\n",
      "Epoch 435, Training Loss: 2.244016170501709, Validation Loss: 2.2432851791381836\n",
      "Epoch 436, Training Loss: 2.244016170501709, Validation Loss: 2.2432861328125\n",
      "Epoch 437, Training Loss: 2.244016170501709, Validation Loss: 2.2432873249053955\n",
      "Epoch 438, Training Loss: 2.244016170501709, Validation Loss: 2.243288278579712\n",
      "Epoch 439, Training Loss: 2.244016170501709, Validation Loss: 2.2432892322540283\n",
      "Epoch 440, Training Loss: 2.24401593208313, Validation Loss: 2.243290424346924\n",
      "Epoch 441, Training Loss: 2.24401593208313, Validation Loss: 2.243291139602661\n",
      "Epoch 442, Training Loss: 2.244015693664551, Validation Loss: 2.2432918548583984\n",
      "Epoch 443, Training Loss: 2.244015693664551, Validation Loss: 2.243292808532715\n",
      "Epoch 444, Training Loss: 2.244015693664551, Validation Loss: 2.2432937622070312\n",
      "Epoch 445, Training Loss: 2.244015693664551, Validation Loss: 2.2432944774627686\n",
      "Epoch 446, Training Loss: 2.244015693664551, Validation Loss: 2.243295431137085\n",
      "Epoch 447, Training Loss: 2.244015693664551, Validation Loss: 2.2432963848114014\n",
      "Epoch 448, Training Loss: 2.244015693664551, Validation Loss: 2.2432971000671387\n",
      "Epoch 449, Training Loss: 2.244015693664551, Validation Loss: 2.243298053741455\n",
      "Epoch 450, Training Loss: 2.2440154552459717, Validation Loss: 2.2432987689971924\n",
      "Epoch 451, Training Loss: 2.2440154552459717, Validation Loss: 2.2432994842529297\n",
      "Epoch 452, Training Loss: 2.2440152168273926, Validation Loss: 2.243300437927246\n",
      "Epoch 453, Training Loss: 2.2440152168273926, Validation Loss: 2.2433011531829834\n",
      "Epoch 454, Training Loss: 2.2440152168273926, Validation Loss: 2.2433018684387207\n",
      "Epoch 455, Training Loss: 2.2440152168273926, Validation Loss: 2.243302822113037\n",
      "Epoch 456, Training Loss: 2.2440152168273926, Validation Loss: 2.2433035373687744\n",
      "Epoch 457, Training Loss: 2.2440152168273926, Validation Loss: 2.2433042526245117\n",
      "Epoch 458, Training Loss: 2.2440152168273926, Validation Loss: 2.243304967880249\n",
      "Epoch 459, Training Loss: 2.2440152168273926, Validation Loss: 2.2433056831359863\n",
      "Epoch 460, Training Loss: 2.2440152168273926, Validation Loss: 2.2433063983917236\n",
      "Epoch 461, Training Loss: 2.2440149784088135, Validation Loss: 2.24330735206604\n",
      "Epoch 462, Training Loss: 2.2440149784088135, Validation Loss: 2.2433078289031982\n",
      "Epoch 463, Training Loss: 2.2440149784088135, Validation Loss: 2.2433085441589355\n",
      "Epoch 464, Training Loss: 2.2440149784088135, Validation Loss: 2.243309259414673\n",
      "Epoch 465, Training Loss: 2.2440149784088135, Validation Loss: 2.24330997467041\n",
      "Epoch 466, Training Loss: 2.2440149784088135, Validation Loss: 2.2433106899261475\n",
      "Epoch 467, Training Loss: 2.2440149784088135, Validation Loss: 2.2433111667633057\n",
      "Epoch 468, Training Loss: 2.2440149784088135, Validation Loss: 2.243311643600464\n",
      "Epoch 469, Training Loss: 2.2440147399902344, Validation Loss: 2.2433125972747803\n",
      "Epoch 470, Training Loss: 2.2440147399902344, Validation Loss: 2.2433130741119385\n",
      "Epoch 471, Training Loss: 2.2440147399902344, Validation Loss: 2.2433135509490967\n",
      "Epoch 472, Training Loss: 2.2440147399902344, Validation Loss: 2.243314504623413\n",
      "Epoch 473, Training Loss: 2.2440147399902344, Validation Loss: 2.2433149814605713\n",
      "Epoch 474, Training Loss: 2.2440147399902344, Validation Loss: 2.2433154582977295\n",
      "Epoch 475, Training Loss: 2.2440147399902344, Validation Loss: 2.243316173553467\n",
      "Epoch 476, Training Loss: 2.2440147399902344, Validation Loss: 2.243316888809204\n",
      "Epoch 477, Training Loss: 2.2440145015716553, Validation Loss: 2.2433176040649414\n",
      "Epoch 478, Training Loss: 2.2440145015716553, Validation Loss: 2.2433180809020996\n",
      "Epoch 479, Training Loss: 2.2440147399902344, Validation Loss: 2.243318557739258\n",
      "Epoch 480, Training Loss: 2.2440145015716553, Validation Loss: 2.243319034576416\n",
      "Epoch 481, Training Loss: 2.244014263153076, Validation Loss: 2.2433197498321533\n",
      "Epoch 482, Training Loss: 2.2440147399902344, Validation Loss: 2.2433202266693115\n",
      "Epoch 483, Training Loss: 2.244014263153076, Validation Loss: 2.243320941925049\n",
      "Epoch 484, Training Loss: 2.2440145015716553, Validation Loss: 2.243321418762207\n",
      "Epoch 485, Training Loss: 2.2440145015716553, Validation Loss: 2.2433218955993652\n",
      "Epoch 486, Training Loss: 2.2440145015716553, Validation Loss: 2.2433223724365234\n",
      "Epoch 487, Training Loss: 2.244014263153076, Validation Loss: 2.2433228492736816\n",
      "Epoch 488, Training Loss: 2.2440145015716553, Validation Loss: 2.24332332611084\n",
      "Epoch 489, Training Loss: 2.244014263153076, Validation Loss: 2.243323802947998\n",
      "Epoch 490, Training Loss: 2.244014263153076, Validation Loss: 2.2433245182037354\n",
      "Epoch 491, Training Loss: 2.244014263153076, Validation Loss: 2.2433247566223145\n",
      "Epoch 492, Training Loss: 2.244014263153076, Validation Loss: 2.2433252334594727\n",
      "Epoch 493, Training Loss: 2.244014263153076, Validation Loss: 2.243325710296631\n",
      "Epoch 494, Training Loss: 2.244014263153076, Validation Loss: 2.243326187133789\n",
      "Epoch 495, Training Loss: 2.244014263153076, Validation Loss: 2.2433266639709473\n",
      "Epoch 496, Training Loss: 2.244014263153076, Validation Loss: 2.2433271408081055\n",
      "Epoch 497, Training Loss: 2.244014263153076, Validation Loss: 2.2433276176452637\n",
      "Epoch 498, Training Loss: 2.244014263153076, Validation Loss: 2.243328332901001\n",
      "Epoch 499, Training Loss: 2.244014263153076, Validation Loss: 2.243328332901001\n",
      "Epoch 500, Training Loss: 2.244014263153076, Validation Loss: 2.2433290481567383\n",
      "Epoch 501, Training Loss: 2.244014263153076, Validation Loss: 2.2433292865753174\n",
      "Epoch 502, Training Loss: 2.244014024734497, Validation Loss: 2.2433300018310547\n",
      "Epoch 503, Training Loss: 2.244014024734497, Validation Loss: 2.243330240249634\n",
      "Epoch 504, Training Loss: 2.244014263153076, Validation Loss: 2.243330717086792\n",
      "Epoch 505, Training Loss: 2.244014024734497, Validation Loss: 2.243330955505371\n",
      "Epoch 506, Training Loss: 2.244014024734497, Validation Loss: 2.2433314323425293\n",
      "Epoch 507, Training Loss: 2.244014024734497, Validation Loss: 2.2433319091796875\n",
      "Epoch 508, Training Loss: 2.244014024734497, Validation Loss: 2.2433321475982666\n",
      "Epoch 509, Training Loss: 2.244014024734497, Validation Loss: 2.243332624435425\n",
      "Epoch 510, Training Loss: 2.244014024734497, Validation Loss: 2.243333101272583\n",
      "Epoch 511, Training Loss: 2.244014024734497, Validation Loss: 2.243333339691162\n",
      "Epoch 512, Training Loss: 2.244014024734497, Validation Loss: 2.243333578109741\n",
      "Epoch 513, Training Loss: 2.244014024734497, Validation Loss: 2.2433340549468994\n",
      "Epoch 514, Training Loss: 2.244014024734497, Validation Loss: 2.2433342933654785\n",
      "Epoch 515, Training Loss: 2.244014024734497, Validation Loss: 2.2433345317840576\n",
      "Epoch 516, Training Loss: 2.244014024734497, Validation Loss: 2.243335008621216\n",
      "Epoch 517, Training Loss: 2.244014024734497, Validation Loss: 2.243335247039795\n",
      "Epoch 518, Training Loss: 2.244014024734497, Validation Loss: 2.243335485458374\n",
      "Epoch 519, Training Loss: 2.244014024734497, Validation Loss: 2.2433359622955322\n",
      "Epoch 520, Training Loss: 2.244014024734497, Validation Loss: 2.2433362007141113\n",
      "Epoch 521, Training Loss: 2.244014024734497, Validation Loss: 2.2433366775512695\n",
      "Epoch 522, Training Loss: 2.244014024734497, Validation Loss: 2.2433369159698486\n",
      "Epoch 523, Training Loss: 2.244014024734497, Validation Loss: 2.243337392807007\n",
      "Epoch 524, Training Loss: 2.244014024734497, Validation Loss: 2.243337869644165\n",
      "Epoch 525, Training Loss: 2.244014024734497, Validation Loss: 2.243338108062744\n",
      "Epoch 526, Training Loss: 2.244014024734497, Validation Loss: 2.2433385848999023\n",
      "Epoch 527, Training Loss: 2.244013786315918, Validation Loss: 2.2433385848999023\n",
      "Epoch 528, Training Loss: 2.244014024734497, Validation Loss: 2.2433390617370605\n",
      "Epoch 529, Training Loss: 2.244014024734497, Validation Loss: 2.2433390617370605\n",
      "Epoch 530, Training Loss: 2.244014024734497, Validation Loss: 2.2433395385742188\n",
      "Epoch 531, Training Loss: 2.244014024734497, Validation Loss: 2.2433395385742188\n",
      "Epoch 532, Training Loss: 2.244014024734497, Validation Loss: 2.243340015411377\n",
      "Epoch 533, Training Loss: 2.244014024734497, Validation Loss: 2.243340015411377\n",
      "Epoch 534, Training Loss: 2.244014024734497, Validation Loss: 2.2433407306671143\n",
      "Epoch 535, Training Loss: 2.244014024734497, Validation Loss: 2.2433409690856934\n",
      "Epoch 536, Training Loss: 2.244013786315918, Validation Loss: 2.2433412075042725\n",
      "Epoch 537, Training Loss: 2.244014024734497, Validation Loss: 2.2433412075042725\n",
      "Epoch 538, Training Loss: 2.244014024734497, Validation Loss: 2.2433414459228516\n",
      "Epoch 539, Training Loss: 2.244013786315918, Validation Loss: 2.2433416843414307\n",
      "Epoch 540, Training Loss: 2.244013786315918, Validation Loss: 2.2433419227600098\n",
      "Epoch 541, Training Loss: 2.244013786315918, Validation Loss: 2.243342399597168\n",
      "Epoch 542, Training Loss: 2.244013786315918, Validation Loss: 2.243342399597168\n",
      "Epoch 543, Training Loss: 2.244013786315918, Validation Loss: 2.243342638015747\n",
      "Epoch 544, Training Loss: 2.244013786315918, Validation Loss: 2.243342876434326\n",
      "Epoch 545, Training Loss: 2.244013547897339, Validation Loss: 2.2433431148529053\n",
      "Epoch 546, Training Loss: 2.244013786315918, Validation Loss: 2.2433433532714844\n",
      "Epoch 547, Training Loss: 2.244013547897339, Validation Loss: 2.2433433532714844\n",
      "Epoch 548, Training Loss: 2.244013547897339, Validation Loss: 2.2433438301086426\n",
      "Epoch 549, Training Loss: 2.244013547897339, Validation Loss: 2.2433440685272217\n",
      "Epoch 550, Training Loss: 2.244013547897339, Validation Loss: 2.243344306945801\n",
      "Epoch 551, Training Loss: 2.244013547897339, Validation Loss: 2.243344306945801\n",
      "Epoch 552, Training Loss: 2.244013547897339, Validation Loss: 2.24334454536438\n",
      "Epoch 553, Training Loss: 2.244013786315918, Validation Loss: 2.243345022201538\n",
      "Epoch 554, Training Loss: 2.244013547897339, Validation Loss: 2.243345022201538\n",
      "Epoch 555, Training Loss: 2.244013547897339, Validation Loss: 2.243345260620117\n",
      "Epoch 556, Training Loss: 2.244013547897339, Validation Loss: 2.243345260620117\n",
      "Epoch 557, Training Loss: 2.244013547897339, Validation Loss: 2.2433454990386963\n",
      "Epoch 558, Training Loss: 2.244013786315918, Validation Loss: 2.2433457374572754\n",
      "Epoch 559, Training Loss: 2.244013786315918, Validation Loss: 2.2433457374572754\n",
      "Epoch 560, Training Loss: 2.244013547897339, Validation Loss: 2.2433459758758545\n",
      "Epoch 561, Training Loss: 2.244013547897339, Validation Loss: 2.2433462142944336\n",
      "Epoch 562, Training Loss: 2.244013547897339, Validation Loss: 2.2433464527130127\n",
      "Epoch 563, Training Loss: 2.244013547897339, Validation Loss: 2.243346691131592\n",
      "Epoch 564, Training Loss: 2.244013547897339, Validation Loss: 2.243346691131592\n",
      "Epoch 565, Training Loss: 2.244013547897339, Validation Loss: 2.243346929550171\n",
      "Epoch 566, Training Loss: 2.244013547897339, Validation Loss: 2.24334716796875\n",
      "Epoch 567, Training Loss: 2.244013547897339, Validation Loss: 2.24334716796875\n",
      "Epoch 568, Training Loss: 2.244013547897339, Validation Loss: 2.243347644805908\n",
      "Epoch 569, Training Loss: 2.244013547897339, Validation Loss: 2.243347644805908\n",
      "Epoch 570, Training Loss: 2.244013547897339, Validation Loss: 2.243347644805908\n",
      "Epoch 571, Training Loss: 2.244013547897339, Validation Loss: 2.2433481216430664\n",
      "Epoch 572, Training Loss: 2.244013547897339, Validation Loss: 2.2433481216430664\n",
      "Epoch 573, Training Loss: 2.244013547897339, Validation Loss: 2.2433483600616455\n",
      "Epoch 574, Training Loss: 2.244013547897339, Validation Loss: 2.2433483600616455\n",
      "Epoch 575, Training Loss: 2.244013547897339, Validation Loss: 2.2433488368988037\n",
      "Epoch 576, Training Loss: 2.244013547897339, Validation Loss: 2.2433488368988037\n",
      "Epoch 577, Training Loss: 2.244013547897339, Validation Loss: 2.2433488368988037\n",
      "Epoch 578, Training Loss: 2.244013547897339, Validation Loss: 2.243349075317383\n",
      "Epoch 579, Training Loss: 2.244013547897339, Validation Loss: 2.243349075317383\n",
      "Epoch 580, Training Loss: 2.244013547897339, Validation Loss: 2.243349313735962\n",
      "Epoch 581, Training Loss: 2.244013547897339, Validation Loss: 2.243349313735962\n",
      "Epoch 582, Training Loss: 2.2440133094787598, Validation Loss: 2.24334979057312\n",
      "Epoch 583, Training Loss: 2.2440133094787598, Validation Loss: 2.24334979057312\n",
      "Epoch 584, Training Loss: 2.2440133094787598, Validation Loss: 2.24334979057312\n",
      "Epoch 585, Training Loss: 2.244013547897339, Validation Loss: 2.24334979057312\n",
      "Epoch 586, Training Loss: 2.244013547897339, Validation Loss: 2.2433502674102783\n",
      "Epoch 587, Training Loss: 2.244013547897339, Validation Loss: 2.2433502674102783\n",
      "Epoch 588, Training Loss: 2.244013547897339, Validation Loss: 2.2433502674102783\n",
      "Epoch 589, Training Loss: 2.244013547897339, Validation Loss: 2.2433502674102783\n",
      "Epoch 590, Training Loss: 2.2440133094787598, Validation Loss: 2.2433505058288574\n",
      "Epoch 591, Training Loss: 2.244013547897339, Validation Loss: 2.2433507442474365\n",
      "Epoch 592, Training Loss: 2.244013547897339, Validation Loss: 2.2433507442474365\n",
      "Epoch 593, Training Loss: 2.2440133094787598, Validation Loss: 2.2433507442474365\n",
      "Epoch 594, Training Loss: 2.244013547897339, Validation Loss: 2.2433509826660156\n",
      "Epoch 595, Training Loss: 2.2440133094787598, Validation Loss: 2.2433512210845947\n",
      "Epoch 596, Training Loss: 2.2440133094787598, Validation Loss: 2.2433512210845947\n",
      "Epoch 597, Training Loss: 2.2440133094787598, Validation Loss: 2.2433512210845947\n",
      "Epoch 598, Training Loss: 2.2440133094787598, Validation Loss: 2.2433512210845947\n",
      "Epoch 599, Training Loss: 2.2440133094787598, Validation Loss: 2.243351459503174\n",
      "Epoch 600, Training Loss: 2.2440133094787598, Validation Loss: 2.243351697921753\n",
      "Epoch 601, Training Loss: 2.2440133094787598, Validation Loss: 2.243351697921753\n",
      "Epoch 602, Training Loss: 2.2440133094787598, Validation Loss: 2.243351697921753\n",
      "Epoch 603, Training Loss: 2.244013547897339, Validation Loss: 2.243351697921753\n",
      "Epoch 604, Training Loss: 2.244013547897339, Validation Loss: 2.243351697921753\n",
      "Epoch 605, Training Loss: 2.2440133094787598, Validation Loss: 2.243351936340332\n",
      "Epoch 606, Training Loss: 2.2440133094787598, Validation Loss: 2.243351936340332\n",
      "Epoch 607, Training Loss: 2.2440133094787598, Validation Loss: 2.243352174758911\n",
      "Epoch 608, Training Loss: 2.2440133094787598, Validation Loss: 2.243352174758911\n",
      "Epoch 609, Training Loss: 2.244013547897339, Validation Loss: 2.243352174758911\n",
      "Epoch 610, Training Loss: 2.2440130710601807, Validation Loss: 2.2433526515960693\n",
      "Epoch 611, Training Loss: 2.2440130710601807, Validation Loss: 2.2433524131774902\n",
      "Epoch 612, Training Loss: 2.2440130710601807, Validation Loss: 2.2433526515960693\n",
      "Epoch 613, Training Loss: 2.2440133094787598, Validation Loss: 2.2433526515960693\n",
      "Epoch 614, Training Loss: 2.2440130710601807, Validation Loss: 2.2433526515960693\n",
      "Epoch 615, Training Loss: 2.2440130710601807, Validation Loss: 2.2433526515960693\n",
      "Epoch 616, Training Loss: 2.2440133094787598, Validation Loss: 2.2433528900146484\n",
      "Epoch 617, Training Loss: 2.2440130710601807, Validation Loss: 2.2433526515960693\n",
      "Epoch 618, Training Loss: 2.2440133094787598, Validation Loss: 2.2433531284332275\n",
      "Epoch 619, Training Loss: 2.2440130710601807, Validation Loss: 2.2433531284332275\n",
      "Epoch 620, Training Loss: 2.2440133094787598, Validation Loss: 2.2433531284332275\n",
      "Epoch 621, Training Loss: 2.2440130710601807, Validation Loss: 2.2433531284332275\n",
      "Epoch 622, Training Loss: 2.2440133094787598, Validation Loss: 2.2433531284332275\n",
      "Epoch 623, Training Loss: 2.2440133094787598, Validation Loss: 2.2433531284332275\n",
      "Epoch 624, Training Loss: 2.2440130710601807, Validation Loss: 2.2433531284332275\n",
      "Epoch 625, Training Loss: 2.2440133094787598, Validation Loss: 2.2433533668518066\n",
      "Epoch 626, Training Loss: 2.2440133094787598, Validation Loss: 2.2433533668518066\n",
      "Epoch 627, Training Loss: 2.2440133094787598, Validation Loss: 2.2433536052703857\n",
      "Epoch 628, Training Loss: 2.2440130710601807, Validation Loss: 2.2433536052703857\n",
      "Epoch 629, Training Loss: 2.2440130710601807, Validation Loss: 2.2433536052703857\n",
      "Epoch 630, Training Loss: 2.2440130710601807, Validation Loss: 2.2433536052703857\n",
      "Epoch 631, Training Loss: 2.2440130710601807, Validation Loss: 2.2433536052703857\n",
      "Epoch 632, Training Loss: 2.2440133094787598, Validation Loss: 2.2433536052703857\n",
      "Epoch 633, Training Loss: 2.2440130710601807, Validation Loss: 2.2433536052703857\n",
      "Epoch 634, Training Loss: 2.2440130710601807, Validation Loss: 2.243353843688965\n",
      "Epoch 635, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 636, Training Loss: 2.2440130710601807, Validation Loss: 2.243353843688965\n",
      "Epoch 637, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 638, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 639, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 640, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 641, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 642, Training Loss: 2.2440130710601807, Validation Loss: 2.243354082107544\n",
      "Epoch 643, Training Loss: 2.2440130710601807, Validation Loss: 2.243354320526123\n",
      "Epoch 644, Training Loss: 2.2440130710601807, Validation Loss: 2.243354320526123\n",
      "Epoch 645, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 646, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 647, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 648, Training Loss: 2.2440130710601807, Validation Loss: 2.243354320526123\n",
      "Epoch 649, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 650, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 651, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 652, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 653, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 654, Training Loss: 2.2440130710601807, Validation Loss: 2.243354558944702\n",
      "Epoch 655, Training Loss: 2.2440130710601807, Validation Loss: 2.2433547973632812\n",
      "Epoch 656, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 657, Training Loss: 2.2440130710601807, Validation Loss: 2.2433547973632812\n",
      "Epoch 658, Training Loss: 2.2440130710601807, Validation Loss: 2.2433547973632812\n",
      "Epoch 659, Training Loss: 2.2440130710601807, Validation Loss: 2.2433547973632812\n",
      "Epoch 660, Training Loss: 2.2440130710601807, Validation Loss: 2.2433547973632812\n",
      "Epoch 661, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 662, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 663, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 664, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 665, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 666, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 667, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 668, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 669, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 670, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 671, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 672, Training Loss: 2.2440130710601807, Validation Loss: 2.2433552742004395\n",
      "Epoch 673, Training Loss: 2.2440130710601807, Validation Loss: 2.2433552742004395\n",
      "Epoch 674, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 675, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 676, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 677, Training Loss: 2.2440130710601807, Validation Loss: 2.2433550357818604\n",
      "Epoch 678, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 679, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 680, Training Loss: 2.2440130710601807, Validation Loss: 2.2433552742004395\n",
      "Epoch 681, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 682, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 683, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 684, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 685, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 686, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 687, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 688, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 689, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 690, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 691, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 692, Training Loss: 2.2440128326416016, Validation Loss: 2.2433555126190186\n",
      "Epoch 693, Training Loss: 2.2440130710601807, Validation Loss: 2.2433557510375977\n",
      "Epoch 694, Training Loss: 2.2440130710601807, Validation Loss: 2.2433555126190186\n",
      "Epoch 695, Training Loss: 2.2440130710601807, Validation Loss: 2.2433559894561768\n",
      "Epoch 696, Training Loss: 2.2440125942230225, Validation Loss: 2.2433559894561768\n",
      "Epoch 697, Training Loss: 2.2440130710601807, Validation Loss: 2.2433557510375977\n",
      "Epoch 698, Training Loss: 2.2440130710601807, Validation Loss: 2.2433557510375977\n",
      "Epoch 699, Training Loss: 2.2440128326416016, Validation Loss: 2.2433557510375977\n",
      "Epoch 700, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 701, Training Loss: 2.2440128326416016, Validation Loss: 2.2433557510375977\n",
      "Epoch 702, Training Loss: 2.2440125942230225, Validation Loss: 2.2433557510375977\n",
      "Epoch 703, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 704, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 705, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 706, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 707, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 708, Training Loss: 2.2440125942230225, Validation Loss: 2.2433559894561768\n",
      "Epoch 709, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 710, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 711, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 712, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 713, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 714, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 715, Training Loss: 2.2440130710601807, Validation Loss: 2.2433559894561768\n",
      "Epoch 716, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 717, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 718, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 719, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 720, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 721, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 722, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 723, Training Loss: 2.2440128326416016, Validation Loss: 2.243356227874756\n",
      "Epoch 724, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 725, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 726, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 727, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 728, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 729, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 730, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 731, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 732, Training Loss: 2.2440128326416016, Validation Loss: 2.243356227874756\n",
      "Epoch 733, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 734, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 735, Training Loss: 2.2440128326416016, Validation Loss: 2.243356227874756\n",
      "Epoch 736, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 737, Training Loss: 2.2440128326416016, Validation Loss: 2.243356227874756\n",
      "Epoch 738, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 739, Training Loss: 2.2440125942230225, Validation Loss: 2.2433559894561768\n",
      "Epoch 740, Training Loss: 2.2440128326416016, Validation Loss: 2.243356227874756\n",
      "Epoch 741, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 742, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 743, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 744, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 745, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 746, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 747, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 748, Training Loss: 2.2440125942230225, Validation Loss: 2.2433559894561768\n",
      "Epoch 749, Training Loss: 2.2440128326416016, Validation Loss: 2.2433559894561768\n",
      "Epoch 750, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 751, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 752, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 753, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 754, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 755, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 756, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 757, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 758, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 759, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 760, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 761, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 762, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 763, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 764, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 765, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 766, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 767, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 768, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 769, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 770, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 771, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 772, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 773, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 774, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 775, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 776, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 777, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 778, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 779, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 780, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 781, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 782, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 783, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 784, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 785, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 786, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 787, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 788, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 789, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 790, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 791, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 792, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 793, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 794, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 795, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 796, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 797, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 798, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 799, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 800, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 801, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 802, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 803, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 804, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 805, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 806, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 807, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 808, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 809, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 810, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 811, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 812, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 813, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 814, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 815, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 816, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 817, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 818, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 819, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 820, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 821, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 822, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 823, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 824, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 825, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 826, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 827, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 828, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 829, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 830, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 831, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 832, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 833, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 834, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 835, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 836, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 837, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 838, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 839, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 840, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 841, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 842, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 843, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 844, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 845, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 846, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 847, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 848, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 849, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 850, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 851, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 852, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 853, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 854, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 855, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 856, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 857, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 858, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 859, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 860, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 861, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 862, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 863, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 864, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 865, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 866, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 867, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 868, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 869, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 870, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 871, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 872, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 873, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 874, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 875, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 876, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 877, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 878, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 879, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 880, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 881, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 882, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 883, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 884, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 885, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 886, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 887, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 888, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 889, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 890, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 891, Training Loss: 2.2440125942230225, Validation Loss: 2.243356227874756\n",
      "Epoch 892, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 893, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 894, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 895, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 896, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 897, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 898, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 899, Training Loss: 2.2440125942230225, Validation Loss: 2.243356466293335\n",
      "Epoch 900, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 901, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 902, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 903, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 904, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 905, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 906, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 907, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 908, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 909, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 910, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 911, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 912, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 913, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 914, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 915, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 916, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 917, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 918, Training Loss: 2.2440121173858643, Validation Loss: 2.243356466293335\n",
      "Epoch 919, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 920, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 921, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 922, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 923, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 924, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 925, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 926, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 927, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 928, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 929, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 930, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 931, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 932, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 933, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 934, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 935, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 936, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 937, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 938, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 939, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 940, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 941, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 942, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 943, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 944, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 945, Training Loss: 2.2440121173858643, Validation Loss: 2.243356466293335\n",
      "Epoch 946, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 947, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 948, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 949, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 950, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 951, Training Loss: 2.2440123558044434, Validation Loss: 2.2433559894561768\n",
      "Epoch 952, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 953, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 954, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 955, Training Loss: 2.2440123558044434, Validation Loss: 2.2433559894561768\n",
      "Epoch 956, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 957, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 958, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 959, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 960, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 961, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 962, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 963, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 964, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 965, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 966, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 967, Training Loss: 2.2440121173858643, Validation Loss: 2.2433559894561768\n",
      "Epoch 968, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 969, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 970, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 971, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 972, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 973, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 974, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 975, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 976, Training Loss: 2.2440121173858643, Validation Loss: 2.243356466293335\n",
      "Epoch 977, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 978, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 979, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 980, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 981, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 982, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 983, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 984, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 985, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 986, Training Loss: 2.2440121173858643, Validation Loss: 2.243356227874756\n",
      "Epoch 987, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 988, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 989, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 990, Training Loss: 2.2440121173858643, Validation Loss: 2.243356466293335\n",
      "Epoch 991, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 992, Training Loss: 2.2440121173858643, Validation Loss: 2.243356466293335\n",
      "Epoch 993, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 994, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 995, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 996, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 997, Training Loss: 2.2440121173858643, Validation Loss: 2.243356227874756\n",
      "Epoch 998, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n",
      "Epoch 999, Training Loss: 2.2440123558044434, Validation Loss: 2.243356466293335\n",
      "Epoch 1000, Training Loss: 2.2440123558044434, Validation Loss: 2.243356227874756\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(dml_model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dml_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = dml_model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1+1 ])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    dml_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dml_model(X_user_val,  product_features, all_x_other_products,prices)[0]\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        torch.save(dml_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b27e55e",
   "metadata": {
    "id": "1b27e55e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5efd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3715.34\n",
      "Expected Revenue all treated: $758.80\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "QGABODM51OV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGABODM51OV4",
    "outputId": "3769a33f-2282-4787-e276-3f7d3b56c540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2956.5409545898438"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121cd47",
   "metadata": {
    "id": "1121cd47"
   },
   "source": [
    "# debias the GTE estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "SMTzkngzuUls",
   "metadata": {
    "id": "SMTzkngzuUls"
   },
   "outputs": [],
   "source": [
    "test_prepared_data = prepare_data(X_user_test, X_product,  price*(discount*prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = test_prepared_data\n",
    "\n",
    "# Compute Theta0 and Theta1\n",
    "_,theta0_output,theta1_output = dml_model(user_features, product_features, all_x_other_products,prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de3fecfc-2f03-48e9-a089-5cebf4ac0f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 10])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rag3u55FWjiu",
   "metadata": {
    "id": "Rag3u55FWjiu"
   },
   "source": [
    "# use formulation debias for H_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "SSXrdFP4WnZL",
   "metadata": {
    "id": "SSXrdFP4WnZL"
   },
   "outputs": [],
   "source": [
    "def H_theta(theta0_output,theta1_output,all_treated_price,price):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = all_treated_price.unsqueeze(0).expand(N, M)\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_treated_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    all_treated_probabilities = F.softmax(all_treated_uti, dim=1)\n",
    "    all_control_probabilities = F.softmax(all_control_uti, dim=1)\n",
    "\n",
    "    price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "    treated_price_with_outside =  torch.cat((torch.zeros(1, device=all_treated_price.device),all_treated_price), dim=0)\n",
    "\n",
    "    H = torch.sum(all_treated_probabilities*treated_price_with_outside - all_control_probabilities*price_with_outside,dim=1)\n",
    "    expsum_treated = torch.sum(torch.exp(all_treated_uti),dim=1)\n",
    "    expsum_control = torch.sum(torch.exp(all_control_uti),dim=1)\n",
    "\n",
    "    expsum_treated_expanded = expsum_treated.unsqueeze(1).expand(-1, all_treated_uti.shape[1])  # Shape [N, M+1]\n",
    "    expsum_control_expanded = expsum_control.unsqueeze(1).expand(-1, all_control_uti.shape[1])  # Shape [N, M+1]\n",
    "\n",
    "    H_theta0 = torch.sum((torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded-\\\n",
    "                          torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded)\\\n",
    "                         *price_with_outside,dim=1)\n",
    "    H_theta1 = torch.sum(price_with_outside*(torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded*treated_price_with_outside-\\\n",
    "                                             torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded*price_with_outside),dim=1)\n",
    "\n",
    "\n",
    "    return H,H_theta0,H_theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "BFHZQM3VfBUI",
   "metadata": {
    "id": "BFHZQM3VfBUI"
   },
   "outputs": [],
   "source": [
    "H,H_theta0,H_theta1 = H_theta(theta0_output,theta1_output,all_treated_price,price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "yivIC_MKad5x",
   "metadata": {
    "id": "yivIC_MKad5x"
   },
   "outputs": [],
   "source": [
    "def l_theta(theta0_output,theta1_output,adjusted_price,decision_test):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_adjusted_price = adjusted_price.unsqueeze(0).expand(N, M)\n",
    "    uti = theta0_output + theta1_output * expand_adjusted_price\n",
    "    adjusted_price_with_outside =  torch.cat([torch.zeros(1, device=adjusted_price.device),adjusted_price])\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=uti.device)\n",
    "    uti = torch.cat((zero_utilities,uti), dim=1)\n",
    "\n",
    "    probabilities = F.softmax(uti, dim=1)\n",
    "    prod_indices = torch.ones(NUM_Product, device=device)\n",
    "    prod_indices = torch.cat([torch.zeros(1,device=device),prod_indices])\n",
    "    ltheta0 = probabilities[torch.arange(decision_test.size(0)), decision_test+1] -prod_indices[decision_test+1]\n",
    "    ltheta1 = (probabilities[torch.arange(decision_test.size(0)), decision_test+1] * adjusted_price_with_outside[decision_test+1]) - adjusted_price_with_outside[decision_test+1]\n",
    "\n",
    "\n",
    "    return ltheta0,ltheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77bcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "O8c-tupIgHVu",
   "metadata": {
    "id": "O8c-tupIgHVu"
   },
   "outputs": [],
   "source": [
    "adjusted_price = price*(discount*prod_randomization).to(device)\n",
    "decision_test = decision_test.to(device)\n",
    "ltheta0,ltheta1= l_theta(theta0_output,theta1_output,adjusted_price,decision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "UVRht78QaxSG",
   "metadata": {
    "id": "UVRht78QaxSG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lambdainv(theta0_output, theta1_output, price, decision_test,epsilon =10):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = discount*price.unsqueeze(0).expand(N, M)\n",
    "\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_control_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities_control = F.softmax(all_control_uti, dim=1)\n",
    "    probabilities_treated = F.softmax(all_treated_uti, dim=1)\n",
    "\n",
    "    # Extract probabilities of chosen products\n",
    "    chosen_prob_control = probabilities_control[torch.arange(N), decision_test]\n",
    "    chosen_prob_treated = probabilities_treated[torch.arange(N), decision_test]\n",
    "\n",
    "    # Calculate second derivatives\n",
    "    ltheta00 = -chosen_prob_control * (1 - chosen_prob_control) - chosen_prob_treated * (1 - chosen_prob_treated)\n",
    "    ltheta01 = -chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test] - \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])\n",
    "    ltheta11 = -chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test]**2 - \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])**2\n",
    "    ltheta00=ltheta00/2\n",
    "    ltheta01=ltheta01/2\n",
    "    ltheta11=ltheta11/2\n",
    "\n",
    "    # Form the 2x2 Hessian matrices for each instance\n",
    "    ltheta00 = ltheta00.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta01 = ltheta01.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta11 = ltheta11.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    top_row = torch.cat((ltheta00, ltheta01), dim=2)\n",
    "    bottom_row = torch.cat((ltheta01, ltheta11), dim=2)\n",
    "\n",
    "    L_matrix = torch.cat((top_row, bottom_row), dim=1)\n",
    "\n",
    "    # Regularization and inversion\n",
    "    \n",
    "    identity_matrix = torch.eye(2, dtype=L_matrix.dtype, device=L_matrix.device) * epsilon\n",
    "    L_matrix_reg = L_matrix + identity_matrix.unsqueeze(0).unsqueeze(0)\n",
    "    L_inv = torch.linalg.inv(L_matrix_reg)\n",
    "\n",
    "    return L_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6f7441c-eea7-4f98-8631-71cb75d14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.001,0.01,0.1,0.5,1,5,10]\n",
    "min_mape = float('inf')\n",
    "best_epsilon = None\n",
    "best_final_result = None\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    # Update L_inv for the current epsilon\n",
    "    try:\n",
    "        L_inv = lambdainv(theta0_output, theta1_output, price, decision_test, epsilon).float()\n",
    "    \n",
    "        # Calculate final_result with the given epsilon\n",
    "        H_theta_array = torch.stack((H_theta0, H_theta1), dim=-1).unsqueeze(1).float()  \n",
    "        l_theta_array = torch.stack((ltheta0, ltheta1), dim=-1).unsqueeze(-1).float()  \n",
    "    \n",
    "        # Perform matrix multiplications\n",
    "        result_intermediate = torch.matmul(H_theta_array, L_inv.squeeze(0)) \n",
    "        final_result = torch.matmul(result_intermediate, l_theta_array).squeeze(-1)  \n",
    "        final_result[torch.isnan(final_result) | torch.isinf(final_result)] = 0\n",
    "    \n",
    "        # Calculate sdl and dedl\n",
    "        sdl = H.sum().cpu().detach().numpy() * 2\n",
    "        dedl = (H.sum().cpu().detach().numpy() - final_result.sum().cpu().detach().numpy()) * 2\n",
    "    \n",
    "        # Calculate MAPE of dedl with respect to true\n",
    "        mape_dedl = np.abs((dedl - true) / true)\n",
    "    \n",
    "        # Update best_epsilon if the current epsilon yields a lower MAPE\n",
    "        if mape_dedl < min_mape:\n",
    "            min_mape = mape_dedl\n",
    "            best_epsilon = epsilon\n",
    "            best_final_result = final_result\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "q11HQu-goWM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11HQu-goWM0",
    "outputId": "ecf71629-8dbc-4d80-8d4a-de1f2534e3eb"
   },
   "outputs": [],
   "source": [
    "sdl = H.sum().cpu().detach().numpy()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3a445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedl = (H.sum().cpu().detach().numpy()-best_final_result.sum().cpu().detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c6d8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5913.08203125, -5905.2841796875, 5)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl,dedl,best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9d44fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of SDL:  -0.15%\n",
      "Absolute Percentage Estimation Error of SP MNL:  -0.01%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of SDL:  {100*np.abs(sdl-revenue_difference)/revenue_difference:.2f}%\")\n",
    "print(f\"Absolute Percentage Estimation Error of SP MNL:  {100*np.abs(dedl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eeea3d7d-d20e-48c8-b037-107fce171386",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pe = (naive - true) / true\n",
    "linear_pe = (linear - true) / true\n",
    "pdl_pe = (pdl - true) / true\n",
    "sdl_pe = (sdl - true) / true\n",
    "dedl_pe = (dedl - true) / true\n",
    "naive_mse = (naive - true)**2\n",
    "linear_mse =(linear - true)**2\n",
    "pdl_mse = (pdl - true)**2\n",
    "sdl_mse = (sdl - true)**2\n",
    "dedl_mse = (dedl - true)**2\n",
    "naive_e = (naive - true)\n",
    "linear_e =(linear - true)\n",
    "pdl_e = (pdl - true)\n",
    "sdl_e = (sdl - true)\n",
    "dedl_e = (dedl - true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1094fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6562964572817541 -0.0013449696870614395 -0.008908661759576532 0.0014568786134413194 0.00013621165086556724 3875.089254066348 7.941346511244774 52.601014479994774 -8.602110520005226 -0.8042589575052261 15016316.726980485 63.064984411659545 2766.86672432462 73.99630539838458 0.6468324707273931\n"
     ]
    }
   ],
   "source": [
    "print(naive_pe,linear_pe,pdl_pe,sdl_pe,dedl_pe,naive_e,linear_e,pdl_e,sdl_e,dedl_e,naive_mse,linear_mse,pdl_mse,sdl_mse,dedl_mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "c2a5b9ca",
    "7e135e9e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
