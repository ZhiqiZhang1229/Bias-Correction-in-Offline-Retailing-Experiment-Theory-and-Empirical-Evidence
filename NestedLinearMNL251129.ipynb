{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27192f",
   "metadata": {
    "id": "6c27192f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14efbb6",
   "metadata": {
    "id": "f14efbb6"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56819b49-b3bb-4d32-ad88-a458e7ee142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ff42b4-0dd7-4555-bfbc-c1d6a7c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Product = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42efd20-8c8f-4675-aa1a-9a2534ee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_percentage = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce9a174-68e6-47f3-ad2f-966cc2698e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc24047e-fa23-40ae-b761-5f0d1fbe595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338e9f4b-a5a7-47a9-b06d-5216d077594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083308b",
   "metadata": {
    "id": "9083308b"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "USER_Cont_FEATURES = 2*user_continuous_feature_multiplier\n",
    "USER_Dicr_FEATURES = 3\n",
    "\n",
    "Product_Cont_FEATURES = 3*prod_continuous_feature_multiplier\n",
    "Product_Dicr_FEATURES = 2\n",
    "OUTSIDE_OPTION_UTILITY = 0\n",
    "utilities = torch.zeros(NUM_USER, NUM_Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442d9dd4",
   "metadata": {
    "id": "442d9dd4"
   },
   "outputs": [],
   "source": [
    "def generate_features(N, C, D):\n",
    "    continuous_features = np.zeros((N, C))\n",
    "    for i in range(C):\n",
    "        continuous_features[:, i] = np.random.uniform(0,1,size=N)\n",
    "    binary_features = np.random.randint(0, 2, (N, D))\n",
    "    return np.hstack((continuous_features, binary_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb84b940",
   "metadata": {
    "id": "cb84b940"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class UtilityDNN(nn.Module):\n",
    "    def __init__(self, user_features, product_features):\n",
    "        super(UtilityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features + product_features, 1)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.uniform_(self.fc1.weight, a=-0.0, b=0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class PriceSensitivityDNN(nn.Module):\n",
    "    def __init__(self, user_features):\n",
    "        super(PriceSensitivityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features,1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "\n",
    "        nn.init.constant_(self.fc1.weight, 0)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.weight, 0)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.abs(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e26f6da",
   "metadata": {
    "id": "0e26f6da"
   },
   "outputs": [],
   "source": [
    "def utility_model(x_user, X_product, price, user_randomization, prod_randomization,pair_utility_model,price_sensitivity_model,gumbel_noise):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    \n",
    "\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    for i in range(num_users):\n",
    "\n",
    "        for j in range(num_products):\n",
    "            # Determine if the user and product are in the treatment group\n",
    "            is_user_treated = (user_randomization[i] == 1)\n",
    "            is_product_treated = (prod_randomization[j] == 1)\n",
    "\n",
    "            # Adjust price based on the experiment conditions\n",
    "            adjusted_price = price[j] * discount if is_user_treated or is_product_treated else price[j]\n",
    "            combined_features = torch.cat((x_user[i], X_product[j]), 0)\n",
    "            utility_from_dnn = pair_utility_model(combined_features)\n",
    "            price_effect = price_sensitivities[i] * adjusted_price\n",
    "\n",
    "            utilities[i, j] = utility_from_dnn - price_effect + gumbel_noise[i,j]\n",
    "\n",
    "    return utility_from_dnn,price_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22bf01fa",
   "metadata": {
    "id": "22bf01fa"
   },
   "outputs": [],
   "source": [
    "def make_decision(utilities):\n",
    "    num_users = utilities.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  \n",
    "    for i in range(num_users):\n",
    "        max_utility, chosen_product = torch.max(utilities[i], dim=0)\n",
    "\n",
    "        # Compare the maximum utility with the outside option (utility = 0)\n",
    "        if max_utility <= 0:\n",
    "            decisions[i] = -1 \n",
    "        else:\n",
    "            decisions[i] = chosen_product \n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b9061",
   "metadata": {
    "id": "3a7b9061"
   },
   "outputs": [],
   "source": [
    "def calculate_revenue(decisions, prices):\n",
    "    total_revenue = 0.0\n",
    "\n",
    "    # Iterate over each decision and add the corresponding product price to total revenue\n",
    "    for i, decision in enumerate(decisions):\n",
    "        if decision != -1:  # Check if the decision is not the outside option\n",
    "            total_revenue += prices[decision].item()  # Add the price of the chosen product\n",
    "\n",
    "    return total_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be800ea9",
   "metadata": {
    "id": "be800ea9"
   },
   "outputs": [],
   "source": [
    "X_user = generate_features(NUM_USER,USER_Cont_FEATURES, USER_Dicr_FEATURES)\n",
    "X_product = generate_features(NUM_Product, Product_Cont_FEATURES, Product_Dicr_FEATURES)\n",
    "price = np.random.uniform(0.5 ,1, NUM_Product)\n",
    "\n",
    "X_user = torch.from_numpy(X_user).float()\n",
    "X_product = torch.from_numpy(X_product).float()\n",
    "price = torch.from_numpy(price).float()\n",
    "gumbel_dist = torch.distributions.Gumbel(0, 1)\n",
    "gumbel_noise = gumbel_dist.sample((NUM_USER, NUM_Product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "991385f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_nest = 3\n",
    "nest_models = nn.ModuleList([UtilityDNN(X_user.shape[1],X_product.shape[1]) for _ in range(Num_nest)])\n",
    "lambda_dissimilarity=[0.1,0.3,0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b1ab05",
   "metadata": {
    "id": "13b1ab05"
   },
   "outputs": [],
   "source": [
    "\n",
    "price_sensitivity_model = PriceSensitivityDNN(X_user.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbb32ec0-62e7-4760-8bda-c816621ef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_biases = torch.from_numpy(np.array(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "084c7cb6-2ebc-4cf1-92ea-d229b229bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new weights and biases\n",
    "with torch.no_grad():  # Avoid tracking this operation in the computation graph\n",
    "    price_sensitivity_model.fc2.bias.copy_(new_biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e280f46-89ba-422d-9bd1-6dc50dab0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.]])\n",
      "Biases: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = price_sensitivity_model.fc2.weight.data\n",
    "layer_biases = price_sensitivity_model.fc2.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a148e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def combined_nested_logit(x_user, X_product, price, user_randomization, prod_randomization, nest_models, price_sensitivity_model, gumbel_noise, Num_nest, batch_size=10):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    products_per_nest = num_products // Num_nest\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  # Initialize decision array\n",
    "\n",
    "    # Convert numpy arrays to tensors if necessary\n",
    "    if isinstance(user_randomization, np.ndarray):\n",
    "        user_randomization = torch.from_numpy(user_randomization).to(torch.bool)\n",
    "    if isinstance(prod_randomization, np.ndarray):\n",
    "        prod_randomization = torch.from_numpy(prod_randomization).to(torch.bool)\n",
    "    if isinstance(price, np.ndarray):\n",
    "        price = torch.from_numpy(price)\n",
    "\n",
    "    # Compute price sensitivities outside the batch loop\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    # Iterate over users in batches\n",
    "    for i in range(0, num_users, batch_size):\n",
    "        batch_end = min(i + batch_size, num_users)\n",
    "        batch_indices = slice(i, batch_end)\n",
    "\n",
    "        # Initialize variables for batch decisions\n",
    "        max_nest_utilities = torch.full((batch_end - i,), float('-inf'), device=x_user.device)\n",
    "        best_products = torch.zeros((batch_end - i,), dtype=torch.long, device=x_user.device)\n",
    "\n",
    "        for n in range(Num_nest):\n",
    "            nest_start = n * products_per_nest\n",
    "            nest_end = (n + 1) * products_per_nest\n",
    "\n",
    "            # Batch product features, price, and randomization across the nest\n",
    "            batch_user_features = x_user[batch_indices].unsqueeze(1).expand(-1, products_per_nest, -1)\n",
    "            batch_prod_features = X_product[nest_start:nest_end].unsqueeze(0).expand(batch_end - i, -1, -1)\n",
    "            batch_price = price[nest_start:nest_end].unsqueeze(0).expand(batch_end - i, -1)\n",
    "            batch_user_treatment = user_randomization[batch_indices].unsqueeze(1).expand(-1, products_per_nest)\n",
    "            batch_prod_treatment = prod_randomization[nest_start:nest_end].unsqueeze(0).expand(batch_end - i, -1)\n",
    "\n",
    "            # Calculate adjusted prices and combined features\n",
    "            batch_adjusted_price = torch.where(batch_user_treatment | batch_prod_treatment, batch_price * 0.2, batch_price)\n",
    "            combined_features = torch.cat((batch_user_features, batch_prod_features), dim=2).view(-1, x_user.shape[1] + X_product.shape[1])\n",
    "\n",
    "            # Compute utilities\n",
    "            utility_from_dnn = nest_models[n](combined_features).view(batch_end - i, products_per_nest)\n",
    "            price_effect = price_sensitivities[batch_indices]* batch_adjusted_price\n",
    "            nest_utilities = utility_from_dnn - price_effect + gumbel_noise[batch_indices, nest_start:nest_end]\n",
    "            nest_utilities /= lambda_dissimilarity[n]\n",
    "\n",
    "            # Determine maximum utility and corresponding product in the current nest\n",
    "            max_utilities, indices = torch.max(nest_utilities, dim=1)\n",
    "            max_nest_utilities, nest_decisions = torch.max(torch.stack((max_nest_utilities, max_utilities), dim=1), dim=1)\n",
    "            best_products = torch.where(max_nest_utilities == max_utilities, nest_start + indices, best_products)\n",
    "\n",
    "        # Assign final decisions, checking against the outside option\n",
    "        decisions[batch_indices] = torch.where(max_nest_utilities > 0, best_products, -1)\n",
    "\n",
    "    return decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97381ec",
   "metadata": {
    "id": "c97381ec"
   },
   "source": [
    "# GTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c9652",
   "metadata": {
    "id": "c74c9652"
   },
   "source": [
    "## All treated scenario: all products are discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8620e2f7",
   "metadata": {
    "id": "8620e2f7"
   },
   "outputs": [],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a69429d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a69429d2",
    "outputId": "da77f021-0faf-424d-91be-088437608bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([1, 2, 2,  ..., 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "decisions_all_treat = combined_nested_logit(X_user, X_product, price, user_randomization, prod_randomization, \n",
    "                                            nest_models, price_sensitivity_model, gumbel_noise, Num_nest, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b7df52-fdc0-46a9-ad64-9119957d6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "all_num_unique = torch.unique(decisions_all_treat).numel()\n",
    "print(all_num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8748518-f99d-47ae-a031-7b89fbbfa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(2585)\n",
      "tensor(3692)\n",
      "tensor(3223)\n",
      "tensor(123)\n",
      "tensor(200)\n",
      "tensor(170)\n",
      "tensor(5)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1,10):\n",
    "    print(torch.sum(decisions_all_treat==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f59995c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "f59995c9",
    "outputId": "b6b193fc-3ab4-4e76-8096-49dfeae0a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue from sales when all products are discounted: $1446.18\n"
     ]
    }
   ],
   "source": [
    "total_revenue_all_treated = calculate_revenue(decisions_all_treat, price*discount)\n",
    "print(f\"Total revenue from sales when all products are discounted: ${total_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86abbc5",
   "metadata": {},
   "source": [
    "## All control scenario: all products remain the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34e8b6df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "34e8b6df",
    "outputId": "846dbf7a-920f-4c54-d79f-1b527ec86ff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([1, 2, 2,  ..., 2, 0, 1])\n",
      "Total Revenue from Sales: $7062.27\n"
     ]
    }
   ],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "\n",
    "decisions_all_control = combined_nested_logit(X_user, X_product, price, user_randomization, prod_randomization, \n",
    "                                            nest_models, price_sensitivity_model, gumbel_noise, Num_nest, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_control)\n",
    "total_revenue_all_control = calculate_revenue(decisions_all_control, price)\n",
    "print(f\"Total Revenue from Sales: ${total_revenue_all_control:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e3c0837",
   "metadata": {
    "id": "2e3c0837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (ALLTreated - ALLControl): $-5616.09\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = total_revenue_all_treated - total_revenue_all_control\n",
    "print(f\"Revenue Difference (ALLTreated - ALLControl): ${revenue_difference:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/total_revenue_all_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "Sp6U7JOV1vEi",
   "metadata": {
    "id": "Sp6U7JOV1vEi"
   },
   "outputs": [],
   "source": [
    "true = revenue_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec3bca",
   "metadata": {
    "id": "d0ec3bca"
   },
   "source": [
    "## product randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cbbd718",
   "metadata": {
    "id": "4cbbd718"
   },
   "outputs": [],
   "source": [
    "def calculate_product_revenue(decisions, prices, prod_randomization):\n",
    "    revenue_treated = 0.0\n",
    "    revenue_control = 0.0\n",
    "\n",
    "    # Iterate over each user's decision\n",
    "    for user_index, decision in enumerate(decisions):\n",
    "        if decision != -1:  # If the user chose a product\n",
    "            product_price = prices[decision].item()  # Get the price of the chosen product\n",
    "\n",
    "            # Check if the product was in the treatment or control group\n",
    "            if prod_randomization[decision] == 1:\n",
    "                revenue_treated += product_price\n",
    "            else:\n",
    "                revenue_control += product_price\n",
    "\n",
    "    return revenue_treated, revenue_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "402ee3f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402ee3f8",
    "outputId": "17e270ca-c5e5-4e92-fd2f-2c675eb2fbf8"
   },
   "outputs": [],
   "source": [
    "utilities = torch.zeros(NUM_USER, NUM_Product)\n",
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1-treatment_percentage, treatment_percentage])\n",
    "decisions_product_randomization = combined_nested_logit(X_user, X_product, price, user_randomization, prod_randomization, \n",
    "                                            nest_models, price_sensitivity_model, gumbel_noise, Num_nest, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "022d9af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022d9af1",
    "outputId": "9f338893-e1f9-4c6a-945c-408a2e5b6f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue from Treated Products: $660.96\n",
      "Revenue from Control Products: $4170.48\n",
      "Revenue Difference (Treated - Control) by naive DIM: $-7019.04\n"
     ]
    }
   ],
   "source": [
    "revenue_treated, revenue_control = calculate_product_revenue(decisions_product_randomization, price-price*(1-discount)*prod_randomization, prod_randomization)\n",
    "naive = revenue_treated/treatment_percentage - revenue_control/(1-treatment_percentage)\n",
    "print(f\"Revenue from Treated Products: ${revenue_treated:.2f}\")\n",
    "print(f\"Revenue from Control Products: ${revenue_control:.2f}\")\n",
    "print(f\"Revenue Difference (Treated - Control) by naive DIM: ${naive:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/revenue_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b9eb",
   "metadata": {},
   "source": [
    "## Prepare training and testing data given experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "wUgBFRaYHK7-",
   "metadata": {
    "id": "wUgBFRaYHK7-"
   },
   "outputs": [],
   "source": [
    "X_user_1, X_user_2, decision_1, decision_2 = train_test_split(\n",
    "X_user, decisions_product_randomization, test_size=1/2, random_state=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bd5975d-53cb-45c6-90e2-c36e313515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\n",
    "    'features': X_user_1,\n",
    "    'labels': decision_1\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    'features': X_user_2,\n",
    "    'labels': decision_2\n",
    "}\n",
    "\n",
    "# Flag to switch between training and test set\n",
    "use_train_set = False  # Set to False for the test set\n",
    "\n",
    "# Function to get the current active dataset\n",
    "def get_active_dataset(use_train):\n",
    "    return train_set if use_train else test_set\n",
    "def get_test_dataset(use_train):\n",
    "    return test_set if use_train else train_set\n",
    "# Retrieve the current dataset based on the flag\n",
    "current_dataset = get_active_dataset(use_train_set)\n",
    "X_user_train = current_dataset['features']\n",
    "decision_train = current_dataset['labels']\n",
    "X_user_test = get_test_dataset(use_train_set)['features']\n",
    "decision_test =  get_test_dataset(use_train_set)['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320887a",
   "metadata": {
    "id": "d320887a"
   },
   "source": [
    "# use simple MNL structural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b14cd096",
   "metadata": {
    "id": "b14cd096"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearMNLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearMNLModel, self).__init__()\n",
    "        # Initialize parameters for user and product features\n",
    "        self.beta_user = nn.Parameter(torch.randn(user_feature_dim))\n",
    "        self.beta_product = nn.Parameter(torch.randn(product_feature_dim))\n",
    "        self.beta_price = nn.Parameter(torch.tensor(-1.0)) \n",
    "\n",
    "    def forward(self, x_user, X_product, price, user_randomization, prod_randomization):\n",
    "        N, M = x_user.shape[0], X_product.shape[0]\n",
    "\n",
    "        # Expand user and product features to create a [N, M, F] shaped tensor for each\n",
    "        x_user_expanded = x_user.unsqueeze(1).expand(-1, M, -1).detach()\n",
    "        X_product_expanded = X_product.unsqueeze(0).expand(N, -1, -1).detach()\n",
    "\n",
    "\n",
    "        # Calculate linear utility from features\n",
    "        utility_user = torch.sum(x_user_expanded * self.beta_user, dim=2)\n",
    "        utility_product = torch.sum(X_product_expanded * self.beta_product, dim=2)\n",
    "\n",
    "        # Adjust prices based on randomization\n",
    "        adjusted_price = torch.where(\n",
    "             prod_randomization.unsqueeze(0),\n",
    "            price * discount,  \n",
    "            price\n",
    "        )\n",
    "\n",
    "        # Calculate utility from price, properly expanding its dimension\n",
    "        utility_price = adjusted_price * self.beta_price  # [M]\n",
    "        utility_price = utility_price.expand(N, M)  # [N, M]\n",
    "\n",
    "        # Total utility including features and price\n",
    "        total_utility = utility_user + utility_product + utility_price\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=total_utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities,total_utility), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d027617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89f614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearMNLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74e297a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74e297a",
    "outputId": "d9b4de3b-2c2d-452e-b491-1b217ba2aaf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.246809482574463\n",
      "Epoch 1000, Loss: 1.611031174659729\n",
      "Epoch 2000, Loss: 1.5712517499923706\n",
      "Epoch 3000, Loss: 1.543168544769287\n",
      "Epoch 4000, Loss: 1.5191725492477417\n",
      "Epoch 5000, Loss: 1.497540831565857\n",
      "Epoch 6000, Loss: 1.478405237197876\n",
      "Epoch 7000, Loss: 1.4621825218200684\n",
      "Epoch 8000, Loss: 1.4487485885620117\n",
      "Epoch 9000, Loss: 1.4374929666519165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    utilities = model(X_user_train, X_product, price, user_randomization, prod_randomization)\n",
    "    choice_probabilities = nn.functional.log_softmax(utilities, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train+1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f78d4b21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78d4b21",
    "outputId": "6078e9c0-ce87-4f8e-dd3d-88c3cfb2a616"
   },
   "outputs": [],
   "source": [
    "beta_price_est = model.beta_price.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ac97d98-d381-4a84-82c7-aa6821c3aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.804243\n"
     ]
    }
   ],
   "source": [
    "print(beta_price_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e4df3c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e4df3c1",
    "outputId": "709d1125-67b8-459d-d512-a95764796126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([58.0628, 53.3536, 18.0330, 17.9637, 17.8469], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1783523",
   "metadata": {
    "id": "a1783523"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17eb38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17eb38d3",
    "outputId": "df4ab9ed-52f4-4538-87a5-66932ad5f9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue: $3228.89\n",
      "Expected Revenue: $661.17\n"
     ]
    }
   ],
   "source": [
    "all_product_control = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "all_product_treated = np.random.choice([0,1], NUM_Product, p=[0, 1])\n",
    "all_product_control = torch.from_numpy(all_product_control).to(X_user_train.device).bool()\n",
    "all_product_treated = torch.from_numpy(all_product_treated).to(X_user_train.device).bool()\n",
    "\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_control)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "expected_revenue = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue.item():.2f}\")\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_treated)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)*discount\n",
    "expected_revenue_treated = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue_treated.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "P7Z_BF2C1kdj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7Z_BF2C1kdj",
    "outputId": "6998bb85-85b3-4cf8-da34-4ecded076114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (Treated - Control) by Linear MNL: $-5135.44\n",
      "Absolute Percentage Estimation Error of Linear MNL:  -8.56%\n"
     ]
    }
   ],
   "source": [
    "linear = (expected_revenue_treated-expected_revenue).cpu().detach().numpy()\n",
    "linear = linear*2\n",
    "print(f\"Revenue Difference (Treated - Control) by Linear MNL: ${linear:.2f}\")\n",
    "print(f\"Absolute Percentage Estimation Error of Linear MNL:  {100*np.abs(linear-revenue_difference)/revenue_difference:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309183f7",
   "metadata": {},
   "source": [
    "# Use NMNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c36b1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "707121c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25b8e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNestedMNL(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearNestedMNL, self).__init__()\n",
    "        \n",
    "  \n",
    "        total_feature_dim = user_feature_dim + product_feature_dim + 1 \n",
    "        self.utility_linear = nn.Linear(total_feature_dim, 1)\n",
    "        self.raw_lambda = nn.Parameter(torch.tensor(2.0)) \n",
    "\n",
    "    def forward(self, x_user, x_product, prices):\n",
    " \n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "\n",
    "        prices_expanded = prices.view(1, M, 1).expand(N, -1, -1)\n",
    "        \n",
    "      \n",
    "        combined_features = torch.cat((\n",
    "            x_user.unsqueeze(1).expand(-1, M, -1),       # (N, M, U_dim)\n",
    "            x_product.unsqueeze(0).expand(N, -1, -1),    # (N, M, P_dim)\n",
    "            prices_expanded                              # (N, M, 1)\n",
    "        ), dim=2)\n",
    "\n",
    "\n",
    "        utilities = self.utility_linear(combined_features).squeeze(-1)\n",
    "\n",
    "       \n",
    "        lam = torch.sigmoid(self.raw_lambda) \n",
    "\n",
    "\n",
    "        scaled_utilities = utilities / lam\n",
    "        inclusive_value_log = torch.logsumexp(scaled_utilities, dim=1, keepdim=True) \n",
    "        \n",
    "        v_buy = lam * inclusive_value_log\n",
    "        v_outside = torch.zeros(N, 1, device=x_user.device)\n",
    "        \n",
    "        # Log Softmax over the two Nests\n",
    "        nest_logits = torch.cat([v_buy, v_outside], dim=1)\n",
    "        nest_log_probs = F.log_softmax(nest_logits, dim=1) \n",
    "        \n",
    "        log_prob_buy_nest = nest_log_probs[:, 0].unsqueeze(1)\n",
    "        log_prob_outside = nest_log_probs[:, 1].unsqueeze(1)\n",
    "\n",
    "        log_prob_item_given_buy = scaled_utilities - inclusive_value_log\n",
    "        final_log_probs_products = log_prob_item_given_buy + log_prob_buy_nest\n",
    "        \n",
    "        # Return: [Log P(Outside), Log P(Prod 1), ..., Log P(Prod M)]\n",
    "        return torch.cat([log_prob_outside, final_log_probs_products], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44ae9fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a4a9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 2.5118, Validation Loss: 2.4933\n",
      "Epoch 100, Training Loss: 1.7653, Validation Loss: 1.7515\n",
      "Epoch 200, Training Loss: 1.6520, Validation Loss: 1.6258\n",
      "Epoch 300, Training Loss: 1.6219, Validation Loss: 1.5890\n",
      "Epoch 400, Training Loss: 1.5992, Validation Loss: 1.5622\n",
      "Epoch 500, Training Loss: 1.5755, Validation Loss: 1.5349\n",
      "Epoch 600, Training Loss: 1.5460, Validation Loss: 1.5019\n",
      "Epoch 700, Training Loss: 1.4873, Validation Loss: 1.4416\n",
      "Epoch 800, Training Loss: 1.3765, Validation Loss: 1.3417\n",
      "Early stopping triggered at Epoch 819\n",
      "Final Lambda (Nesting Parameter): 0.0579\n",
      "Expected Revenue (Control): $3239.77\n",
      "Expected Revenue (Treated): $671.74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LinearNestedMNL(\n",
    "    user_feature_dim=X_user_train1.shape[1], \n",
    "    product_feature_dim=X_product.shape[1]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    log_probs = model(user_features, product_features, prices)\n",
    "    loss = -torch.mean(log_probs[torch.arange(log_probs.shape[0]), decision_train1 + 1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_log_probs = model(X_user_val, product_features, prices)\n",
    "        val_loss = -torch.mean(val_log_probs[torch.arange(val_log_probs.shape[0]), decision_val + 1])\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0 \n",
    "    else:\n",
    "        patience_counter += 1 \n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at Epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Lambda (Nesting Parameter): {torch.sigmoid(model.raw_lambda).item():.4f}\")\n",
    "\n",
    "\n",
    "def calculate_expected_revenue(model, user_features, product_features, prices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        log_probs = model(user_features, product_features, prices)\n",
    "        probabilities = torch.exp(log_probs)\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device), prices), dim=0)\n",
    "        \n",
    "        total_expected_revenue = (probabilities * price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()\n",
    "\n",
    "\n",
    "X_user_test = X_user_test.to(device)\n",
    "X_product = X_product.to(device)\n",
    "price = price.to(device)\n",
    "\n",
    "\n",
    "user_features_test, product_features_test, prices_control, _ = prepare_data(X_user_test, X_product, price)\n",
    "\n",
    "expected_revenue_control = calculate_expected_revenue(\n",
    "    model, user_features_test, product_features_test, prices_control\n",
    ")\n",
    "print(f\"Expected Revenue (Control): ${expected_revenue_control:.2f}\")\n",
    "\n",
    "all_treated_price = price * discount\n",
    "user_features_test, product_features_test, prices_treated, _ = prepare_data(X_user_test, X_product, all_treated_price)\n",
    "\n",
    "expected_revenue_treated = calculate_expected_revenue(\n",
    "    model, user_features_test, product_features_test, prices_treated\n",
    ")\n",
    "print(f\"Expected Revenue (Treated): ${expected_revenue_treated:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "252c6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference: $-5136.08\n",
      "Absolute Percentage Estimation Error of PDL:  -8.55%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Revenue Difference: ${2*(expected_revenue_treated - expected_revenue_control):.2f}\")\n",
    "nmnl = 2*(expected_revenue_treated - expected_revenue_control)\n",
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(nmnl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab4aae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2899bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "134acfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39864359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(PDLModel, self).__init__()\n",
    "        # Combined feature dimension includes product features, price, and user features, as well as other products' features and prices\n",
    "        total_feature_dim = user_feature_dim + 2*product_feature_dim + 1  # +1 for price\n",
    "\n",
    "        # Single neural network to process the combined features\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1) \n",
    "        )\n",
    "            # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_user, x_product, x_other_products,prices):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                        x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        aggregated_other_features.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        prices.view(1, -1, 1).expand(N, -1, -1)),\n",
    "                                        dim=2)\n",
    "   \n",
    "\n",
    "        # Compute utility for each combined feature set\n",
    "        utilities = self.network(combined_features).squeeze(-1)\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=utilities.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utilities), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a160035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdlmodel = PDLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1ceb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.4561848640441895, Validation Loss: 2.4470479488372803\n",
      "Epoch 2, Training Loss: 2.4477126598358154, Validation Loss: 2.4384140968322754\n",
      "Epoch 3, Training Loss: 2.439161777496338, Validation Loss: 2.429058313369751\n",
      "Epoch 4, Training Loss: 2.429955244064331, Validation Loss: 2.418344020843506\n",
      "Epoch 5, Training Loss: 2.419363260269165, Validation Loss: 2.4055874347686768\n",
      "Epoch 6, Training Loss: 2.406768321990967, Validation Loss: 2.390681505203247\n",
      "Epoch 7, Training Loss: 2.3921046257019043, Validation Loss: 2.373140335083008\n",
      "Epoch 8, Training Loss: 2.3748323917388916, Validation Loss: 2.3520705699920654\n",
      "Epoch 9, Training Loss: 2.354085922241211, Validation Loss: 2.326667070388794\n",
      "Epoch 10, Training Loss: 2.32905912399292, Validation Loss: 2.2959189414978027\n",
      "Epoch 11, Training Loss: 2.298788547515869, Validation Loss: 2.2598748207092285\n",
      "Epoch 12, Training Loss: 2.2632744312286377, Validation Loss: 2.2173399925231934\n",
      "Epoch 13, Training Loss: 2.2214739322662354, Validation Loss: 2.167402982711792\n",
      "Epoch 14, Training Loss: 2.172628164291382, Validation Loss: 2.1102120876312256\n",
      "Epoch 15, Training Loss: 2.116764783859253, Validation Loss: 2.0467169284820557\n",
      "Epoch 16, Training Loss: 2.054705858230591, Validation Loss: 1.9789835214614868\n",
      "Epoch 17, Training Loss: 1.9887583255767822, Validation Loss: 1.9097578525543213\n",
      "Epoch 18, Training Loss: 1.9216744899749756, Validation Loss: 1.8428239822387695\n",
      "Epoch 19, Training Loss: 1.8574475049972534, Validation Loss: 1.783350944519043\n",
      "Epoch 20, Training Loss: 1.801026701927185, Validation Loss: 1.7359213829040527\n",
      "Epoch 21, Training Loss: 1.757140874862671, Validation Loss: 1.7033238410949707\n",
      "Epoch 22, Training Loss: 1.7283967733383179, Validation Loss: 1.68423593044281\n",
      "Epoch 23, Training Loss: 1.713445782661438, Validation Loss: 1.676200032234192\n",
      "Epoch 24, Training Loss: 1.708807110786438, Validation Loss: 1.6739625930786133\n",
      "Epoch 25, Training Loss: 1.7104158401489258, Validation Loss: 1.6745606660842896\n",
      "Epoch 26, Training Loss: 1.7142888307571411, Validation Loss: 1.674052357673645\n",
      "Epoch 27, Training Loss: 1.7161246538162231, Validation Loss: 1.6712336540222168\n",
      "Epoch 28, Training Loss: 1.714521884918213, Validation Loss: 1.6677016019821167\n",
      "Epoch 29, Training Loss: 1.7112278938293457, Validation Loss: 1.6612918376922607\n",
      "Epoch 30, Training Loss: 1.7050317525863647, Validation Loss: 1.6518120765686035\n",
      "Epoch 31, Training Loss: 1.695785403251648, Validation Loss: 1.6422430276870728\n",
      "Epoch 32, Training Loss: 1.685747504234314, Validation Loss: 1.6320914030075073\n",
      "Epoch 33, Training Loss: 1.6740161180496216, Validation Loss: 1.6219085454940796\n",
      "Epoch 34, Training Loss: 1.6616573333740234, Validation Loss: 1.613423466682434\n",
      "Epoch 35, Training Loss: 1.6508522033691406, Validation Loss: 1.6047098636627197\n",
      "Epoch 36, Training Loss: 1.6403411626815796, Validation Loss: 1.5947418212890625\n",
      "Epoch 37, Training Loss: 1.6292721033096313, Validation Loss: 1.5851954221725464\n",
      "Epoch 38, Training Loss: 1.6189062595367432, Validation Loss: 1.5768159627914429\n",
      "Epoch 39, Training Loss: 1.6096022129058838, Validation Loss: 1.569875717163086\n",
      "Epoch 40, Training Loss: 1.6014353036880493, Validation Loss: 1.563910722732544\n",
      "Epoch 41, Training Loss: 1.5940011739730835, Validation Loss: 1.5586295127868652\n",
      "Epoch 42, Training Loss: 1.5872077941894531, Validation Loss: 1.5538742542266846\n",
      "Epoch 43, Training Loss: 1.5811808109283447, Validation Loss: 1.549281358718872\n",
      "Epoch 44, Training Loss: 1.575758695602417, Validation Loss: 1.5447967052459717\n",
      "Epoch 45, Training Loss: 1.5709716081619263, Validation Loss: 1.5408457517623901\n",
      "Epoch 46, Training Loss: 1.566723108291626, Validation Loss: 1.5374960899353027\n",
      "Epoch 47, Training Loss: 1.5623935461044312, Validation Loss: 1.5345627069473267\n",
      "Epoch 48, Training Loss: 1.558268666267395, Validation Loss: 1.5305936336517334\n",
      "Epoch 49, Training Loss: 1.55393648147583, Validation Loss: 1.5256102085113525\n",
      "Epoch 50, Training Loss: 1.5492923259735107, Validation Loss: 1.5205801725387573\n",
      "Epoch 51, Training Loss: 1.5446614027023315, Validation Loss: 1.5158917903900146\n",
      "Epoch 52, Training Loss: 1.5398935079574585, Validation Loss: 1.5116771459579468\n",
      "Epoch 53, Training Loss: 1.5350195169448853, Validation Loss: 1.507378339767456\n",
      "Epoch 54, Training Loss: 1.5303699970245361, Validation Loss: 1.502523422241211\n",
      "Epoch 55, Training Loss: 1.5257270336151123, Validation Loss: 1.4976180791854858\n",
      "Epoch 56, Training Loss: 1.5212966203689575, Validation Loss: 1.4931851625442505\n",
      "Epoch 57, Training Loss: 1.5171093940734863, Validation Loss: 1.4892562627792358\n",
      "Epoch 58, Training Loss: 1.5130527019500732, Validation Loss: 1.4855196475982666\n",
      "Epoch 59, Training Loss: 1.5091137886047363, Validation Loss: 1.4819566011428833\n",
      "Epoch 60, Training Loss: 1.5052226781845093, Validation Loss: 1.4784436225891113\n",
      "Epoch 61, Training Loss: 1.5013843774795532, Validation Loss: 1.4751020669937134\n",
      "Epoch 62, Training Loss: 1.4976032972335815, Validation Loss: 1.4719051122665405\n",
      "Epoch 63, Training Loss: 1.493901014328003, Validation Loss: 1.468639612197876\n",
      "Epoch 64, Training Loss: 1.4902865886688232, Validation Loss: 1.465151309967041\n",
      "Epoch 65, Training Loss: 1.4867019653320312, Validation Loss: 1.4615132808685303\n",
      "Epoch 66, Training Loss: 1.4831563234329224, Validation Loss: 1.4579499959945679\n",
      "Epoch 67, Training Loss: 1.4797310829162598, Validation Loss: 1.4545884132385254\n",
      "Epoch 68, Training Loss: 1.476381540298462, Validation Loss: 1.451356291770935\n",
      "Epoch 69, Training Loss: 1.4730585813522339, Validation Loss: 1.448299765586853\n",
      "Epoch 70, Training Loss: 1.469809651374817, Validation Loss: 1.4454307556152344\n",
      "Epoch 71, Training Loss: 1.4666930437088013, Validation Loss: 1.4424973726272583\n",
      "Epoch 72, Training Loss: 1.4636296033859253, Validation Loss: 1.4394104480743408\n",
      "Epoch 73, Training Loss: 1.4605987071990967, Validation Loss: 1.4364012479782104\n",
      "Epoch 74, Training Loss: 1.4576996564865112, Validation Loss: 1.4336057901382446\n",
      "Epoch 75, Training Loss: 1.4548991918563843, Validation Loss: 1.431032419204712\n",
      "Epoch 76, Training Loss: 1.4521174430847168, Validation Loss: 1.4286854267120361\n",
      "Epoch 77, Training Loss: 1.4494355916976929, Validation Loss: 1.4263657331466675\n",
      "Epoch 78, Training Loss: 1.446864366531372, Validation Loss: 1.4238907098770142\n",
      "Epoch 79, Training Loss: 1.4443224668502808, Validation Loss: 1.4213998317718506\n",
      "Epoch 80, Training Loss: 1.4418654441833496, Validation Loss: 1.4190949201583862\n",
      "Epoch 81, Training Loss: 1.4395095109939575, Validation Loss: 1.4170112609863281\n",
      "Epoch 82, Training Loss: 1.4371899366378784, Validation Loss: 1.4151103496551514\n",
      "Epoch 83, Training Loss: 1.4349429607391357, Validation Loss: 1.4132729768753052\n",
      "Epoch 84, Training Loss: 1.4327905178070068, Validation Loss: 1.4112969636917114\n",
      "Epoch 85, Training Loss: 1.4306752681732178, Validation Loss: 1.4092800617218018\n",
      "Epoch 86, Training Loss: 1.4286378622055054, Validation Loss: 1.4074492454528809\n",
      "Epoch 87, Training Loss: 1.426687479019165, Validation Loss: 1.4058455228805542\n",
      "Epoch 88, Training Loss: 1.4247618913650513, Validation Loss: 1.4044815301895142\n",
      "Epoch 89, Training Loss: 1.4228904247283936, Validation Loss: 1.403046727180481\n",
      "Epoch 90, Training Loss: 1.4211288690567017, Validation Loss: 1.4012196063995361\n",
      "Epoch 91, Training Loss: 1.419359803199768, Validation Loss: 1.3995610475540161\n",
      "Epoch 92, Training Loss: 1.4177348613739014, Validation Loss: 1.3982923030853271\n",
      "Epoch 93, Training Loss: 1.4160912036895752, Validation Loss: 1.3971178531646729\n",
      "Epoch 94, Training Loss: 1.4145689010620117, Validation Loss: 1.3956570625305176\n",
      "Epoch 95, Training Loss: 1.4130367040634155, Validation Loss: 1.3941081762313843\n",
      "Epoch 96, Training Loss: 1.4115850925445557, Validation Loss: 1.3928577899932861\n",
      "Epoch 97, Training Loss: 1.4101499319076538, Validation Loss: 1.391834020614624\n",
      "Epoch 98, Training Loss: 1.4087696075439453, Validation Loss: 1.3905972242355347\n",
      "Epoch 99, Training Loss: 1.407426357269287, Validation Loss: 1.3891443014144897\n",
      "Epoch 100, Training Loss: 1.4061335325241089, Validation Loss: 1.3879715204238892\n",
      "Epoch 101, Training Loss: 1.4048854112625122, Validation Loss: 1.3870854377746582\n",
      "Epoch 102, Training Loss: 1.4036781787872314, Validation Loss: 1.3859773874282837\n",
      "Epoch 103, Training Loss: 1.4025098085403442, Validation Loss: 1.3846826553344727\n",
      "Epoch 104, Training Loss: 1.4013746976852417, Validation Loss: 1.3836406469345093\n",
      "Epoch 105, Training Loss: 1.4002728462219238, Validation Loss: 1.3828318119049072\n",
      "Epoch 106, Training Loss: 1.399198055267334, Validation Loss: 1.381857991218567\n",
      "Epoch 107, Training Loss: 1.3981596231460571, Validation Loss: 1.3807319402694702\n",
      "Epoch 108, Training Loss: 1.3971527814865112, Validation Loss: 1.3798182010650635\n",
      "Epoch 109, Training Loss: 1.3961809873580933, Validation Loss: 1.379090666770935\n",
      "Epoch 110, Training Loss: 1.3952436447143555, Validation Loss: 1.3781813383102417\n",
      "Epoch 111, Training Loss: 1.3943320512771606, Validation Loss: 1.3772060871124268\n",
      "Epoch 112, Training Loss: 1.3934539556503296, Validation Loss: 1.3764288425445557\n",
      "Epoch 113, Training Loss: 1.3925902843475342, Validation Loss: 1.3757151365280151\n",
      "Epoch 114, Training Loss: 1.3917534351348877, Validation Loss: 1.3748772144317627\n",
      "Epoch 115, Training Loss: 1.3909289836883545, Validation Loss: 1.3739774227142334\n",
      "Epoch 116, Training Loss: 1.3901326656341553, Validation Loss: 1.3732243776321411\n",
      "Epoch 117, Training Loss: 1.3893542289733887, Validation Loss: 1.3725835084915161\n",
      "Epoch 118, Training Loss: 1.3886046409606934, Validation Loss: 1.3717879056930542\n",
      "Epoch 119, Training Loss: 1.3878700733184814, Validation Loss: 1.370993971824646\n",
      "Epoch 120, Training Loss: 1.387162685394287, Validation Loss: 1.3703449964523315\n",
      "Epoch 121, Training Loss: 1.3864688873291016, Validation Loss: 1.3697490692138672\n",
      "Epoch 122, Training Loss: 1.3857874870300293, Validation Loss: 1.3690276145935059\n",
      "Epoch 123, Training Loss: 1.3851202726364136, Validation Loss: 1.3683178424835205\n",
      "Epoch 124, Training Loss: 1.3844709396362305, Validation Loss: 1.367767572402954\n",
      "Epoch 125, Training Loss: 1.3838392496109009, Validation Loss: 1.3671966791152954\n",
      "Epoch 126, Training Loss: 1.3832358121871948, Validation Loss: 1.3665482997894287\n",
      "Epoch 127, Training Loss: 1.3826475143432617, Validation Loss: 1.3659746646881104\n",
      "Epoch 128, Training Loss: 1.3820832967758179, Validation Loss: 1.3655188083648682\n",
      "Epoch 129, Training Loss: 1.381538987159729, Validation Loss: 1.3650484085083008\n",
      "Epoch 130, Training Loss: 1.3810169696807861, Validation Loss: 1.3644524812698364\n",
      "Epoch 131, Training Loss: 1.3805102109909058, Validation Loss: 1.3639566898345947\n",
      "Epoch 132, Training Loss: 1.3800238370895386, Validation Loss: 1.363603949546814\n",
      "Epoch 133, Training Loss: 1.3795456886291504, Validation Loss: 1.3632022142410278\n",
      "Epoch 134, Training Loss: 1.3790854215621948, Validation Loss: 1.3626766204833984\n",
      "Epoch 135, Training Loss: 1.3786354064941406, Validation Loss: 1.3623127937316895\n",
      "Epoch 136, Training Loss: 1.3781778812408447, Validation Loss: 1.3620632886886597\n",
      "Epoch 137, Training Loss: 1.3777260780334473, Validation Loss: 1.3616276979446411\n",
      "Epoch 138, Training Loss: 1.3772810697555542, Validation Loss: 1.361167550086975\n",
      "Epoch 139, Training Loss: 1.3768564462661743, Validation Loss: 1.3608475923538208\n",
      "Epoch 140, Training Loss: 1.376443862915039, Validation Loss: 1.3606019020080566\n",
      "Epoch 141, Training Loss: 1.3760484457015991, Validation Loss: 1.360224723815918\n",
      "Epoch 142, Training Loss: 1.3756614923477173, Validation Loss: 1.3598461151123047\n",
      "Epoch 143, Training Loss: 1.375287413597107, Validation Loss: 1.3596014976501465\n",
      "Epoch 144, Training Loss: 1.3749192953109741, Validation Loss: 1.3593369722366333\n",
      "Epoch 145, Training Loss: 1.3745620250701904, Validation Loss: 1.359031319618225\n",
      "Epoch 146, Training Loss: 1.374212622642517, Validation Loss: 1.3587146997451782\n",
      "Epoch 147, Training Loss: 1.3738741874694824, Validation Loss: 1.3584377765655518\n",
      "Epoch 148, Training Loss: 1.3735445737838745, Validation Loss: 1.358190894126892\n",
      "Epoch 149, Training Loss: 1.3732227087020874, Validation Loss: 1.3579063415527344\n",
      "Epoch 150, Training Loss: 1.3729069232940674, Validation Loss: 1.3576277494430542\n",
      "Epoch 151, Training Loss: 1.372598648071289, Validation Loss: 1.3573887348175049\n",
      "Epoch 152, Training Loss: 1.372298002243042, Validation Loss: 1.3571505546569824\n",
      "Epoch 153, Training Loss: 1.3720065355300903, Validation Loss: 1.356895089149475\n",
      "Epoch 154, Training Loss: 1.3717232942581177, Validation Loss: 1.3566484451293945\n",
      "Epoch 155, Training Loss: 1.3714468479156494, Validation Loss: 1.35641610622406\n",
      "Epoch 156, Training Loss: 1.3711764812469482, Validation Loss: 1.3561713695526123\n",
      "Epoch 157, Training Loss: 1.3709121942520142, Validation Loss: 1.3559424877166748\n",
      "Epoch 158, Training Loss: 1.3706547021865845, Validation Loss: 1.355722427368164\n",
      "Epoch 159, Training Loss: 1.3704044818878174, Validation Loss: 1.3554903268814087\n",
      "Epoch 160, Training Loss: 1.3701612949371338, Validation Loss: 1.355271816253662\n",
      "Epoch 161, Training Loss: 1.3699239492416382, Validation Loss: 1.3550752401351929\n",
      "Epoch 162, Training Loss: 1.3696918487548828, Validation Loss: 1.3548624515533447\n",
      "Epoch 163, Training Loss: 1.3694648742675781, Validation Loss: 1.3546518087387085\n",
      "Epoch 164, Training Loss: 1.3692432641983032, Validation Loss: 1.3544800281524658\n",
      "Epoch 165, Training Loss: 1.3690274953842163, Validation Loss: 1.3542782068252563\n",
      "Epoch 166, Training Loss: 1.368816614151001, Validation Loss: 1.3540693521499634\n",
      "Epoch 167, Training Loss: 1.3686107397079468, Validation Loss: 1.3538813591003418\n",
      "Epoch 168, Training Loss: 1.3684090375900269, Validation Loss: 1.3537191152572632\n",
      "Epoch 169, Training Loss: 1.3682118654251099, Validation Loss: 1.3535547256469727\n",
      "Epoch 170, Training Loss: 1.3680189847946167, Validation Loss: 1.3533514738082886\n",
      "Epoch 171, Training Loss: 1.3678300380706787, Validation Loss: 1.3531882762908936\n",
      "Epoch 172, Training Loss: 1.3676457405090332, Validation Loss: 1.353035807609558\n",
      "Epoch 173, Training Loss: 1.3674657344818115, Validation Loss: 1.3528491258621216\n",
      "Epoch 174, Training Loss: 1.367289423942566, Validation Loss: 1.3526850938796997\n",
      "Epoch 175, Training Loss: 1.367116928100586, Validation Loss: 1.3525469303131104\n",
      "Epoch 176, Training Loss: 1.366947889328003, Validation Loss: 1.3523741960525513\n",
      "Epoch 177, Training Loss: 1.3667824268341064, Validation Loss: 1.352216362953186\n",
      "Epoch 178, Training Loss: 1.366620659828186, Validation Loss: 1.3520833253860474\n",
      "Epoch 179, Training Loss: 1.3664624691009521, Validation Loss: 1.3519177436828613\n",
      "Epoch 180, Training Loss: 1.366307258605957, Validation Loss: 1.351770043373108\n",
      "Epoch 181, Training Loss: 1.3661551475524902, Validation Loss: 1.351636290550232\n",
      "Epoch 182, Training Loss: 1.3660061359405518, Validation Loss: 1.3514797687530518\n",
      "Epoch 183, Training Loss: 1.3658599853515625, Validation Loss: 1.3513381481170654\n",
      "Epoch 184, Training Loss: 1.3657170534133911, Validation Loss: 1.3512119054794312\n",
      "Epoch 185, Training Loss: 1.3655767440795898, Validation Loss: 1.3510628938674927\n",
      "Epoch 186, Training Loss: 1.3654391765594482, Validation Loss: 1.3509238958358765\n",
      "Epoch 187, Training Loss: 1.3653042316436768, Validation Loss: 1.3508116006851196\n",
      "Epoch 188, Training Loss: 1.3651719093322754, Validation Loss: 1.350657343864441\n",
      "Epoch 189, Training Loss: 1.3650420904159546, Validation Loss: 1.350541591644287\n",
      "Epoch 190, Training Loss: 1.3649147748947144, Validation Loss: 1.3504259586334229\n",
      "Epoch 191, Training Loss: 1.3647899627685547, Validation Loss: 1.3502815961837769\n",
      "Epoch 192, Training Loss: 1.364667296409607, Validation Loss: 1.3501763343811035\n",
      "Epoch 193, Training Loss: 1.3645468950271606, Validation Loss: 1.3500581979751587\n",
      "Epoch 194, Training Loss: 1.3644288778305054, Validation Loss: 1.3499265909194946\n",
      "Epoch 195, Training Loss: 1.364313006401062, Validation Loss: 1.3498269319534302\n",
      "Epoch 196, Training Loss: 1.3641990423202515, Validation Loss: 1.3497133255004883\n",
      "Epoch 197, Training Loss: 1.3640873432159424, Validation Loss: 1.34959077835083\n",
      "Epoch 198, Training Loss: 1.3639776706695557, Validation Loss: 1.349498987197876\n",
      "Epoch 199, Training Loss: 1.3638696670532227, Validation Loss: 1.3493895530700684\n",
      "Epoch 200, Training Loss: 1.3637639284133911, Validation Loss: 1.3492752313613892\n",
      "Epoch 201, Training Loss: 1.3636598587036133, Validation Loss: 1.3491878509521484\n",
      "Epoch 202, Training Loss: 1.3635575771331787, Validation Loss: 1.349085807800293\n",
      "Epoch 203, Training Loss: 1.363457202911377, Validation Loss: 1.3489770889282227\n",
      "Epoch 204, Training Loss: 1.363358497619629, Validation Loss: 1.3488948345184326\n",
      "Epoch 205, Training Loss: 1.3632614612579346, Validation Loss: 1.34879469871521\n",
      "Epoch 206, Training Loss: 1.3631659746170044, Validation Loss: 1.348695158958435\n",
      "Epoch 207, Training Loss: 1.3630722761154175, Validation Loss: 1.348620057106018\n",
      "Epoch 208, Training Loss: 1.3629802465438843, Validation Loss: 1.3485187292099\n",
      "Epoch 209, Training Loss: 1.3628895282745361, Validation Loss: 1.3484306335449219\n",
      "Epoch 210, Training Loss: 1.3628003597259521, Validation Loss: 1.3483593463897705\n",
      "Epoch 211, Training Loss: 1.3627127408981323, Validation Loss: 1.3482565879821777\n",
      "Epoch 212, Training Loss: 1.362626314163208, Validation Loss: 1.348182201385498\n",
      "Epoch 213, Training Loss: 1.3625414371490479, Validation Loss: 1.348109245300293\n",
      "Epoch 214, Training Loss: 1.3624579906463623, Validation Loss: 1.3480123281478882\n",
      "Epoch 215, Training Loss: 1.3623758554458618, Validation Loss: 1.3479456901550293\n",
      "Epoch 216, Training Loss: 1.3622949123382568, Validation Loss: 1.3478714227676392\n",
      "Epoch 217, Training Loss: 1.362215280532837, Validation Loss: 1.3477857112884521\n",
      "Epoch 218, Training Loss: 1.362136960029602, Validation Loss: 1.347720742225647\n",
      "Epoch 219, Training Loss: 1.3620597124099731, Validation Loss: 1.3476465940475464\n",
      "Epoch 220, Training Loss: 1.3619836568832397, Validation Loss: 1.3475704193115234\n",
      "Epoch 221, Training Loss: 1.3619089126586914, Validation Loss: 1.3475127220153809\n",
      "Epoch 222, Training Loss: 1.3618351221084595, Validation Loss: 1.3474321365356445\n",
      "Epoch 223, Training Loss: 1.361762523651123, Validation Loss: 1.3473790884017944\n",
      "Epoch 224, Training Loss: 1.3616909980773926, Validation Loss: 1.3473060131072998\n",
      "Epoch 225, Training Loss: 1.3616204261779785, Validation Loss: 1.3472449779510498\n",
      "Epoch 226, Training Loss: 1.36155104637146, Validation Loss: 1.3471888303756714\n",
      "Epoch 227, Training Loss: 1.3614823818206787, Validation Loss: 1.3471219539642334\n",
      "Epoch 228, Training Loss: 1.3614146709442139, Validation Loss: 1.3470579385757446\n",
      "Epoch 229, Training Loss: 1.361348271369934, Validation Loss: 1.3470056056976318\n",
      "Epoch 230, Training Loss: 1.3612829446792603, Validation Loss: 1.3469443321228027\n",
      "Epoch 231, Training Loss: 1.3612185716629028, Validation Loss: 1.3468804359436035\n",
      "Epoch 232, Training Loss: 1.3611551523208618, Validation Loss: 1.3468436002731323\n",
      "Epoch 233, Training Loss: 1.3610929250717163, Validation Loss: 1.3467638492584229\n",
      "Epoch 234, Training Loss: 1.361031413078308, Validation Loss: 1.3467265367507935\n",
      "Epoch 235, Training Loss: 1.3609707355499268, Validation Loss: 1.3466609716415405\n",
      "Epoch 236, Training Loss: 1.3609110116958618, Validation Loss: 1.3466005325317383\n",
      "Epoch 237, Training Loss: 1.3608522415161133, Validation Loss: 1.3465657234191895\n",
      "Epoch 238, Training Loss: 1.3607944250106812, Validation Loss: 1.3464852571487427\n",
      "Epoch 239, Training Loss: 1.3607373237609863, Validation Loss: 1.3464621305465698\n",
      "Epoch 240, Training Loss: 1.3606810569763184, Validation Loss: 1.346378207206726\n",
      "Epoch 241, Training Loss: 1.3606255054473877, Validation Loss: 1.3463550806045532\n",
      "Epoch 242, Training Loss: 1.3605705499649048, Validation Loss: 1.3462765216827393\n",
      "Epoch 243, Training Loss: 1.3605164289474487, Validation Loss: 1.346246600151062\n",
      "Epoch 244, Training Loss: 1.3604629039764404, Validation Loss: 1.3461803197860718\n",
      "Epoch 245, Training Loss: 1.360410213470459, Validation Loss: 1.3461356163024902\n",
      "Epoch 246, Training Loss: 1.3603583574295044, Validation Loss: 1.3460795879364014\n",
      "Epoch 247, Training Loss: 1.3603070974349976, Validation Loss: 1.346039056777954\n",
      "Epoch 248, Training Loss: 1.3602569103240967, Validation Loss: 1.3459588289260864\n",
      "Epoch 249, Training Loss: 1.3602075576782227, Validation Loss: 1.3459601402282715\n",
      "Epoch 250, Training Loss: 1.360158920288086, Validation Loss: 1.3458431959152222\n",
      "Epoch 251, Training Loss: 1.3601107597351074, Validation Loss: 1.3458744287490845\n",
      "Epoch 252, Training Loss: 1.3600634336471558, Validation Loss: 1.3457300662994385\n",
      "Epoch 253, Training Loss: 1.3600167036056519, Validation Loss: 1.3457967042922974\n",
      "Epoch 254, Training Loss: 1.3599706888198853, Validation Loss: 1.345612645149231\n",
      "Epoch 255, Training Loss: 1.3599257469177246, Validation Loss: 1.345688819885254\n",
      "Epoch 256, Training Loss: 1.3598787784576416, Validation Loss: 1.345563292503357\n",
      "Epoch 257, Training Loss: 1.3598331212997437, Validation Loss: 1.3455631732940674\n",
      "Epoch 258, Training Loss: 1.3597886562347412, Validation Loss: 1.3454936742782593\n",
      "Epoch 259, Training Loss: 1.3597447872161865, Validation Loss: 1.3454570770263672\n",
      "Epoch 260, Training Loss: 1.3597018718719482, Validation Loss: 1.3454234600067139\n",
      "Epoch 261, Training Loss: 1.3596594333648682, Validation Loss: 1.345359444618225\n",
      "Epoch 262, Training Loss: 1.3596174716949463, Validation Loss: 1.3453445434570312\n",
      "Epoch 263, Training Loss: 1.3595761060714722, Validation Loss: 1.345275640487671\n",
      "Epoch 264, Training Loss: 1.3595350980758667, Validation Loss: 1.3452558517456055\n",
      "Epoch 265, Training Loss: 1.3594945669174194, Validation Loss: 1.345210075378418\n",
      "Epoch 266, Training Loss: 1.3594543933868408, Validation Loss: 1.3451564311981201\n",
      "Epoch 267, Training Loss: 1.3594151735305786, Validation Loss: 1.3451553583145142\n",
      "Epoch 268, Training Loss: 1.3593764305114746, Validation Loss: 1.3450639247894287\n",
      "Epoch 269, Training Loss: 1.3593381643295288, Validation Loss: 1.345106601715088\n",
      "Epoch 270, Training Loss: 1.3593007326126099, Validation Loss: 1.3449687957763672\n",
      "Epoch 271, Training Loss: 1.3592638969421387, Validation Loss: 1.3450742959976196\n",
      "Epoch 272, Training Loss: 1.3592283725738525, Validation Loss: 1.3448467254638672\n",
      "Epoch 273, Training Loss: 1.3591954708099365, Validation Loss: 1.3450666666030884\n",
      "Epoch 274, Training Loss: 1.3591617345809937, Validation Loss: 1.3447288274765015\n",
      "Epoch 275, Training Loss: 1.359131097793579, Validation Loss: 1.3449671268463135\n",
      "Epoch 276, Training Loss: 1.3590868711471558, Validation Loss: 1.3447859287261963\n",
      "Epoch 277, Training Loss: 1.3590469360351562, Validation Loss: 1.3447637557983398\n",
      "Epoch 278, Training Loss: 1.3590118885040283, Validation Loss: 1.3448482751846313\n",
      "Epoch 279, Training Loss: 1.3589801788330078, Validation Loss: 1.3446245193481445\n",
      "Epoch 280, Training Loss: 1.3589500188827515, Validation Loss: 1.3448588848114014\n",
      "Epoch 281, Training Loss: 1.3589202165603638, Validation Loss: 1.344491720199585\n",
      "Epoch 282, Training Loss: 1.3588982820510864, Validation Loss: 1.3448277711868286\n",
      "Epoch 283, Training Loss: 1.3588584661483765, Validation Loss: 1.3444539308547974\n",
      "Epoch 284, Training Loss: 1.3588255643844604, Validation Loss: 1.3446780443191528\n",
      "Epoch 285, Training Loss: 1.358784556388855, Validation Loss: 1.3445453643798828\n",
      "Epoch 286, Training Loss: 1.3587486743927002, Validation Loss: 1.3444643020629883\n",
      "Epoch 287, Training Loss: 1.3587182760238647, Validation Loss: 1.3445583581924438\n",
      "Epoch 288, Training Loss: 1.3586875200271606, Validation Loss: 1.3444348573684692\n",
      "Epoch 289, Training Loss: 1.3586554527282715, Validation Loss: 1.3444663286209106\n",
      "Epoch 290, Training Loss: 1.3586244583129883, Validation Loss: 1.3444514274597168\n",
      "Epoch 291, Training Loss: 1.3585948944091797, Validation Loss: 1.3443658351898193\n",
      "Epoch 292, Training Loss: 1.358566164970398, Validation Loss: 1.344448447227478\n",
      "Epoch 293, Training Loss: 1.3585381507873535, Validation Loss: 1.3442389965057373\n",
      "Epoch 294, Training Loss: 1.3585134744644165, Validation Loss: 1.34445321559906\n",
      "Epoch 295, Training Loss: 1.3584856986999512, Validation Loss: 1.3441581726074219\n",
      "Epoch 296, Training Loss: 1.3584579229354858, Validation Loss: 1.3443607091903687\n",
      "Epoch 297, Training Loss: 1.3584270477294922, Validation Loss: 1.3442034721374512\n",
      "Epoch 298, Training Loss: 1.3583941459655762, Validation Loss: 1.3441336154937744\n",
      "Epoch 299, Training Loss: 1.3583685159683228, Validation Loss: 1.3443232774734497\n",
      "Epoch 300, Training Loss: 1.3583472967147827, Validation Loss: 1.3440234661102295\n",
      "Epoch 301, Training Loss: 1.358322262763977, Validation Loss: 1.3442943096160889\n",
      "Epoch 302, Training Loss: 1.3582942485809326, Validation Loss: 1.3439961671829224\n",
      "Epoch 303, Training Loss: 1.3582674264907837, Validation Loss: 1.3441895246505737\n",
      "Epoch 304, Training Loss: 1.3582357168197632, Validation Loss: 1.3440403938293457\n",
      "Epoch 305, Training Loss: 1.3582072257995605, Validation Loss: 1.3440459966659546\n",
      "Epoch 306, Training Loss: 1.3581809997558594, Validation Loss: 1.3440674543380737\n",
      "Epoch 307, Training Loss: 1.3581563234329224, Validation Loss: 1.3439607620239258\n",
      "Epoch 308, Training Loss: 1.3581310510635376, Validation Loss: 1.3439483642578125\n",
      "Epoch 309, Training Loss: 1.3581055402755737, Validation Loss: 1.3439756631851196\n",
      "Epoch 310, Training Loss: 1.358081340789795, Validation Loss: 1.343867540359497\n",
      "Epoch 311, Training Loss: 1.3580573797225952, Validation Loss: 1.3439580202102661\n",
      "Epoch 312, Training Loss: 1.358034372329712, Validation Loss: 1.3437507152557373\n",
      "Epoch 313, Training Loss: 1.358016848564148, Validation Loss: 1.3440943956375122\n",
      "Epoch 314, Training Loss: 1.3580076694488525, Validation Loss: 1.3435989618301392\n",
      "Epoch 315, Training Loss: 1.3579992055892944, Validation Loss: 1.344200611114502\n",
      "Epoch 316, Training Loss: 1.3579931259155273, Validation Loss: 1.343430757522583\n",
      "Epoch 317, Training Loss: 1.3580347299575806, Validation Loss: 1.3445066213607788\n",
      "Epoch 318, Training Loss: 1.358081340789795, Validation Loss: 1.3432567119598389\n",
      "Epoch 319, Training Loss: 1.3582135438919067, Validation Loss: 1.344473958015442\n",
      "Epoch 320, Training Loss: 1.358028531074524, Validation Loss: 1.343388557434082\n",
      "Epoch 321, Training Loss: 1.3579068183898926, Validation Loss: 1.343682885169983\n",
      "Epoch 322, Training Loss: 1.357812523841858, Validation Loss: 1.344057559967041\n",
      "Epoch 323, Training Loss: 1.3578486442565918, Validation Loss: 1.3432387113571167\n",
      "Epoch 324, Training Loss: 1.3579214811325073, Validation Loss: 1.3441914319992065\n",
      "Epoch 325, Training Loss: 1.3578476905822754, Validation Loss: 1.3433316946029663\n",
      "Epoch 326, Training Loss: 1.3577815294265747, Validation Loss: 1.3438137769699097\n",
      "Epoch 327, Training Loss: 1.3577138185501099, Validation Loss: 1.343585729598999\n",
      "Epoch 328, Training Loss: 1.3576788902282715, Validation Loss: 1.3434734344482422\n",
      "Epoch 329, Training Loss: 1.3576627969741821, Validation Loss: 1.3437420129776\n",
      "Epoch 330, Training Loss: 1.3576499223709106, Validation Loss: 1.3433557748794556\n",
      "Epoch 331, Training Loss: 1.3576326370239258, Validation Loss: 1.3437143564224243\n",
      "Epoch 332, Training Loss: 1.3576141595840454, Validation Loss: 1.3433433771133423\n",
      "Epoch 333, Training Loss: 1.3575876951217651, Validation Loss: 1.3435983657836914\n",
      "Epoch 334, Training Loss: 1.3575637340545654, Validation Loss: 1.3433295488357544\n",
      "Epoch 335, Training Loss: 1.3575453758239746, Validation Loss: 1.3436126708984375\n",
      "Epoch 336, Training Loss: 1.3575299978256226, Validation Loss: 1.3433043956756592\n",
      "Epoch 337, Training Loss: 1.3575068712234497, Validation Loss: 1.3435430526733398\n",
      "Epoch 338, Training Loss: 1.3574858903884888, Validation Loss: 1.3433513641357422\n",
      "Epoch 339, Training Loss: 1.3574622869491577, Validation Loss: 1.343392252922058\n",
      "Epoch 340, Training Loss: 1.3574419021606445, Validation Loss: 1.3433901071548462\n",
      "Epoch 341, Training Loss: 1.3574227094650269, Validation Loss: 1.3432954549789429\n",
      "Epoch 342, Training Loss: 1.3574057817459106, Validation Loss: 1.343436360359192\n",
      "Epoch 343, Training Loss: 1.3573917150497437, Validation Loss: 1.3431689739227295\n",
      "Epoch 344, Training Loss: 1.3573827743530273, Validation Loss: 1.343575119972229\n",
      "Epoch 345, Training Loss: 1.3573813438415527, Validation Loss: 1.3430627584457397\n",
      "Epoch 346, Training Loss: 1.3573700189590454, Validation Loss: 1.3436225652694702\n",
      "Epoch 347, Training Loss: 1.357357382774353, Validation Loss: 1.3430310487747192\n",
      "Epoch 348, Training Loss: 1.3573416471481323, Validation Loss: 1.3436081409454346\n",
      "Epoch 349, Training Loss: 1.3573256731033325, Validation Loss: 1.3430098295211792\n",
      "Epoch 350, Training Loss: 1.3573073148727417, Validation Loss: 1.3435723781585693\n",
      "Epoch 351, Training Loss: 1.3572887182235718, Validation Loss: 1.3430030345916748\n",
      "Epoch 352, Training Loss: 1.3572629690170288, Validation Loss: 1.3434797525405884\n",
      "Epoch 353, Training Loss: 1.3572417497634888, Validation Loss: 1.3430042266845703\n",
      "Epoch 354, Training Loss: 1.3572189807891846, Validation Loss: 1.3434100151062012\n",
      "Epoch 355, Training Loss: 1.3572027683258057, Validation Loss: 1.3430120944976807\n",
      "Epoch 356, Training Loss: 1.357177734375, Validation Loss: 1.3433119058609009\n",
      "Epoch 357, Training Loss: 1.35715913772583, Validation Loss: 1.3430248498916626\n",
      "Epoch 358, Training Loss: 1.3571391105651855, Validation Loss: 1.343271255493164\n",
      "Epoch 359, Training Loss: 1.35712468624115, Validation Loss: 1.3430365324020386\n",
      "Epoch 360, Training Loss: 1.357103705406189, Validation Loss: 1.3431565761566162\n",
      "Epoch 361, Training Loss: 1.357086181640625, Validation Loss: 1.3430345058441162\n",
      "Epoch 362, Training Loss: 1.3570698499679565, Validation Loss: 1.3431223630905151\n",
      "Epoch 363, Training Loss: 1.3570542335510254, Validation Loss: 1.342978835105896\n",
      "Epoch 364, Training Loss: 1.357040286064148, Validation Loss: 1.3431628942489624\n",
      "Epoch 365, Training Loss: 1.3570302724838257, Validation Loss: 1.3428250551223755\n",
      "Epoch 366, Training Loss: 1.3570325374603271, Validation Loss: 1.343409538269043\n",
      "Epoch 367, Training Loss: 1.3570513725280762, Validation Loss: 1.3426457643508911\n",
      "Epoch 368, Training Loss: 1.3570820093154907, Validation Loss: 1.343675136566162\n",
      "Epoch 369, Training Loss: 1.3571287393569946, Validation Loss: 1.3424954414367676\n",
      "Epoch 370, Training Loss: 1.3572473526000977, Validation Loss: 1.3438493013381958\n",
      "Epoch 371, Training Loss: 1.357190728187561, Validation Loss: 1.3424807786941528\n",
      "Epoch 372, Training Loss: 1.357161045074463, Validation Loss: 1.34341299533844\n",
      "Epoch 373, Training Loss: 1.356989860534668, Validation Loss: 1.342782735824585\n",
      "Epoch 374, Training Loss: 1.3569046258926392, Validation Loss: 1.3428001403808594\n",
      "Epoch 375, Training Loss: 1.3568860292434692, Validation Loss: 1.3432866334915161\n",
      "Epoch 376, Training Loss: 1.3569210767745972, Validation Loss: 1.342492938041687\n",
      "Epoch 377, Training Loss: 1.3569923639297485, Validation Loss: 1.343576192855835\n",
      "Epoch 378, Training Loss: 1.357008934020996, Validation Loss: 1.3424216508865356\n",
      "Epoch 379, Training Loss: 1.3570504188537598, Validation Loss: 1.3435323238372803\n",
      "Epoch 380, Training Loss: 1.3569731712341309, Validation Loss: 1.342475414276123\n",
      "Epoch 381, Training Loss: 1.3569141626358032, Validation Loss: 1.343183994293213\n",
      "Epoch 382, Training Loss: 1.3568298816680908, Validation Loss: 1.3427064418792725\n",
      "Epoch 383, Training Loss: 1.3567771911621094, Validation Loss: 1.3427475690841675\n",
      "Epoch 384, Training Loss: 1.3567593097686768, Validation Loss: 1.3430359363555908\n",
      "Epoch 385, Training Loss: 1.3567677736282349, Validation Loss: 1.3425171375274658\n",
      "Epoch 386, Training Loss: 1.356783151626587, Validation Loss: 1.3431931734085083\n",
      "Epoch 387, Training Loss: 1.35678231716156, Validation Loss: 1.3424620628356934\n",
      "Epoch 388, Training Loss: 1.3567805290222168, Validation Loss: 1.3431588411331177\n",
      "Epoch 389, Training Loss: 1.356752634048462, Validation Loss: 1.3425270318984985\n",
      "Epoch 390, Training Loss: 1.3567180633544922, Validation Loss: 1.342953085899353\n",
      "Epoch 391, Training Loss: 1.3566837310791016, Validation Loss: 1.3427278995513916\n",
      "Epoch 392, Training Loss: 1.3566564321517944, Validation Loss: 1.342627763748169\n",
      "Epoch 393, Training Loss: 1.3566505908966064, Validation Loss: 1.3429949283599854\n",
      "Epoch 394, Training Loss: 1.356658697128296, Validation Loss: 1.3424724340438843\n",
      "Epoch 395, Training Loss: 1.356661081314087, Validation Loss: 1.3430676460266113\n",
      "Epoch 396, Training Loss: 1.3566534519195557, Validation Loss: 1.3424692153930664\n",
      "Epoch 397, Training Loss: 1.3566336631774902, Validation Loss: 1.3429620265960693\n",
      "Epoch 398, Training Loss: 1.3566092252731323, Validation Loss: 1.3425801992416382\n",
      "Epoch 399, Training Loss: 1.356580138206482, Validation Loss: 1.3427408933639526\n",
      "Epoch 400, Training Loss: 1.3565609455108643, Validation Loss: 1.3427258729934692\n",
      "Epoch 401, Training Loss: 1.3565490245819092, Validation Loss: 1.3425853252410889\n",
      "Epoch 402, Training Loss: 1.3565418720245361, Validation Loss: 1.3428537845611572\n",
      "Epoch 403, Training Loss: 1.356539249420166, Validation Loss: 1.3424724340438843\n",
      "Epoch 404, Training Loss: 1.3565362691879272, Validation Loss: 1.3429515361785889\n",
      "Epoch 405, Training Loss: 1.3565361499786377, Validation Loss: 1.3424257040023804\n",
      "Epoch 406, Training Loss: 1.3565232753753662, Validation Loss: 1.3429287672042847\n",
      "Epoch 407, Training Loss: 1.3565117120742798, Validation Loss: 1.3424447774887085\n",
      "Epoch 408, Training Loss: 1.3564921617507935, Validation Loss: 1.3428370952606201\n",
      "Epoch 409, Training Loss: 1.3564753532409668, Validation Loss: 1.3425265550613403\n",
      "Epoch 410, Training Loss: 1.3564529418945312, Validation Loss: 1.342652440071106\n",
      "Epoch 411, Training Loss: 1.3564375638961792, Validation Loss: 1.34262216091156\n",
      "Epoch 412, Training Loss: 1.3564260005950928, Validation Loss: 1.3425229787826538\n",
      "Epoch 413, Training Loss: 1.356416940689087, Validation Loss: 1.3426886796951294\n",
      "Epoch 414, Training Loss: 1.3564097881317139, Validation Loss: 1.3424288034439087\n",
      "Epoch 415, Training Loss: 1.3564043045043945, Validation Loss: 1.3427894115447998\n",
      "Epoch 416, Training Loss: 1.3564049005508423, Validation Loss: 1.3423161506652832\n",
      "Epoch 417, Training Loss: 1.3564077615737915, Validation Loss: 1.3429126739501953\n",
      "Epoch 418, Training Loss: 1.3564151525497437, Validation Loss: 1.3422133922576904\n",
      "Epoch 419, Training Loss: 1.356429934501648, Validation Loss: 1.3430579900741577\n",
      "Epoch 420, Training Loss: 1.356447696685791, Validation Loss: 1.3421419858932495\n",
      "Epoch 421, Training Loss: 1.3564567565917969, Validation Loss: 1.3431155681610107\n",
      "Epoch 422, Training Loss: 1.3564558029174805, Validation Loss: 1.342081904411316\n",
      "Epoch 423, Training Loss: 1.3564839363098145, Validation Loss: 1.3432050943374634\n",
      "Epoch 424, Training Loss: 1.3564783334732056, Validation Loss: 1.3420274257659912\n",
      "Epoch 425, Training Loss: 1.3565032482147217, Validation Loss: 1.3431276082992554\n",
      "Epoch 426, Training Loss: 1.3564327955245972, Validation Loss: 1.3421443700790405\n",
      "Epoch 427, Training Loss: 1.356346845626831, Validation Loss: 1.3426740169525146\n",
      "Epoch 428, Training Loss: 1.3562813997268677, Validation Loss: 1.3424712419509888\n",
      "Epoch 429, Training Loss: 1.3562535047531128, Validation Loss: 1.3422660827636719\n",
      "Epoch 430, Training Loss: 1.356263518333435, Validation Loss: 1.3428287506103516\n",
      "Epoch 431, Training Loss: 1.356294870376587, Validation Loss: 1.3420679569244385\n",
      "Epoch 432, Training Loss: 1.3563334941864014, Validation Loss: 1.3430182933807373\n",
      "Epoch 433, Training Loss: 1.356345772743225, Validation Loss: 1.3420169353485107\n",
      "Epoch 434, Training Loss: 1.3563475608825684, Validation Loss: 1.3429388999938965\n",
      "Epoch 435, Training Loss: 1.3563034534454346, Validation Loss: 1.3421275615692139\n",
      "Epoch 436, Training Loss: 1.3562405109405518, Validation Loss: 1.342568278312683\n",
      "Epoch 437, Training Loss: 1.3561898469924927, Validation Loss: 1.3424264192581177\n",
      "Epoch 438, Training Loss: 1.3561691045761108, Validation Loss: 1.34220290184021\n",
      "Epoch 439, Training Loss: 1.3561785221099854, Validation Loss: 1.3427624702453613\n",
      "Epoch 440, Training Loss: 1.3562091588974, Validation Loss: 1.3420027494430542\n",
      "Epoch 441, Training Loss: 1.356247901916504, Validation Loss: 1.342963695526123\n",
      "Epoch 442, Training Loss: 1.3562625646591187, Validation Loss: 1.342005729675293\n",
      "Epoch 443, Training Loss: 1.3562290668487549, Validation Loss: 1.342759132385254\n",
      "Epoch 444, Training Loss: 1.3561781644821167, Validation Loss: 1.3421725034713745\n",
      "Epoch 445, Training Loss: 1.3561328649520874, Validation Loss: 1.3424683809280396\n",
      "Epoch 446, Training Loss: 1.3561022281646729, Validation Loss: 1.3424243927001953\n",
      "Epoch 447, Training Loss: 1.3560903072357178, Validation Loss: 1.3421891927719116\n",
      "Epoch 448, Training Loss: 1.3560960292816162, Validation Loss: 1.3426727056503296\n",
      "Epoch 449, Training Loss: 1.356114149093628, Validation Loss: 1.3420034646987915\n",
      "Epoch 450, Training Loss: 1.3561453819274902, Validation Loss: 1.3428950309753418\n",
      "Epoch 451, Training Loss: 1.3561700582504272, Validation Loss: 1.3419407606124878\n",
      "Epoch 452, Training Loss: 1.3561725616455078, Validation Loss: 1.3428078889846802\n",
      "Epoch 453, Training Loss: 1.356124997138977, Validation Loss: 1.3420791625976562\n",
      "Epoch 454, Training Loss: 1.3560725450515747, Validation Loss: 1.342512607574463\n",
      "Epoch 455, Training Loss: 1.3560351133346558, Validation Loss: 1.3422927856445312\n",
      "Epoch 456, Training Loss: 1.3560144901275635, Validation Loss: 1.3422521352767944\n",
      "Epoch 457, Training Loss: 1.3560080528259277, Validation Loss: 1.3425270318984985\n",
      "Epoch 458, Training Loss: 1.3560155630111694, Validation Loss: 1.3420472145080566\n",
      "Epoch 459, Training Loss: 1.356032133102417, Validation Loss: 1.342725157737732\n",
      "Epoch 460, Training Loss: 1.356052279472351, Validation Loss: 1.3419667482376099\n",
      "Epoch 461, Training Loss: 1.3560477495193481, Validation Loss: 1.3427200317382812\n",
      "Epoch 462, Training Loss: 1.356038212776184, Validation Loss: 1.3420040607452393\n",
      "Epoch 463, Training Loss: 1.3560106754302979, Validation Loss: 1.3425674438476562\n",
      "Epoch 464, Training Loss: 1.3559834957122803, Validation Loss: 1.3421679735183716\n",
      "Epoch 465, Training Loss: 1.3559494018554688, Validation Loss: 1.3422842025756836\n",
      "Epoch 466, Training Loss: 1.355933666229248, Validation Loss: 1.3423811197280884\n",
      "Epoch 467, Training Loss: 1.3559311628341675, Validation Loss: 1.3420894145965576\n",
      "Epoch 468, Training Loss: 1.3559340238571167, Validation Loss: 1.3425321578979492\n",
      "Epoch 469, Training Loss: 1.355942964553833, Validation Loss: 1.3419381380081177\n",
      "Epoch 470, Training Loss: 1.355962872505188, Validation Loss: 1.3427294492721558\n",
      "Epoch 471, Training Loss: 1.3559887409210205, Validation Loss: 1.3418484926223755\n",
      "Epoch 472, Training Loss: 1.3559985160827637, Validation Loss: 1.3427879810333252\n",
      "Epoch 473, Training Loss: 1.3559980392456055, Validation Loss: 1.3418623208999634\n",
      "Epoch 474, Training Loss: 1.35597825050354, Validation Loss: 1.3426772356033325\n",
      "Epoch 475, Training Loss: 1.3559479713439941, Validation Loss: 1.3419623374938965\n",
      "Epoch 476, Training Loss: 1.3559097051620483, Validation Loss: 1.342465877532959\n",
      "Epoch 477, Training Loss: 1.3558776378631592, Validation Loss: 1.342126727104187\n",
      "Epoch 478, Training Loss: 1.3558493852615356, Validation Loss: 1.3422091007232666\n",
      "Epoch 479, Training Loss: 1.3558349609375, Validation Loss: 1.342322587966919\n",
      "Epoch 480, Training Loss: 1.3558337688446045, Validation Loss: 1.3419824838638306\n",
      "Epoch 481, Training Loss: 1.3558458089828491, Validation Loss: 1.3425508737564087\n",
      "Epoch 482, Training Loss: 1.3558673858642578, Validation Loss: 1.3418159484863281\n",
      "Epoch 483, Training Loss: 1.3559027910232544, Validation Loss: 1.342807412147522\n",
      "Epoch 484, Training Loss: 1.3559415340423584, Validation Loss: 1.341715693473816\n",
      "Epoch 485, Training Loss: 1.3559941053390503, Validation Loss: 1.3429101705551147\n",
      "Epoch 486, Training Loss: 1.3559720516204834, Validation Loss: 1.341747760772705\n",
      "Epoch 487, Training Loss: 1.3559490442276, Validation Loss: 1.3427454233169556\n",
      "Epoch 488, Training Loss: 1.3558948040008545, Validation Loss: 1.3418388366699219\n",
      "Epoch 489, Training Loss: 1.3558531999588013, Validation Loss: 1.3425019979476929\n",
      "Epoch 490, Training Loss: 1.355804204940796, Validation Loss: 1.3420330286026\n",
      "Epoch 491, Training Loss: 1.3557615280151367, Validation Loss: 1.3421642780303955\n",
      "Epoch 492, Training Loss: 1.3557435274124146, Validation Loss: 1.3423088788986206\n",
      "Epoch 493, Training Loss: 1.3557472229003906, Validation Loss: 1.341883897781372\n",
      "Epoch 494, Training Loss: 1.3557732105255127, Validation Loss: 1.3426299095153809\n",
      "Epoch 495, Training Loss: 1.3558142185211182, Validation Loss: 1.3417543172836304\n",
      "Epoch 496, Training Loss: 1.3558319807052612, Validation Loss: 1.3427146673202515\n",
      "Epoch 497, Training Loss: 1.3558332920074463, Validation Loss: 1.3417690992355347\n",
      "Epoch 498, Training Loss: 1.3558175563812256, Validation Loss: 1.3426023721694946\n",
      "Epoch 499, Training Loss: 1.3557837009429932, Validation Loss: 1.3418828248977661\n",
      "Epoch 500, Training Loss: 1.3557415008544922, Validation Loss: 1.3423482179641724\n",
      "Epoch 501, Training Loss: 1.3557045459747314, Validation Loss: 1.3420703411102295\n",
      "Epoch 502, Training Loss: 1.3556809425354004, Validation Loss: 1.3420894145965576\n",
      "Epoch 503, Training Loss: 1.3556714057922363, Validation Loss: 1.3422776460647583\n",
      "Epoch 504, Training Loss: 1.3556759357452393, Validation Loss: 1.3418625593185425\n",
      "Epoch 505, Training Loss: 1.3556944131851196, Validation Loss: 1.3425394296646118\n",
      "Epoch 506, Training Loss: 1.355724573135376, Validation Loss: 1.3417003154754639\n",
      "Epoch 507, Training Loss: 1.355770230293274, Validation Loss: 1.3427276611328125\n",
      "Epoch 508, Training Loss: 1.3557777404785156, Validation Loss: 1.3416814804077148\n",
      "Epoch 509, Training Loss: 1.3557853698730469, Validation Loss: 1.3426531553268433\n",
      "Epoch 510, Training Loss: 1.3557384014129639, Validation Loss: 1.341779351234436\n",
      "Epoch 511, Training Loss: 1.35570228099823, Validation Loss: 1.342456579208374\n",
      "Epoch 512, Training Loss: 1.3556644916534424, Validation Loss: 1.341952919960022\n",
      "Epoch 513, Training Loss: 1.3556253910064697, Validation Loss: 1.3421673774719238\n",
      "Epoch 514, Training Loss: 1.3556036949157715, Validation Loss: 1.3421716690063477\n",
      "Epoch 515, Training Loss: 1.3555974960327148, Validation Loss: 1.3419370651245117\n",
      "Epoch 516, Training Loss: 1.3556036949157715, Validation Loss: 1.3424042463302612\n",
      "Epoch 517, Training Loss: 1.3556239604949951, Validation Loss: 1.3417598009109497\n",
      "Epoch 518, Training Loss: 1.3556501865386963, Validation Loss: 1.342637300491333\n",
      "Epoch 519, Training Loss: 1.355682611465454, Validation Loss: 1.34168541431427\n",
      "Epoch 520, Training Loss: 1.3556969165802002, Validation Loss: 1.3427178859710693\n",
      "Epoch 521, Training Loss: 1.3557014465332031, Validation Loss: 1.341687798500061\n",
      "Epoch 522, Training Loss: 1.3557026386260986, Validation Loss: 1.342689037322998\n",
      "Epoch 523, Training Loss: 1.3556808233261108, Validation Loss: 1.3417131900787354\n",
      "Epoch 524, Training Loss: 1.3556675910949707, Validation Loss: 1.3425931930541992\n",
      "Epoch 525, Training Loss: 1.3556361198425293, Validation Loss: 1.3417856693267822\n",
      "Epoch 526, Training Loss: 1.3555938005447388, Validation Loss: 1.342347502708435\n",
      "Epoch 527, Training Loss: 1.3555550575256348, Validation Loss: 1.3419848680496216\n",
      "Epoch 528, Training Loss: 1.355522871017456, Validation Loss: 1.3420215845108032\n",
      "Epoch 529, Training Loss: 1.3555141687393188, Validation Loss: 1.3422517776489258\n",
      "Epoch 530, Training Loss: 1.355523705482483, Validation Loss: 1.341770052909851\n",
      "Epoch 531, Training Loss: 1.355553150177002, Validation Loss: 1.3425683975219727\n",
      "Epoch 532, Training Loss: 1.3555964231491089, Validation Loss: 1.3416051864624023\n",
      "Epoch 533, Training Loss: 1.3556569814682007, Validation Loss: 1.3427547216415405\n",
      "Epoch 534, Training Loss: 1.3556586503982544, Validation Loss: 1.3415944576263428\n",
      "Epoch 535, Training Loss: 1.3556568622589111, Validation Loss: 1.3426049947738647\n",
      "Epoch 536, Training Loss: 1.3555924892425537, Validation Loss: 1.3417038917541504\n",
      "Epoch 537, Training Loss: 1.3555470705032349, Validation Loss: 1.3423620462417603\n",
      "Epoch 538, Training Loss: 1.3555058240890503, Validation Loss: 1.341867208480835\n",
      "Epoch 539, Training Loss: 1.3554733991622925, Validation Loss: 1.3421024084091187\n",
      "Epoch 540, Training Loss: 1.3554543256759644, Validation Loss: 1.3420660495758057\n",
      "Epoch 541, Training Loss: 1.3554469347000122, Validation Loss: 1.3418972492218018\n",
      "Epoch 542, Training Loss: 1.3554491996765137, Validation Loss: 1.3422574996948242\n",
      "Epoch 543, Training Loss: 1.3554600477218628, Validation Loss: 1.3417154550552368\n",
      "Epoch 544, Training Loss: 1.3554887771606445, Validation Loss: 1.3425536155700684\n",
      "Epoch 545, Training Loss: 1.3555290699005127, Validation Loss: 1.341575026512146\n",
      "Epoch 546, Training Loss: 1.3555858135223389, Validation Loss: 1.3427329063415527\n",
      "Epoch 547, Training Loss: 1.3555865287780762, Validation Loss: 1.341587781906128\n",
      "Epoch 548, Training Loss: 1.355574369430542, Validation Loss: 1.3425757884979248\n",
      "Epoch 549, Training Loss: 1.3555148839950562, Validation Loss: 1.3417242765426636\n",
      "Epoch 550, Training Loss: 1.3554648160934448, Validation Loss: 1.342313528060913\n",
      "Epoch 551, Training Loss: 1.355425477027893, Validation Loss: 1.341929316520691\n",
      "Epoch 552, Training Loss: 1.3553951978683472, Validation Loss: 1.342028260231018\n",
      "Epoch 553, Training Loss: 1.3553829193115234, Validation Loss: 1.3421541452407837\n",
      "Epoch 554, Training Loss: 1.355385184288025, Validation Loss: 1.3418010473251343\n",
      "Epoch 555, Training Loss: 1.355400800704956, Validation Loss: 1.3424235582351685\n",
      "Epoch 556, Training Loss: 1.3554309606552124, Validation Loss: 1.3416646718978882\n",
      "Epoch 557, Training Loss: 1.3554489612579346, Validation Loss: 1.3425853252410889\n",
      "Epoch 558, Training Loss: 1.3554733991622925, Validation Loss: 1.3415762186050415\n",
      "Epoch 559, Training Loss: 1.355522632598877, Validation Loss: 1.3427354097366333\n",
      "Epoch 560, Training Loss: 1.3555231094360352, Validation Loss: 1.3415706157684326\n",
      "Epoch 561, Training Loss: 1.3555216789245605, Validation Loss: 1.3426121473312378\n",
      "Epoch 562, Training Loss: 1.355466365814209, Validation Loss: 1.34165358543396\n",
      "Epoch 563, Training Loss: 1.3554275035858154, Validation Loss: 1.3424124717712402\n",
      "Epoch 564, Training Loss: 1.3553897142410278, Validation Loss: 1.3417679071426392\n",
      "Epoch 565, Training Loss: 1.3553578853607178, Validation Loss: 1.3422141075134277\n",
      "Epoch 566, Training Loss: 1.3553378582000732, Validation Loss: 1.3418378829956055\n",
      "Epoch 567, Training Loss: 1.3553274869918823, Validation Loss: 1.3421703577041626\n",
      "Epoch 568, Training Loss: 1.3553218841552734, Validation Loss: 1.3418238162994385\n",
      "Epoch 569, Training Loss: 1.3553191423416138, Validation Loss: 1.3422167301177979\n",
      "Epoch 570, Training Loss: 1.3553204536437988, Validation Loss: 1.3417466878890991\n",
      "Epoch 571, Training Loss: 1.3553293943405151, Validation Loss: 1.3423880338668823\n",
      "Epoch 572, Training Loss: 1.3553485870361328, Validation Loss: 1.341663122177124\n",
      "Epoch 573, Training Loss: 1.3553590774536133, Validation Loss: 1.342509388923645\n",
      "Epoch 574, Training Loss: 1.355376958847046, Validation Loss: 1.3415800333023071\n",
      "Epoch 575, Training Loss: 1.3554160594940186, Validation Loss: 1.3427364826202393\n",
      "Epoch 576, Training Loss: 1.3554539680480957, Validation Loss: 1.3415039777755737\n",
      "Epoch 577, Training Loss: 1.3555009365081787, Validation Loss: 1.342809796333313\n",
      "Epoch 578, Training Loss: 1.3554750680923462, Validation Loss: 1.3415149450302124\n",
      "Epoch 579, Training Loss: 1.3554590940475464, Validation Loss: 1.3425984382629395\n",
      "Epoch 580, Training Loss: 1.355379581451416, Validation Loss: 1.341632604598999\n",
      "Epoch 581, Training Loss: 1.355332612991333, Validation Loss: 1.3423470258712769\n",
      "Epoch 582, Training Loss: 1.355293869972229, Validation Loss: 1.3418384790420532\n",
      "Epoch 583, Training Loss: 1.3552515506744385, Validation Loss: 1.342020034790039\n",
      "Epoch 584, Training Loss: 1.3552343845367432, Validation Loss: 1.3420886993408203\n",
      "Epoch 585, Training Loss: 1.3552342653274536, Validation Loss: 1.3418103456497192\n",
      "Epoch 586, Training Loss: 1.3552441596984863, Validation Loss: 1.3423128128051758\n",
      "Epoch 587, Training Loss: 1.3552623987197876, Validation Loss: 1.3416987657546997\n",
      "Epoch 588, Training Loss: 1.3552663326263428, Validation Loss: 1.3423874378204346\n",
      "Epoch 589, Training Loss: 1.3552725315093994, Validation Loss: 1.3416588306427002\n",
      "Epoch 590, Training Loss: 1.3552743196487427, Validation Loss: 1.3424326181411743\n",
      "Epoch 591, Training Loss: 1.3552778959274292, Validation Loss: 1.3416221141815186\n",
      "Epoch 592, Training Loss: 1.3552870750427246, Validation Loss: 1.3425239324569702\n",
      "Epoch 593, Training Loss: 1.355299949645996, Validation Loss: 1.3415520191192627\n",
      "Epoch 594, Training Loss: 1.3553322553634644, Validation Loss: 1.3427000045776367\n",
      "Epoch 595, Training Loss: 1.3553584814071655, Validation Loss: 1.3414829969406128\n",
      "Epoch 596, Training Loss: 1.3554030656814575, Validation Loss: 1.3427760601043701\n",
      "Epoch 597, Training Loss: 1.3553810119628906, Validation Loss: 1.3414897918701172\n",
      "Epoch 598, Training Loss: 1.3553767204284668, Validation Loss: 1.3426238298416138\n",
      "Epoch 599, Training Loss: 1.3553091287612915, Validation Loss: 1.3415963649749756\n",
      "Epoch 600, Training Loss: 1.3552640676498413, Validation Loss: 1.3423763513565063\n",
      "Epoch 601, Training Loss: 1.3552190065383911, Validation Loss: 1.341800332069397\n",
      "Epoch 602, Training Loss: 1.3551756143569946, Validation Loss: 1.3420549631118774\n",
      "Epoch 603, Training Loss: 1.3551541566848755, Validation Loss: 1.3420348167419434\n",
      "Epoch 604, Training Loss: 1.3551489114761353, Validation Loss: 1.3418426513671875\n",
      "Epoch 605, Training Loss: 1.355155110359192, Validation Loss: 1.3422647714614868\n",
      "Epoch 606, Training Loss: 1.3551721572875977, Validation Loss: 1.3417136669158936\n",
      "Epoch 607, Training Loss: 1.3551762104034424, Validation Loss: 1.3423362970352173\n",
      "Epoch 608, Training Loss: 1.3551801443099976, Validation Loss: 1.3417112827301025\n",
      "Epoch 609, Training Loss: 1.3551692962646484, Validation Loss: 1.342295527458191\n",
      "Epoch 610, Training Loss: 1.355161428451538, Validation Loss: 1.3417820930480957\n",
      "Epoch 611, Training Loss: 1.35514235496521, Validation Loss: 1.3421809673309326\n",
      "Epoch 612, Training Loss: 1.35513174533844, Validation Loss: 1.341859221458435\n",
      "Epoch 613, Training Loss: 1.355118751525879, Validation Loss: 1.3420921564102173\n",
      "Epoch 614, Training Loss: 1.3551108837127686, Validation Loss: 1.3418632745742798\n",
      "Epoch 615, Training Loss: 1.3551068305969238, Validation Loss: 1.342126488685608\n",
      "Epoch 616, Training Loss: 1.355107069015503, Validation Loss: 1.3417738676071167\n",
      "Epoch 617, Training Loss: 1.3551126718521118, Validation Loss: 1.342282772064209\n",
      "Epoch 618, Training Loss: 1.3551274538040161, Validation Loss: 1.3416236639022827\n",
      "Epoch 619, Training Loss: 1.3551640510559082, Validation Loss: 1.3426724672317505\n",
      "Epoch 620, Training Loss: 1.3552346229553223, Validation Loss: 1.3414428234100342\n",
      "Epoch 621, Training Loss: 1.3553787469863892, Validation Loss: 1.343226671218872\n",
      "Epoch 622, Training Loss: 1.3554798364639282, Validation Loss: 1.3413740396499634\n",
      "Epoch 623, Training Loss: 1.3557476997375488, Validation Loss: 1.3435420989990234\n",
      "Epoch 624, Training Loss: 1.355641484260559, Validation Loss: 1.3413302898406982\n",
      "Epoch 625, Training Loss: 1.3556736707687378, Validation Loss: 1.3428492546081543\n",
      "Epoch 626, Training Loss: 1.355292558670044, Validation Loss: 1.3417677879333496\n",
      "Epoch 627, Training Loss: 1.3550798892974854, Validation Loss: 1.3416990041732788\n",
      "Epoch 628, Training Loss: 1.3550915718078613, Validation Loss: 1.3428092002868652\n",
      "Epoch 629, Training Loss: 1.355258584022522, Validation Loss: 1.3413915634155273\n",
      "Epoch 630, Training Loss: 1.3555173873901367, Validation Loss: 1.3432066440582275\n",
      "Epoch 631, Training Loss: 1.355444312095642, Validation Loss: 1.3414250612258911\n",
      "Epoch 632, Training Loss: 1.355401873588562, Validation Loss: 1.342615008354187\n",
      "Epoch 633, Training Loss: 1.3551723957061768, Validation Loss: 1.3417836427688599\n",
      "Epoch 634, Training Loss: 1.3550453186035156, Validation Loss: 1.3417785167694092\n",
      "Epoch 635, Training Loss: 1.3550394773483276, Validation Loss: 1.3425008058547974\n",
      "Epoch 636, Training Loss: 1.3551301956176758, Validation Loss: 1.3413866758346558\n",
      "Epoch 637, Training Loss: 1.355265498161316, Validation Loss: 1.3428288698196411\n",
      "Epoch 638, Training Loss: 1.3552491664886475, Validation Loss: 1.3414703607559204\n",
      "Epoch 639, Training Loss: 1.3551666736602783, Validation Loss: 1.3422372341156006\n",
      "Epoch 640, Training Loss: 1.355040431022644, Validation Loss: 1.3420021533966064\n",
      "Epoch 641, Training Loss: 1.3550046682357788, Validation Loss: 1.3416807651519775\n",
      "Epoch 642, Training Loss: 1.3550478219985962, Validation Loss: 1.3425958156585693\n",
      "Epoch 643, Training Loss: 1.355127215385437, Validation Loss: 1.3415007591247559\n",
      "Epoch 644, Training Loss: 1.355183482170105, Validation Loss: 1.3426001071929932\n",
      "Epoch 645, Training Loss: 1.3551170825958252, Validation Loss: 1.3416966199874878\n",
      "Epoch 646, Training Loss: 1.355035424232483, Validation Loss: 1.3420888185501099\n",
      "Epoch 647, Training Loss: 1.354986310005188, Validation Loss: 1.3421372175216675\n",
      "Epoch 648, Training Loss: 1.3549875020980835, Validation Loss: 1.3416903018951416\n",
      "Epoch 649, Training Loss: 1.355026125907898, Validation Loss: 1.3425076007843018\n",
      "Epoch 650, Training Loss: 1.3550634384155273, Validation Loss: 1.341559648513794\n",
      "Epoch 651, Training Loss: 1.3550989627838135, Validation Loss: 1.3425252437591553\n",
      "Epoch 652, Training Loss: 1.3550593852996826, Validation Loss: 1.341672658920288\n",
      "Epoch 653, Training Loss: 1.355023980140686, Validation Loss: 1.342262625694275\n",
      "Epoch 654, Training Loss: 1.3549820184707642, Validation Loss: 1.341916561126709\n",
      "Epoch 655, Training Loss: 1.3549585342407227, Validation Loss: 1.3419854640960693\n",
      "Epoch 656, Training Loss: 1.3549511432647705, Validation Loss: 1.342189908027649\n",
      "Epoch 657, Training Loss: 1.3549575805664062, Validation Loss: 1.341776967048645\n",
      "Epoch 658, Training Loss: 1.3549751043319702, Validation Loss: 1.342416763305664\n",
      "Epoch 659, Training Loss: 1.3549959659576416, Validation Loss: 1.3416389226913452\n",
      "Epoch 660, Training Loss: 1.355024814605713, Validation Loss: 1.3425695896148682\n",
      "Epoch 661, Training Loss: 1.3550333976745605, Validation Loss: 1.3415801525115967\n",
      "Epoch 662, Training Loss: 1.3550529479980469, Validation Loss: 1.3425419330596924\n",
      "Epoch 663, Training Loss: 1.3550182580947876, Validation Loss: 1.3416496515274048\n",
      "Epoch 664, Training Loss: 1.3549962043762207, Validation Loss: 1.342383861541748\n",
      "Epoch 665, Training Loss: 1.3549656867980957, Validation Loss: 1.3417719602584839\n",
      "Epoch 666, Training Loss: 1.3549455404281616, Validation Loss: 1.3422473669052124\n",
      "Epoch 667, Training Loss: 1.3549293279647827, Validation Loss: 1.341894507408142\n",
      "Epoch 668, Training Loss: 1.3549175262451172, Validation Loss: 1.3421250581741333\n",
      "Epoch 669, Training Loss: 1.3549084663391113, Validation Loss: 1.3419960737228394\n",
      "Epoch 670, Training Loss: 1.3549025058746338, Validation Loss: 1.3420270681381226\n",
      "Epoch 671, Training Loss: 1.3548980951309204, Validation Loss: 1.3420586585998535\n",
      "Epoch 672, Training Loss: 1.3548952341079712, Validation Loss: 1.3419370651245117\n",
      "Epoch 673, Training Loss: 1.3548939228057861, Validation Loss: 1.342127799987793\n",
      "Epoch 674, Training Loss: 1.354893445968628, Validation Loss: 1.3418562412261963\n",
      "Epoch 675, Training Loss: 1.3548952341079712, Validation Loss: 1.3422389030456543\n",
      "Epoch 676, Training Loss: 1.354899287223816, Validation Loss: 1.3417574167251587\n",
      "Epoch 677, Training Loss: 1.3549121618270874, Validation Loss: 1.3424501419067383\n",
      "Epoch 678, Training Loss: 1.3549336194992065, Validation Loss: 1.3416072130203247\n",
      "Epoch 679, Training Loss: 1.354980230331421, Validation Loss: 1.3427913188934326\n",
      "Epoch 680, Training Loss: 1.3550379276275635, Validation Loss: 1.3414547443389893\n",
      "Epoch 681, Training Loss: 1.3551392555236816, Validation Loss: 1.3431079387664795\n",
      "Epoch 682, Training Loss: 1.3551673889160156, Validation Loss: 1.341393232345581\n",
      "Epoch 683, Training Loss: 1.3552194833755493, Validation Loss: 1.3429317474365234\n",
      "Epoch 684, Training Loss: 1.3550894260406494, Validation Loss: 1.3415056467056274\n",
      "Epoch 685, Training Loss: 1.3550044298171997, Validation Loss: 1.3423833847045898\n",
      "Epoch 686, Training Loss: 1.3549039363861084, Validation Loss: 1.3418248891830444\n",
      "Epoch 687, Training Loss: 1.3548591136932373, Validation Loss: 1.342005729675293\n",
      "Epoch 688, Training Loss: 1.3548431396484375, Validation Loss: 1.3421719074249268\n",
      "Epoch 689, Training Loss: 1.354848861694336, Validation Loss: 1.341748595237732\n",
      "Epoch 690, Training Loss: 1.3548771142959595, Validation Loss: 1.3425359725952148\n",
      "Epoch 691, Training Loss: 1.3549177646636963, Validation Loss: 1.3415144681930542\n",
      "Epoch 692, Training Loss: 1.3550093173980713, Validation Loss: 1.3430490493774414\n",
      "Epoch 693, Training Loss: 1.3551090955734253, Validation Loss: 1.3413337469100952\n",
      "Epoch 694, Training Loss: 1.355290412902832, Validation Loss: 1.343268871307373\n",
      "Epoch 695, Training Loss: 1.355211853981018, Validation Loss: 1.3413242101669312\n",
      "Epoch 696, Training Loss: 1.3551676273345947, Validation Loss: 1.3426443338394165\n",
      "Epoch 697, Training Loss: 1.354961633682251, Validation Loss: 1.341697335243225\n",
      "Epoch 698, Training Loss: 1.3548498153686523, Validation Loss: 1.3419556617736816\n",
      "Epoch 699, Training Loss: 1.354811429977417, Validation Loss: 1.342332363128662\n",
      "Epoch 700, Training Loss: 1.3548401594161987, Validation Loss: 1.3416239023208618\n",
      "Epoch 701, Training Loss: 1.3549206256866455, Validation Loss: 1.3429056406021118\n",
      "Epoch 702, Training Loss: 1.3550043106079102, Validation Loss: 1.3415098190307617\n",
      "Epoch 703, Training Loss: 1.3550870418548584, Validation Loss: 1.3431066274642944\n",
      "Epoch 704, Training Loss: 1.3550834655761719, Validation Loss: 1.3415039777755737\n",
      "Epoch 705, Training Loss: 1.355035424232483, Validation Loss: 1.3426631689071655\n",
      "Epoch 706, Training Loss: 1.3549014329910278, Validation Loss: 1.3417361974716187\n",
      "Epoch 707, Training Loss: 1.3548253774642944, Validation Loss: 1.3420569896697998\n",
      "Epoch 708, Training Loss: 1.3547894954681396, Validation Loss: 1.3421467542648315\n",
      "Epoch 709, Training Loss: 1.3547930717468262, Validation Loss: 1.3417236804962158\n",
      "Epoch 710, Training Loss: 1.3548228740692139, Validation Loss: 1.3425742387771606\n",
      "Epoch 711, Training Loss: 1.3548589944839478, Validation Loss: 1.341599941253662\n",
      "Epoch 712, Training Loss: 1.354910135269165, Validation Loss: 1.3428157567977905\n",
      "Epoch 713, Training Loss: 1.3549244403839111, Validation Loss: 1.3415848016738892\n",
      "Epoch 714, Training Loss: 1.354936122894287, Validation Loss: 1.3427584171295166\n",
      "Epoch 715, Training Loss: 1.3548985719680786, Validation Loss: 1.3416544198989868\n",
      "Epoch 716, Training Loss: 1.3548555374145508, Validation Loss: 1.3424208164215088\n",
      "Epoch 717, Training Loss: 1.3548035621643066, Validation Loss: 1.3418264389038086\n",
      "Epoch 718, Training Loss: 1.3547741174697876, Validation Loss: 1.3421345949172974\n",
      "Epoch 719, Training Loss: 1.354757308959961, Validation Loss: 1.3420072793960571\n",
      "Epoch 720, Training Loss: 1.3547496795654297, Validation Loss: 1.341978669166565\n",
      "Epoch 721, Training Loss: 1.3547478914260864, Validation Loss: 1.3422086238861084\n",
      "Epoch 722, Training Loss: 1.3547511100769043, Validation Loss: 1.3418561220169067\n",
      "Epoch 723, Training Loss: 1.3547619581222534, Validation Loss: 1.3424280881881714\n",
      "Epoch 724, Training Loss: 1.3547747135162354, Validation Loss: 1.3417502641677856\n",
      "Epoch 725, Training Loss: 1.3547947406768799, Validation Loss: 1.3426238298416138\n",
      "Epoch 726, Training Loss: 1.3548120260238647, Validation Loss: 1.3416460752487183\n",
      "Epoch 727, Training Loss: 1.3548455238342285, Validation Loss: 1.3427993059158325\n",
      "Epoch 728, Training Loss: 1.3548593521118164, Validation Loss: 1.3415615558624268\n",
      "Epoch 729, Training Loss: 1.3549082279205322, Validation Loss: 1.3428584337234497\n",
      "Epoch 730, Training Loss: 1.3548766374588013, Validation Loss: 1.3415820598602295\n",
      "Epoch 731, Training Loss: 1.35487699508667, Validation Loss: 1.3426717519760132\n",
      "Epoch 732, Training Loss: 1.3548110723495483, Validation Loss: 1.3417302370071411\n",
      "Epoch 733, Training Loss: 1.3547775745391846, Validation Loss: 1.3424596786499023\n",
      "Epoch 734, Training Loss: 1.354743242263794, Validation Loss: 1.341898798942566\n",
      "Epoch 735, Training Loss: 1.3547227382659912, Validation Loss: 1.3423080444335938\n",
      "Epoch 736, Training Loss: 1.3547066450119019, Validation Loss: 1.3420473337173462\n",
      "Epoch 737, Training Loss: 1.3546953201293945, Validation Loss: 1.3421719074249268\n",
      "Epoch 738, Training Loss: 1.3546870946884155, Validation Loss: 1.342151165008545\n",
      "Epoch 739, Training Loss: 1.3546818494796753, Validation Loss: 1.3420554399490356\n",
      "Epoch 740, Training Loss: 1.3546791076660156, Validation Loss: 1.3422269821166992\n",
      "Epoch 741, Training Loss: 1.3546775579452515, Validation Loss: 1.3419803380966187\n",
      "Epoch 742, Training Loss: 1.3546773195266724, Validation Loss: 1.3423265218734741\n",
      "Epoch 743, Training Loss: 1.354678750038147, Validation Loss: 1.341901421546936\n",
      "Epoch 744, Training Loss: 1.3546867370605469, Validation Loss: 1.342529058456421\n",
      "Epoch 745, Training Loss: 1.354702115058899, Validation Loss: 1.3417686223983765\n",
      "Epoch 746, Training Loss: 1.354744791984558, Validation Loss: 1.3429440259933472\n",
      "Epoch 747, Training Loss: 1.354805827140808, Validation Loss: 1.341573715209961\n",
      "Epoch 748, Training Loss: 1.3549721240997314, Validation Loss: 1.3436223268508911\n",
      "Epoch 749, Training Loss: 1.3550946712493896, Validation Loss: 1.3414714336395264\n",
      "Epoch 750, Training Loss: 1.3554149866104126, Validation Loss: 1.3439477682113647\n",
      "Epoch 751, Training Loss: 1.3552803993225098, Validation Loss: 1.3414102792739868\n",
      "Epoch 752, Training Loss: 1.3552751541137695, Validation Loss: 1.343066930770874\n",
      "Epoch 753, Training Loss: 1.3548829555511475, Validation Loss: 1.341770052909851\n",
      "Epoch 754, Training Loss: 1.3546801805496216, Validation Loss: 1.34197199344635\n",
      "Epoch 755, Training Loss: 1.3546265363693237, Validation Loss: 1.342832326889038\n",
      "Epoch 756, Training Loss: 1.3547444343566895, Validation Loss: 1.3415707349777222\n",
      "Epoch 757, Training Loss: 1.3550127744674683, Validation Loss: 1.343701958656311\n",
      "Epoch 758, Training Loss: 1.3551287651062012, Validation Loss: 1.341547966003418\n",
      "Epoch 759, Training Loss: 1.355198621749878, Validation Loss: 1.3433412313461304\n",
      "Epoch 760, Training Loss: 1.3549433946609497, Validation Loss: 1.3415987491607666\n",
      "Epoch 761, Training Loss: 1.3547694683074951, Validation Loss: 1.342353343963623\n",
      "Epoch 762, Training Loss: 1.3546308279037476, Validation Loss: 1.3421030044555664\n",
      "Epoch 763, Training Loss: 1.3546016216278076, Validation Loss: 1.3416770696640015\n",
      "Epoch 764, Training Loss: 1.3546584844589233, Validation Loss: 1.3427510261535645\n",
      "Epoch 765, Training Loss: 1.3547310829162598, Validation Loss: 1.341525673866272\n",
      "Epoch 766, Training Loss: 1.3548107147216797, Validation Loss: 1.3428491353988647\n",
      "Epoch 767, Training Loss: 1.354737401008606, Validation Loss: 1.3416873216629028\n",
      "Epoch 768, Training Loss: 1.354684829711914, Validation Loss: 1.3424537181854248\n",
      "Epoch 769, Training Loss: 1.3546149730682373, Validation Loss: 1.3420062065124512\n",
      "Epoch 770, Training Loss: 1.354575276374817, Validation Loss: 1.342008113861084\n",
      "Epoch 771, Training Loss: 1.354570984840393, Validation Loss: 1.3423705101013184\n",
      "Epoch 772, Training Loss: 1.3545902967453003, Validation Loss: 1.3417567014694214\n",
      "Epoch 773, Training Loss: 1.3546264171600342, Validation Loss: 1.342648983001709\n",
      "Epoch 774, Training Loss: 1.35464608669281, Validation Loss: 1.3417013883590698\n",
      "Epoch 775, Training Loss: 1.3546624183654785, Validation Loss: 1.342681646347046\n",
      "Epoch 776, Training Loss: 1.3546379804611206, Validation Loss: 1.3418126106262207\n",
      "Epoch 777, Training Loss: 1.354614019393921, Validation Loss: 1.3425043821334839\n",
      "Epoch 778, Training Loss: 1.354578971862793, Validation Loss: 1.3420066833496094\n",
      "Epoch 779, Training Loss: 1.3545548915863037, Validation Loss: 1.34226655960083\n",
      "Epoch 780, Training Loss: 1.3545387983322144, Validation Loss: 1.3422200679779053\n",
      "Epoch 781, Training Loss: 1.354533314704895, Validation Loss: 1.342064380645752\n",
      "Epoch 782, Training Loss: 1.3545353412628174, Validation Loss: 1.3423911333084106\n",
      "Epoch 783, Training Loss: 1.3545418977737427, Validation Loss: 1.3419485092163086\n",
      "Epoch 784, Training Loss: 1.354549765586853, Validation Loss: 1.3425337076187134\n",
      "Epoch 785, Training Loss: 1.3545585870742798, Validation Loss: 1.3418848514556885\n",
      "Epoch 786, Training Loss: 1.3545700311660767, Validation Loss: 1.3426599502563477\n",
      "Epoch 787, Training Loss: 1.3545749187469482, Validation Loss: 1.3418545722961426\n",
      "Epoch 788, Training Loss: 1.3545876741409302, Validation Loss: 1.3427581787109375\n",
      "Epoch 789, Training Loss: 1.3545887470245361, Validation Loss: 1.3418201208114624\n",
      "Epoch 790, Training Loss: 1.3546074628829956, Validation Loss: 1.342862606048584\n",
      "Epoch 791, Training Loss: 1.3546125888824463, Validation Loss: 1.3417423963546753\n",
      "Epoch 792, Training Loss: 1.3546621799468994, Validation Loss: 1.3431028127670288\n",
      "Epoch 793, Training Loss: 1.354689359664917, Validation Loss: 1.3416621685028076\n",
      "Epoch 794, Training Loss: 1.3547672033309937, Validation Loss: 1.3432546854019165\n",
      "Epoch 795, Training Loss: 1.3547426462173462, Validation Loss: 1.3416509628295898\n",
      "Epoch 796, Training Loss: 1.3547710180282593, Validation Loss: 1.3431060314178467\n",
      "Epoch 797, Training Loss: 1.3546820878982544, Validation Loss: 1.3417302370071411\n",
      "Epoch 798, Training Loss: 1.354649305343628, Validation Loss: 1.3427361249923706\n",
      "Epoch 799, Training Loss: 1.354552149772644, Validation Loss: 1.3419405221939087\n",
      "Epoch 800, Training Loss: 1.354514479637146, Validation Loss: 1.3424592018127441\n",
      "Epoch 801, Training Loss: 1.3544800281524658, Validation Loss: 1.3421956300735474\n",
      "Epoch 802, Training Loss: 1.354462742805481, Validation Loss: 1.3422077894210815\n",
      "Epoch 803, Training Loss: 1.354459285736084, Validation Loss: 1.3424639701843262\n",
      "Epoch 804, Training Loss: 1.3544670343399048, Validation Loss: 1.3420203924179077\n",
      "Epoch 805, Training Loss: 1.3544800281524658, Validation Loss: 1.3426578044891357\n",
      "Epoch 806, Training Loss: 1.354498267173767, Validation Loss: 1.34183669090271\n",
      "Epoch 807, Training Loss: 1.354547142982483, Validation Loss: 1.342984676361084\n",
      "Epoch 808, Training Loss: 1.3545914888381958, Validation Loss: 1.3417012691497803\n",
      "Epoch 809, Training Loss: 1.3546879291534424, Validation Loss: 1.3433269262313843\n",
      "Epoch 810, Training Loss: 1.3547098636627197, Validation Loss: 1.34166419506073\n",
      "Epoch 811, Training Loss: 1.3547942638397217, Validation Loss: 1.343438744544983\n",
      "Epoch 812, Training Loss: 1.354742407798767, Validation Loss: 1.3416979312896729\n",
      "Epoch 813, Training Loss: 1.3547554016113281, Validation Loss: 1.3432097434997559\n",
      "Epoch 814, Training Loss: 1.3546334505081177, Validation Loss: 1.3418402671813965\n",
      "Epoch 815, Training Loss: 1.3545714616775513, Validation Loss: 1.3427730798721313\n",
      "Epoch 816, Training Loss: 1.3544762134552002, Validation Loss: 1.3421224355697632\n",
      "Epoch 817, Training Loss: 1.3544279336929321, Validation Loss: 1.3424345254898071\n",
      "Epoch 818, Training Loss: 1.3544042110443115, Validation Loss: 1.3424265384674072\n",
      "Epoch 819, Training Loss: 1.35439932346344, Validation Loss: 1.3422441482543945\n",
      "Epoch 820, Training Loss: 1.3544046878814697, Validation Loss: 1.342642903327942\n",
      "Epoch 821, Training Loss: 1.354413390159607, Validation Loss: 1.3420854806900024\n",
      "Epoch 822, Training Loss: 1.3544349670410156, Validation Loss: 1.3429317474365234\n",
      "Epoch 823, Training Loss: 1.3544785976409912, Validation Loss: 1.3418723344802856\n",
      "Epoch 824, Training Loss: 1.354563593864441, Validation Loss: 1.343410849571228\n",
      "Epoch 825, Training Loss: 1.3546463251113892, Validation Loss: 1.3417468070983887\n",
      "Epoch 826, Training Loss: 1.35479736328125, Validation Loss: 1.343754529953003\n",
      "Epoch 827, Training Loss: 1.354797124862671, Validation Loss: 1.3417136669158936\n",
      "Epoch 828, Training Loss: 1.3548702001571655, Validation Loss: 1.343470573425293\n",
      "Epoch 829, Training Loss: 1.3546773195266724, Validation Loss: 1.3418242931365967\n",
      "Epoch 830, Training Loss: 1.3545461893081665, Validation Loss: 1.3426811695098877\n",
      "Epoch 831, Training Loss: 1.3543987274169922, Validation Loss: 1.3423794507980347\n",
      "Epoch 832, Training Loss: 1.3543500900268555, Validation Loss: 1.3421155214309692\n",
      "Epoch 833, Training Loss: 1.354390263557434, Validation Loss: 1.3431532382965088\n",
      "Epoch 834, Training Loss: 1.3544833660125732, Validation Loss: 1.341921329498291\n",
      "Epoch 835, Training Loss: 1.3546172380447388, Validation Loss: 1.3436496257781982\n",
      "Epoch 836, Training Loss: 1.3546619415283203, Validation Loss: 1.3418620824813843\n",
      "Epoch 837, Training Loss: 1.3546847105026245, Validation Loss: 1.343267798423767\n",
      "Epoch 838, Training Loss: 1.3545255661010742, Validation Loss: 1.3419876098632812\n",
      "Epoch 839, Training Loss: 1.3544161319732666, Validation Loss: 1.3425909280776978\n",
      "Epoch 840, Training Loss: 1.3543429374694824, Validation Loss: 1.3424277305603027\n",
      "Epoch 841, Training Loss: 1.354318618774414, Validation Loss: 1.3421616554260254\n",
      "Epoch 842, Training Loss: 1.3543429374694824, Validation Loss: 1.3429787158966064\n",
      "Epoch 843, Training Loss: 1.3543860912322998, Validation Loss: 1.3420782089233398\n",
      "Epoch 844, Training Loss: 1.3544340133666992, Validation Loss: 1.3433505296707153\n",
      "Epoch 845, Training Loss: 1.3544784784317017, Validation Loss: 1.341998815536499\n",
      "Epoch 846, Training Loss: 1.3545315265655518, Validation Loss: 1.343475103378296\n",
      "Epoch 847, Training Loss: 1.3545293807983398, Validation Loss: 1.3419232368469238\n",
      "Epoch 848, Training Loss: 1.3545451164245605, Validation Loss: 1.3432095050811768\n",
      "Epoch 849, Training Loss: 1.3544483184814453, Validation Loss: 1.3420435190200806\n",
      "Epoch 850, Training Loss: 1.3543636798858643, Validation Loss: 1.3425376415252686\n",
      "Epoch 851, Training Loss: 1.3542743921279907, Validation Loss: 1.3427422046661377\n",
      "Epoch 852, Training Loss: 1.3542873859405518, Validation Loss: 1.3420943021774292\n",
      "Epoch 853, Training Loss: 1.3543797731399536, Validation Loss: 1.3434560298919678\n",
      "Epoch 854, Training Loss: 1.3544564247131348, Validation Loss: 1.342040777206421\n",
      "Epoch 855, Training Loss: 1.354583740234375, Validation Loss: 1.3438884019851685\n",
      "Epoch 856, Training Loss: 1.354615569114685, Validation Loss: 1.3419984579086304\n",
      "Epoch 857, Training Loss: 1.3546655178070068, Validation Loss: 1.3436193466186523\n",
      "Epoch 858, Training Loss: 1.3545169830322266, Validation Loss: 1.3420295715332031\n",
      "Epoch 859, Training Loss: 1.3544050455093384, Validation Loss: 1.3428010940551758\n",
      "Epoch 860, Training Loss: 1.3542571067810059, Validation Loss: 1.3425567150115967\n",
      "Epoch 861, Training Loss: 1.3542159795761108, Validation Loss: 1.3422434329986572\n",
      "Epoch 862, Training Loss: 1.354256272315979, Validation Loss: 1.3432109355926514\n",
      "Epoch 863, Training Loss: 1.354310154914856, Validation Loss: 1.342179536819458\n",
      "Epoch 864, Training Loss: 1.354379653930664, Validation Loss: 1.3435750007629395\n",
      "Epoch 865, Training Loss: 1.354398250579834, Validation Loss: 1.3421990871429443\n",
      "Epoch 866, Training Loss: 1.3544114828109741, Validation Loss: 1.3434990644454956\n",
      "Epoch 867, Training Loss: 1.354358434677124, Validation Loss: 1.3421924114227295\n",
      "Epoch 868, Training Loss: 1.354322075843811, Validation Loss: 1.3432146310806274\n",
      "Epoch 869, Training Loss: 1.3542728424072266, Validation Loss: 1.342231273651123\n",
      "Epoch 870, Training Loss: 1.3542327880859375, Validation Loss: 1.3428956270217896\n",
      "Epoch 871, Training Loss: 1.3541778326034546, Validation Loss: 1.3425476551055908\n",
      "Epoch 872, Training Loss: 1.3541405200958252, Validation Loss: 1.3426101207733154\n",
      "Epoch 873, Training Loss: 1.3541306257247925, Validation Loss: 1.3429841995239258\n",
      "Epoch 874, Training Loss: 1.354143500328064, Validation Loss: 1.3424776792526245\n",
      "Epoch 875, Training Loss: 1.3541690111160278, Validation Loss: 1.3433464765548706\n",
      "Epoch 876, Training Loss: 1.354202151298523, Validation Loss: 1.342345118522644\n",
      "Epoch 877, Training Loss: 1.3542518615722656, Validation Loss: 1.3436323404312134\n",
      "Epoch 878, Training Loss: 1.354297161102295, Validation Loss: 1.3422152996063232\n",
      "Epoch 879, Training Loss: 1.3543825149536133, Validation Loss: 1.3437808752059937\n",
      "Epoch 880, Training Loss: 1.3543648719787598, Validation Loss: 1.3421951532363892\n",
      "Epoch 881, Training Loss: 1.3543522357940674, Validation Loss: 1.3434876203536987\n",
      "Epoch 882, Training Loss: 1.354229211807251, Validation Loss: 1.3423982858657837\n",
      "Epoch 883, Training Loss: 1.3541443347930908, Validation Loss: 1.3430874347686768\n",
      "Epoch 884, Training Loss: 1.3540681600570679, Validation Loss: 1.342820644378662\n",
      "Epoch 885, Training Loss: 1.3540332317352295, Validation Loss: 1.3427685499191284\n",
      "Epoch 886, Training Loss: 1.3540349006652832, Validation Loss: 1.3432649374008179\n",
      "Epoch 887, Training Loss: 1.3540525436401367, Validation Loss: 1.3426027297973633\n",
      "Epoch 888, Training Loss: 1.3540798425674438, Validation Loss: 1.3435176610946655\n",
      "Epoch 889, Training Loss: 1.3540971279144287, Validation Loss: 1.3424965143203735\n",
      "Epoch 890, Training Loss: 1.3541232347488403, Validation Loss: 1.3436216115951538\n",
      "Epoch 891, Training Loss: 1.3541147708892822, Validation Loss: 1.3425129652023315\n",
      "Epoch 892, Training Loss: 1.3541001081466675, Validation Loss: 1.343552827835083\n",
      "Epoch 893, Training Loss: 1.3540520668029785, Validation Loss: 1.3426809310913086\n",
      "Epoch 894, Training Loss: 1.3540066480636597, Validation Loss: 1.3433648347854614\n",
      "Epoch 895, Training Loss: 1.3539540767669678, Validation Loss: 1.3429205417633057\n",
      "Epoch 896, Training Loss: 1.3539170026779175, Validation Loss: 1.3431590795516968\n",
      "Epoch 897, Training Loss: 1.3538867235183716, Validation Loss: 1.3430039882659912\n",
      "Epoch 898, Training Loss: 1.3538628816604614, Validation Loss: 1.342909812927246\n",
      "Epoch 899, Training Loss: 1.3538440465927124, Validation Loss: 1.343116044998169\n",
      "Epoch 900, Training Loss: 1.3538403511047363, Validation Loss: 1.3426077365875244\n",
      "Epoch 901, Training Loss: 1.3538644313812256, Validation Loss: 1.343405842781067\n",
      "Epoch 902, Training Loss: 1.3539084196090698, Validation Loss: 1.34247624874115\n",
      "Epoch 903, Training Loss: 1.353995680809021, Validation Loss: 1.3439347743988037\n",
      "Epoch 904, Training Loss: 1.354081630706787, Validation Loss: 1.342494249343872\n",
      "Epoch 905, Training Loss: 1.3542016744613647, Validation Loss: 1.344224214553833\n",
      "Epoch 906, Training Loss: 1.3541851043701172, Validation Loss: 1.3425477743148804\n",
      "Epoch 907, Training Loss: 1.3541829586029053, Validation Loss: 1.3439332246780396\n",
      "Epoch 908, Training Loss: 1.3539937734603882, Validation Loss: 1.342761516571045\n",
      "Epoch 909, Training Loss: 1.3538477420806885, Validation Loss: 1.3433774709701538\n",
      "Epoch 910, Training Loss: 1.353732705116272, Validation Loss: 1.3433984518051147\n",
      "Epoch 911, Training Loss: 1.3536931276321411, Validation Loss: 1.3432577848434448\n",
      "Epoch 912, Training Loss: 1.353736162185669, Validation Loss: 1.3445247411727905\n",
      "Epoch 913, Training Loss: 1.3538860082626343, Validation Loss: 1.343383550643921\n",
      "Epoch 914, Training Loss: 1.354231357574463, Validation Loss: 1.3460420370101929\n",
      "Epoch 915, Training Loss: 1.3546677827835083, Validation Loss: 1.3440793752670288\n",
      "Epoch 916, Training Loss: 1.3559170961380005, Validation Loss: 1.3470309972763062\n",
      "Epoch 917, Training Loss: 1.3553581237792969, Validation Loss: 1.3434126377105713\n",
      "Epoch 918, Training Loss: 1.3550883531570435, Validation Loss: 1.3439717292785645\n",
      "Epoch 919, Training Loss: 1.3538658618927002, Validation Loss: 1.3443832397460938\n",
      "Epoch 920, Training Loss: 1.3540406227111816, Validation Loss: 1.3432512283325195\n",
      "Epoch 921, Training Loss: 1.3549021482467651, Validation Loss: 1.345098614692688\n",
      "Epoch 922, Training Loss: 1.3541964292526245, Validation Loss: 1.3434025049209595\n",
      "Epoch 923, Training Loss: 1.353602409362793, Validation Loss: 1.3434661626815796\n",
      "Epoch 924, Training Loss: 1.3536933660507202, Validation Loss: 1.3451895713806152\n",
      "Epoch 925, Training Loss: 1.3541488647460938, Validation Loss: 1.3435673713684082\n",
      "Epoch 926, Training Loss: 1.3545284271240234, Validation Loss: 1.3449592590332031\n",
      "Epoch 927, Training Loss: 1.3539429903030396, Validation Loss: 1.3434875011444092\n",
      "Epoch 928, Training Loss: 1.3535149097442627, Validation Loss: 1.3433597087860107\n",
      "Epoch 929, Training Loss: 1.3536098003387451, Validation Loss: 1.3448553085327148\n",
      "Epoch 930, Training Loss: 1.3538554906845093, Validation Loss: 1.3435332775115967\n",
      "Epoch 931, Training Loss: 1.3538446426391602, Validation Loss: 1.344205617904663\n",
      "Epoch 932, Training Loss: 1.353435754776001, Validation Loss: 1.3450592756271362\n",
      "Epoch 933, Training Loss: 1.353618860244751, Validation Loss: 1.3441245555877686\n",
      "Epoch 934, Training Loss: 1.3540409803390503, Validation Loss: 1.3457890748977661\n",
      "Epoch 935, Training Loss: 1.3537797927856445, Validation Loss: 1.3445217609405518\n",
      "Epoch 936, Training Loss: 1.353415846824646, Validation Loss: 1.3446115255355835\n",
      "Epoch 937, Training Loss: 1.3533880710601807, Validation Loss: 1.345842957496643\n",
      "Epoch 938, Training Loss: 1.3536256551742554, Validation Loss: 1.3445038795471191\n",
      "Epoch 939, Training Loss: 1.3537384271621704, Validation Loss: 1.3456350564956665\n",
      "Epoch 940, Training Loss: 1.3534890413284302, Validation Loss: 1.3447906970977783\n",
      "Epoch 941, Training Loss: 1.3532735109329224, Validation Loss: 1.3446568250656128\n",
      "Epoch 942, Training Loss: 1.3533200025558472, Validation Loss: 1.3457647562026978\n",
      "Epoch 943, Training Loss: 1.3534519672393799, Validation Loss: 1.3446658849716187\n",
      "Epoch 944, Training Loss: 1.3534839153289795, Validation Loss: 1.3456628322601318\n",
      "Epoch 945, Training Loss: 1.3533273935317993, Validation Loss: 1.3449764251708984\n",
      "Epoch 946, Training Loss: 1.3531904220581055, Validation Loss: 1.3450167179107666\n",
      "Epoch 947, Training Loss: 1.3531649112701416, Validation Loss: 1.3455767631530762\n",
      "Epoch 948, Training Loss: 1.3532150983810425, Validation Loss: 1.344918966293335\n",
      "Epoch 949, Training Loss: 1.3532291650772095, Validation Loss: 1.3457146883010864\n",
      "Epoch 950, Training Loss: 1.3531814813613892, Validation Loss: 1.3451611995697021\n",
      "Epoch 951, Training Loss: 1.353114128112793, Validation Loss: 1.3455686569213867\n",
      "Epoch 952, Training Loss: 1.3530548810958862, Validation Loss: 1.3456834554672241\n",
      "Epoch 953, Training Loss: 1.353034257888794, Validation Loss: 1.3455207347869873\n",
      "Epoch 954, Training Loss: 1.3530417680740356, Validation Loss: 1.3461495637893677\n",
      "Epoch 955, Training Loss: 1.3530515432357788, Validation Loss: 1.3456103801727295\n",
      "Epoch 956, Training Loss: 1.3530579805374146, Validation Loss: 1.3464075326919556\n",
      "Epoch 957, Training Loss: 1.3530677556991577, Validation Loss: 1.3456494808197021\n",
      "Epoch 958, Training Loss: 1.3530818223953247, Validation Loss: 1.3465845584869385\n",
      "Epoch 959, Training Loss: 1.353087067604065, Validation Loss: 1.3457164764404297\n",
      "Epoch 960, Training Loss: 1.3530546426773071, Validation Loss: 1.3465235233306885\n",
      "Epoch 961, Training Loss: 1.3529895544052124, Validation Loss: 1.3459779024124146\n",
      "Epoch 962, Training Loss: 1.3528934717178345, Validation Loss: 1.3463740348815918\n",
      "Epoch 963, Training Loss: 1.3528422117233276, Validation Loss: 1.3464690446853638\n",
      "Epoch 964, Training Loss: 1.3528094291687012, Validation Loss: 1.3467110395431519\n",
      "Epoch 965, Training Loss: 1.3527911901474, Validation Loss: 1.3468085527420044\n",
      "Epoch 966, Training Loss: 1.3527761697769165, Validation Loss: 1.3469195365905762\n",
      "Epoch 967, Training Loss: 1.3527568578720093, Validation Loss: 1.3468296527862549\n",
      "Epoch 968, Training Loss: 1.3527361154556274, Validation Loss: 1.3469111919403076\n",
      "Epoch 969, Training Loss: 1.352721929550171, Validation Loss: 1.346771001815796\n",
      "Epoch 970, Training Loss: 1.3527239561080933, Validation Loss: 1.3473612070083618\n",
      "Epoch 971, Training Loss: 1.3527593612670898, Validation Loss: 1.3468657732009888\n",
      "Epoch 972, Training Loss: 1.3528414964675903, Validation Loss: 1.348190426826477\n",
      "Epoch 973, Training Loss: 1.3529750108718872, Validation Loss: 1.3470489978790283\n",
      "Epoch 974, Training Loss: 1.3531538248062134, Validation Loss: 1.348777174949646\n",
      "Epoch 975, Training Loss: 1.353219747543335, Validation Loss: 1.347151517868042\n",
      "Epoch 976, Training Loss: 1.3532315492630005, Validation Loss: 1.3483171463012695\n",
      "Epoch 977, Training Loss: 1.3529510498046875, Validation Loss: 1.3471418619155884\n",
      "Epoch 978, Training Loss: 1.352710247039795, Validation Loss: 1.3474857807159424\n",
      "Epoch 979, Training Loss: 1.3525855541229248, Validation Loss: 1.3478690385818481\n",
      "Epoch 980, Training Loss: 1.3526002168655396, Validation Loss: 1.3475984334945679\n",
      "Epoch 981, Training Loss: 1.3527151346206665, Validation Loss: 1.349147081375122\n",
      "Epoch 982, Training Loss: 1.352942705154419, Validation Loss: 1.3479524850845337\n",
      "Epoch 983, Training Loss: 1.3531213998794556, Validation Loss: 1.3496387004852295\n",
      "Epoch 984, Training Loss: 1.3531500101089478, Validation Loss: 1.3478708267211914\n",
      "Epoch 985, Training Loss: 1.3530373573303223, Validation Loss: 1.348636269569397\n",
      "Epoch 986, Training Loss: 1.3526848554611206, Validation Loss: 1.3479013442993164\n",
      "Epoch 987, Training Loss: 1.3524831533432007, Validation Loss: 1.3478907346725464\n",
      "Epoch 988, Training Loss: 1.3525538444519043, Validation Loss: 1.3490546941757202\n",
      "Epoch 989, Training Loss: 1.352683186531067, Validation Loss: 1.348349690437317\n",
      "Epoch 990, Training Loss: 1.3526948690414429, Validation Loss: 1.349560022354126\n",
      "Epoch 991, Training Loss: 1.3526215553283691, Validation Loss: 1.3487979173660278\n",
      "Epoch 992, Training Loss: 1.3525035381317139, Validation Loss: 1.3496248722076416\n",
      "Epoch 993, Training Loss: 1.3524549007415771, Validation Loss: 1.3491111993789673\n",
      "Epoch 994, Training Loss: 1.3524227142333984, Validation Loss: 1.3498663902282715\n",
      "Epoch 995, Training Loss: 1.3524459600448608, Validation Loss: 1.3491706848144531\n",
      "Epoch 996, Training Loss: 1.3524854183197021, Validation Loss: 1.3500596284866333\n",
      "Epoch 997, Training Loss: 1.3525148630142212, Validation Loss: 1.3491220474243164\n",
      "Epoch 998, Training Loss: 1.3525294065475464, Validation Loss: 1.3500701189041138\n",
      "Epoch 999, Training Loss: 1.3525707721710205, Validation Loss: 1.3490911722183228\n",
      "Epoch 1000, Training Loss: 1.3524729013442993, Validation Loss: 1.3496496677398682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(pdlmodel.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pdlmodel.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = pdlmodel(user_features, product_features, all_x_other_products,prices)\n",
    "    choice_probabilities = F.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]),decision_train1+1])\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    pdlmodel.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = pdlmodel(X_user_val,  product_features, all_x_other_products,prices)\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        # torch.save(pdlmodel.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b5374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c915a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3678.00\n",
      "Expected Revenue all treated: $726.88\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17cb1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = (expected_revenue_all_treated-expected_revenue_all_control)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a4df632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of PDL:  -5.10%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(pdl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c900cf",
   "metadata": {
    "id": "63c900cf"
   },
   "source": [
    "# use dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecd9bca0-6191-4297-8452-7f8af22d5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityEstimator(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(UtilityEstimator, self).__init__()\n",
    "        \n",
    "        # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "        self.theta0 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        # Output layer for Theta1 (takes xi, zj, z-j, p-j)\n",
    "        self.theta1 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_user, x_product, x_other_products,price):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "    \n",
    "\n",
    "        # Combine features for Theta0\n",
    "        \n",
    "        combined_features_theta =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                               x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                               aggregated_other_features.unsqueeze(0).expand(N, -1, -1)),\n",
    "                                                 dim=2)\n",
    "        theta0_output = self.theta0(combined_features_theta).squeeze(-1)\n",
    "        theta1_output = self.theta1(combined_features_theta).squeeze(-1)\n",
    "        \n",
    "        price = price.unsqueeze(-1)  \n",
    "        utility = theta0_output + theta1_output * price.squeeze(-1)\n",
    "\n",
    "        # Include the outside option (utility = 0)\n",
    "        zero_utilities = torch.zeros(x_user.shape[0], 1, device=utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utility), dim=1)\n",
    "        \n",
    "        return utilities_with_outside,theta0_output,theta1_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf77eeb6",
   "metadata": {
    "id": "cf77eeb6"
   },
   "outputs": [],
   "source": [
    "dml_model = UtilityEstimator(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "QT_wrrh3rIws",
   "metadata": {
    "id": "QT_wrrh3rIws"
   },
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31b522ed-0c36-4199-a309-74f71aece365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0ff32e5-5c64-49ad-bbf0-f6aa2a53d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b386e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b386e12",
    "outputId": "aab71afc-5217-4c73-eba6-26cb31bed335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.3717823028564453, Validation Loss: 2.358442544937134\n",
      "Epoch 2, Training Loss: 2.3588833808898926, Validation Loss: 2.3478481769561768\n",
      "Epoch 3, Training Loss: 2.348390579223633, Validation Loss: 2.337275981903076\n",
      "Epoch 4, Training Loss: 2.337783098220825, Validation Loss: 2.3258087635040283\n",
      "Epoch 5, Training Loss: 2.3259663581848145, Validation Loss: 2.3119027614593506\n",
      "Epoch 6, Training Loss: 2.3116936683654785, Validation Loss: 2.295121908187866\n",
      "Epoch 7, Training Loss: 2.2947421073913574, Validation Loss: 2.275484800338745\n",
      "Epoch 8, Training Loss: 2.2752573490142822, Validation Loss: 2.251657247543335\n",
      "Epoch 9, Training Loss: 2.2520055770874023, Validation Loss: 2.2230002880096436\n",
      "Epoch 10, Training Loss: 2.224419116973877, Validation Loss: 2.1900620460510254\n",
      "Epoch 11, Training Loss: 2.192349910736084, Validation Loss: 2.1523053646087646\n",
      "Epoch 12, Training Loss: 2.1553354263305664, Validation Loss: 2.1095495223999023\n",
      "Epoch 13, Training Loss: 2.113286018371582, Validation Loss: 2.0620248317718506\n",
      "Epoch 14, Training Loss: 2.0665695667266846, Validation Loss: 2.0097739696502686\n",
      "Epoch 15, Training Loss: 2.0151357650756836, Validation Loss: 1.9517356157302856\n",
      "Epoch 16, Training Loss: 1.958248496055603, Validation Loss: 1.8891271352767944\n",
      "Epoch 17, Training Loss: 1.8969192504882812, Validation Loss: 1.8254377841949463\n",
      "Epoch 18, Training Loss: 1.8346035480499268, Validation Loss: 1.7613201141357422\n",
      "Epoch 19, Training Loss: 1.7721753120422363, Validation Loss: 1.6990660429000854\n",
      "Epoch 20, Training Loss: 1.7121965885162354, Validation Loss: 1.6431615352630615\n",
      "Epoch 21, Training Loss: 1.658806324005127, Validation Loss: 1.5987473726272583\n",
      "Epoch 22, Training Loss: 1.6171984672546387, Validation Loss: 1.5660632848739624\n",
      "Epoch 23, Training Loss: 1.5870176553726196, Validation Loss: 1.542426586151123\n",
      "Epoch 24, Training Loss: 1.566230058670044, Validation Loss: 1.5286285877227783\n",
      "Epoch 25, Training Loss: 1.5549505949020386, Validation Loss: 1.5265084505081177\n",
      "Epoch 26, Training Loss: 1.5547335147857666, Validation Loss: 1.5269557237625122\n",
      "Epoch 27, Training Loss: 1.5570054054260254, Validation Loss: 1.5261783599853516\n",
      "Epoch 28, Training Loss: 1.5577740669250488, Validation Loss: 1.5243728160858154\n",
      "Epoch 29, Training Loss: 1.5566879510879517, Validation Loss: 1.5229612588882446\n",
      "Epoch 30, Training Loss: 1.5553897619247437, Validation Loss: 1.5224683284759521\n",
      "Epoch 31, Training Loss: 1.5547306537628174, Validation Loss: 1.5177972316741943\n",
      "Epoch 32, Training Loss: 1.5498530864715576, Validation Loss: 1.510759711265564\n",
      "Epoch 33, Training Loss: 1.5427429676055908, Validation Loss: 1.5035449266433716\n",
      "Epoch 34, Training Loss: 1.5353577136993408, Validation Loss: 1.4975790977478027\n",
      "Epoch 35, Training Loss: 1.5297105312347412, Validation Loss: 1.4924103021621704\n",
      "Epoch 36, Training Loss: 1.5241568088531494, Validation Loss: 1.488068699836731\n",
      "Epoch 37, Training Loss: 1.518772840499878, Validation Loss: 1.483936071395874\n",
      "Epoch 38, Training Loss: 1.513271450996399, Validation Loss: 1.4804491996765137\n",
      "Epoch 39, Training Loss: 1.5083664655685425, Validation Loss: 1.4767816066741943\n",
      "Epoch 40, Training Loss: 1.5035698413848877, Validation Loss: 1.4724220037460327\n",
      "Epoch 41, Training Loss: 1.498434066772461, Validation Loss: 1.4675142765045166\n",
      "Epoch 42, Training Loss: 1.493086338043213, Validation Loss: 1.4626811742782593\n",
      "Epoch 43, Training Loss: 1.4879677295684814, Validation Loss: 1.4582358598709106\n",
      "Epoch 44, Training Loss: 1.483224868774414, Validation Loss: 1.4539767503738403\n",
      "Epoch 45, Training Loss: 1.4785678386688232, Validation Loss: 1.449861764907837\n",
      "Epoch 46, Training Loss: 1.4739162921905518, Validation Loss: 1.4457749128341675\n",
      "Epoch 47, Training Loss: 1.4691641330718994, Validation Loss: 1.4417309761047363\n",
      "Epoch 48, Training Loss: 1.4643718004226685, Validation Loss: 1.4379805326461792\n",
      "Epoch 49, Training Loss: 1.4598708152770996, Validation Loss: 1.434347152709961\n",
      "Epoch 50, Training Loss: 1.4556623697280884, Validation Loss: 1.4303990602493286\n",
      "Epoch 51, Training Loss: 1.4513437747955322, Validation Loss: 1.4261776208877563\n",
      "Epoch 52, Training Loss: 1.44691801071167, Validation Loss: 1.4218586683273315\n",
      "Epoch 53, Training Loss: 1.4424850940704346, Validation Loss: 1.4174625873565674\n",
      "Epoch 54, Training Loss: 1.4380123615264893, Validation Loss: 1.4131224155426025\n",
      "Epoch 55, Training Loss: 1.4335709810256958, Validation Loss: 1.409044861793518\n",
      "Epoch 56, Training Loss: 1.429424524307251, Validation Loss: 1.404705286026001\n",
      "Epoch 57, Training Loss: 1.4248794317245483, Validation Loss: 1.4002349376678467\n",
      "Epoch 58, Training Loss: 1.4200496673583984, Validation Loss: 1.3960644006729126\n",
      "Epoch 59, Training Loss: 1.4156018495559692, Validation Loss: 1.3917757272720337\n",
      "Epoch 60, Training Loss: 1.4112050533294678, Validation Loss: 1.3876093626022339\n",
      "Epoch 61, Training Loss: 1.4070695638656616, Validation Loss: 1.3837404251098633\n",
      "Epoch 62, Training Loss: 1.4032081365585327, Validation Loss: 1.3802820444107056\n",
      "Epoch 63, Training Loss: 1.399633526802063, Validation Loss: 1.3770700693130493\n",
      "Epoch 64, Training Loss: 1.3958516120910645, Validation Loss: 1.3741601705551147\n",
      "Epoch 65, Training Loss: 1.3922291994094849, Validation Loss: 1.3713297843933105\n",
      "Epoch 66, Training Loss: 1.3891469240188599, Validation Loss: 1.3682639598846436\n",
      "Epoch 67, Training Loss: 1.3861229419708252, Validation Loss: 1.3653463125228882\n",
      "Epoch 68, Training Loss: 1.3834290504455566, Validation Loss: 1.3626781702041626\n",
      "Epoch 69, Training Loss: 1.3810129165649414, Validation Loss: 1.3604483604431152\n",
      "Epoch 70, Training Loss: 1.3789339065551758, Validation Loss: 1.3586233854293823\n",
      "Epoch 71, Training Loss: 1.3771556615829468, Validation Loss: 1.3570923805236816\n",
      "Epoch 72, Training Loss: 1.3755700588226318, Validation Loss: 1.3555196523666382\n",
      "Epoch 73, Training Loss: 1.3742294311523438, Validation Loss: 1.3539464473724365\n",
      "Epoch 74, Training Loss: 1.3731043338775635, Validation Loss: 1.3525242805480957\n",
      "Epoch 75, Training Loss: 1.3722091913223267, Validation Loss: 1.3512309789657593\n",
      "Epoch 76, Training Loss: 1.3714253902435303, Validation Loss: 1.3500697612762451\n",
      "Epoch 77, Training Loss: 1.370704174041748, Validation Loss: 1.3490623235702515\n",
      "Epoch 78, Training Loss: 1.3700276613235474, Validation Loss: 1.3481299877166748\n",
      "Epoch 79, Training Loss: 1.3693915605545044, Validation Loss: 1.3471966981887817\n",
      "Epoch 80, Training Loss: 1.3687562942504883, Validation Loss: 1.3462576866149902\n",
      "Epoch 81, Training Loss: 1.3681148290634155, Validation Loss: 1.3453537225723267\n",
      "Epoch 82, Training Loss: 1.3674416542053223, Validation Loss: 1.344562292098999\n",
      "Epoch 83, Training Loss: 1.3667497634887695, Validation Loss: 1.343887448310852\n",
      "Epoch 84, Training Loss: 1.366026520729065, Validation Loss: 1.3433109521865845\n",
      "Epoch 85, Training Loss: 1.365283489227295, Validation Loss: 1.3427813053131104\n",
      "Epoch 86, Training Loss: 1.3645325899124146, Validation Loss: 1.342292070388794\n",
      "Epoch 87, Training Loss: 1.3637751340866089, Validation Loss: 1.3418582677841187\n",
      "Epoch 88, Training Loss: 1.3630402088165283, Validation Loss: 1.3414984941482544\n",
      "Epoch 89, Training Loss: 1.3623223304748535, Validation Loss: 1.3412234783172607\n",
      "Epoch 90, Training Loss: 1.361622929573059, Validation Loss: 1.3409239053726196\n",
      "Epoch 91, Training Loss: 1.3609559535980225, Validation Loss: 1.3406487703323364\n",
      "Epoch 92, Training Loss: 1.3603061437606812, Validation Loss: 1.3404179811477661\n",
      "Epoch 93, Training Loss: 1.3596681356430054, Validation Loss: 1.3401910066604614\n",
      "Epoch 94, Training Loss: 1.3590724468231201, Validation Loss: 1.3399258852005005\n",
      "Epoch 95, Training Loss: 1.358515739440918, Validation Loss: 1.339664101600647\n",
      "Epoch 96, Training Loss: 1.3579978942871094, Validation Loss: 1.339470624923706\n",
      "Epoch 97, Training Loss: 1.357511043548584, Validation Loss: 1.339266061782837\n",
      "Epoch 98, Training Loss: 1.3570516109466553, Validation Loss: 1.3390523195266724\n",
      "Epoch 99, Training Loss: 1.356600284576416, Validation Loss: 1.3388267755508423\n",
      "Epoch 100, Training Loss: 1.3561521768569946, Validation Loss: 1.3385705947875977\n",
      "Epoch 101, Training Loss: 1.3557343482971191, Validation Loss: 1.3383255004882812\n",
      "Epoch 102, Training Loss: 1.3553677797317505, Validation Loss: 1.3381202220916748\n",
      "Epoch 103, Training Loss: 1.3550268411636353, Validation Loss: 1.337939739227295\n",
      "Epoch 104, Training Loss: 1.3546916246414185, Validation Loss: 1.3377728462219238\n",
      "Epoch 105, Training Loss: 1.3543919324874878, Validation Loss: 1.3375909328460693\n",
      "Epoch 106, Training Loss: 1.3541213274002075, Validation Loss: 1.3373475074768066\n",
      "Epoch 107, Training Loss: 1.3538477420806885, Validation Loss: 1.3370357751846313\n",
      "Epoch 108, Training Loss: 1.353581428527832, Validation Loss: 1.3367129564285278\n",
      "Epoch 109, Training Loss: 1.353326678276062, Validation Loss: 1.3364145755767822\n",
      "Epoch 110, Training Loss: 1.3530890941619873, Validation Loss: 1.336161494255066\n",
      "Epoch 111, Training Loss: 1.3528621196746826, Validation Loss: 1.335956335067749\n",
      "Epoch 112, Training Loss: 1.352649450302124, Validation Loss: 1.3357571363449097\n",
      "Epoch 113, Training Loss: 1.352453589439392, Validation Loss: 1.3355438709259033\n",
      "Epoch 114, Training Loss: 1.3522584438323975, Validation Loss: 1.3353359699249268\n",
      "Epoch 115, Training Loss: 1.3520593643188477, Validation Loss: 1.335132122039795\n",
      "Epoch 116, Training Loss: 1.3518621921539307, Validation Loss: 1.3349084854125977\n",
      "Epoch 117, Training Loss: 1.3516653776168823, Validation Loss: 1.334680438041687\n",
      "Epoch 118, Training Loss: 1.3514735698699951, Validation Loss: 1.3344600200653076\n",
      "Epoch 119, Training Loss: 1.3512877225875854, Validation Loss: 1.3342576026916504\n",
      "Epoch 120, Training Loss: 1.3511165380477905, Validation Loss: 1.3340651988983154\n",
      "Epoch 121, Training Loss: 1.3509563207626343, Validation Loss: 1.3338669538497925\n",
      "Epoch 122, Training Loss: 1.3508052825927734, Validation Loss: 1.3336765766143799\n",
      "Epoch 123, Training Loss: 1.3506584167480469, Validation Loss: 1.333505630493164\n",
      "Epoch 124, Training Loss: 1.3505114316940308, Validation Loss: 1.333362340927124\n",
      "Epoch 125, Training Loss: 1.3503801822662354, Validation Loss: 1.3332068920135498\n",
      "Epoch 126, Training Loss: 1.350252628326416, Validation Loss: 1.3330609798431396\n",
      "Epoch 127, Training Loss: 1.3501265048980713, Validation Loss: 1.3329309225082397\n",
      "Epoch 128, Training Loss: 1.3500077724456787, Validation Loss: 1.3328229188919067\n",
      "Epoch 129, Training Loss: 1.3498953580856323, Validation Loss: 1.332727313041687\n",
      "Epoch 130, Training Loss: 1.3497825860977173, Validation Loss: 1.3326447010040283\n",
      "Epoch 131, Training Loss: 1.349672794342041, Validation Loss: 1.332574725151062\n",
      "Epoch 132, Training Loss: 1.3495734930038452, Validation Loss: 1.3325103521347046\n",
      "Epoch 133, Training Loss: 1.3494750261306763, Validation Loss: 1.3324590921401978\n",
      "Epoch 134, Training Loss: 1.349374771118164, Validation Loss: 1.3323994874954224\n",
      "Epoch 135, Training Loss: 1.3492776155471802, Validation Loss: 1.3323596715927124\n",
      "Epoch 136, Training Loss: 1.349189043045044, Validation Loss: 1.332327127456665\n",
      "Epoch 137, Training Loss: 1.349104881286621, Validation Loss: 1.3322834968566895\n",
      "Epoch 138, Training Loss: 1.3490179777145386, Validation Loss: 1.3322439193725586\n",
      "Epoch 139, Training Loss: 1.3489410877227783, Validation Loss: 1.3322033882141113\n",
      "Epoch 140, Training Loss: 1.3488645553588867, Validation Loss: 1.3321609497070312\n",
      "Epoch 141, Training Loss: 1.3487911224365234, Validation Loss: 1.332109808921814\n",
      "Epoch 142, Training Loss: 1.3487224578857422, Validation Loss: 1.332064151763916\n",
      "Epoch 143, Training Loss: 1.3486528396606445, Validation Loss: 1.3320220708847046\n",
      "Epoch 144, Training Loss: 1.3485840559005737, Validation Loss: 1.3319708108901978\n",
      "Epoch 145, Training Loss: 1.3485162258148193, Validation Loss: 1.3319214582443237\n",
      "Epoch 146, Training Loss: 1.3484512567520142, Validation Loss: 1.3318772315979004\n",
      "Epoch 147, Training Loss: 1.3483860492706299, Validation Loss: 1.3318382501602173\n",
      "Epoch 148, Training Loss: 1.3483209609985352, Validation Loss: 1.3318032026290894\n",
      "Epoch 149, Training Loss: 1.34825599193573, Validation Loss: 1.3317784070968628\n",
      "Epoch 150, Training Loss: 1.348190426826477, Validation Loss: 1.3317400217056274\n",
      "Epoch 151, Training Loss: 1.3481249809265137, Validation Loss: 1.331706166267395\n",
      "Epoch 152, Training Loss: 1.3480581045150757, Validation Loss: 1.331681251525879\n",
      "Epoch 153, Training Loss: 1.3479901552200317, Validation Loss: 1.331644892692566\n",
      "Epoch 154, Training Loss: 1.3479194641113281, Validation Loss: 1.3315820693969727\n",
      "Epoch 155, Training Loss: 1.347848653793335, Validation Loss: 1.331525206565857\n",
      "Epoch 156, Training Loss: 1.3477765321731567, Validation Loss: 1.3314507007598877\n",
      "Epoch 157, Training Loss: 1.3477052450180054, Validation Loss: 1.331343412399292\n",
      "Epoch 158, Training Loss: 1.3476414680480957, Validation Loss: 1.3312286138534546\n",
      "Epoch 159, Training Loss: 1.3475803136825562, Validation Loss: 1.3310939073562622\n",
      "Epoch 160, Training Loss: 1.3475244045257568, Validation Loss: 1.3309507369995117\n",
      "Epoch 161, Training Loss: 1.3474663496017456, Validation Loss: 1.3308498859405518\n",
      "Epoch 162, Training Loss: 1.3474102020263672, Validation Loss: 1.3307480812072754\n",
      "Epoch 163, Training Loss: 1.3473566770553589, Validation Loss: 1.3306702375411987\n",
      "Epoch 164, Training Loss: 1.347303867340088, Validation Loss: 1.3306248188018799\n",
      "Epoch 165, Training Loss: 1.3472493886947632, Validation Loss: 1.3305851221084595\n",
      "Epoch 166, Training Loss: 1.3471949100494385, Validation Loss: 1.3305479288101196\n",
      "Epoch 167, Training Loss: 1.347139596939087, Validation Loss: 1.3305308818817139\n",
      "Epoch 168, Training Loss: 1.3470898866653442, Validation Loss: 1.330564022064209\n",
      "Epoch 169, Training Loss: 1.3470371961593628, Validation Loss: 1.3306304216384888\n",
      "Epoch 170, Training Loss: 1.3469890356063843, Validation Loss: 1.3306570053100586\n",
      "Epoch 171, Training Loss: 1.3469408750534058, Validation Loss: 1.3306913375854492\n",
      "Epoch 172, Training Loss: 1.346901535987854, Validation Loss: 1.3307470083236694\n",
      "Epoch 173, Training Loss: 1.346859335899353, Validation Loss: 1.3307937383651733\n",
      "Epoch 174, Training Loss: 1.3468198776245117, Validation Loss: 1.330814242362976\n",
      "Epoch 175, Training Loss: 1.3467799425125122, Validation Loss: 1.3308261632919312\n",
      "Epoch 176, Training Loss: 1.3467421531677246, Validation Loss: 1.3308379650115967\n",
      "Epoch 177, Training Loss: 1.3467050790786743, Validation Loss: 1.330845594406128\n",
      "Epoch 178, Training Loss: 1.3466694355010986, Validation Loss: 1.3308488130569458\n",
      "Epoch 179, Training Loss: 1.3466335535049438, Validation Loss: 1.3308535814285278\n",
      "Epoch 180, Training Loss: 1.3465979099273682, Validation Loss: 1.3308614492416382\n",
      "Epoch 181, Training Loss: 1.3465625047683716, Validation Loss: 1.3308738470077515\n",
      "Epoch 182, Training Loss: 1.3465290069580078, Validation Loss: 1.330888271331787\n",
      "Epoch 183, Training Loss: 1.34649658203125, Validation Loss: 1.3309000730514526\n",
      "Epoch 184, Training Loss: 1.3464659452438354, Validation Loss: 1.330913782119751\n",
      "Epoch 185, Training Loss: 1.3464354276657104, Validation Loss: 1.330926775932312\n",
      "Epoch 186, Training Loss: 1.3464056253433228, Validation Loss: 1.3309378623962402\n",
      "Epoch 187, Training Loss: 1.3463762998580933, Validation Loss: 1.330948829650879\n",
      "Epoch 188, Training Loss: 1.3463480472564697, Validation Loss: 1.3309636116027832\n",
      "Epoch 189, Training Loss: 1.3463213443756104, Validation Loss: 1.3309752941131592\n",
      "Epoch 190, Training Loss: 1.3462951183319092, Validation Loss: 1.3309855461120605\n",
      "Epoch 191, Training Loss: 1.3462685346603394, Validation Loss: 1.3309822082519531\n",
      "Epoch 192, Training Loss: 1.3462417125701904, Validation Loss: 1.3309626579284668\n",
      "Epoch 193, Training Loss: 1.3462138175964355, Validation Loss: 1.3309297561645508\n",
      "Epoch 194, Training Loss: 1.346187710762024, Validation Loss: 1.3309067487716675\n",
      "Epoch 195, Training Loss: 1.3461607694625854, Validation Loss: 1.3308881521224976\n",
      "Epoch 196, Training Loss: 1.3461343050003052, Validation Loss: 1.3308680057525635\n",
      "Epoch 197, Training Loss: 1.346107006072998, Validation Loss: 1.3308593034744263\n",
      "Epoch 198, Training Loss: 1.3460826873779297, Validation Loss: 1.3308614492416382\n",
      "Epoch 199, Training Loss: 1.3460569381713867, Validation Loss: 1.3308539390563965\n",
      "Epoch 200, Training Loss: 1.3460332155227661, Validation Loss: 1.3308271169662476\n",
      "Epoch 201, Training Loss: 1.34600830078125, Validation Loss: 1.3307899236679077\n",
      "Epoch 202, Training Loss: 1.3459832668304443, Validation Loss: 1.3307454586029053\n",
      "Epoch 203, Training Loss: 1.3459601402282715, Validation Loss: 1.330710768699646\n",
      "Epoch 204, Training Loss: 1.3459347486495972, Validation Loss: 1.330690622329712\n",
      "Epoch 205, Training Loss: 1.3459135293960571, Validation Loss: 1.3306611776351929\n",
      "Epoch 206, Training Loss: 1.3458900451660156, Validation Loss: 1.3306457996368408\n",
      "Epoch 207, Training Loss: 1.3458679914474487, Validation Loss: 1.3306506872177124\n",
      "Epoch 208, Training Loss: 1.3458448648452759, Validation Loss: 1.3306634426116943\n",
      "Epoch 209, Training Loss: 1.345822811126709, Validation Loss: 1.3306666612625122\n",
      "Epoch 210, Training Loss: 1.345801591873169, Validation Loss: 1.3306574821472168\n",
      "Epoch 211, Training Loss: 1.3457797765731812, Validation Loss: 1.330661654472351\n",
      "Epoch 212, Training Loss: 1.3457586765289307, Validation Loss: 1.3306834697723389\n",
      "Epoch 213, Training Loss: 1.3457366228103638, Validation Loss: 1.330707311630249\n",
      "Epoch 214, Training Loss: 1.345715045928955, Validation Loss: 1.3307229280471802\n",
      "Epoch 215, Training Loss: 1.3456933498382568, Validation Loss: 1.3307271003723145\n",
      "Epoch 216, Training Loss: 1.3456714153289795, Validation Loss: 1.3307210206985474\n",
      "Epoch 217, Training Loss: 1.3456501960754395, Validation Loss: 1.3307228088378906\n",
      "Epoch 218, Training Loss: 1.345629096031189, Validation Loss: 1.3307212591171265\n",
      "Epoch 219, Training Loss: 1.3456087112426758, Validation Loss: 1.3307163715362549\n",
      "Epoch 220, Training Loss: 1.3455874919891357, Validation Loss: 1.3307069540023804\n",
      "Epoch 221, Training Loss: 1.3455666303634644, Validation Loss: 1.330691933631897\n",
      "Epoch 222, Training Loss: 1.3455464839935303, Validation Loss: 1.3306790590286255\n",
      "Epoch 223, Training Loss: 1.3455251455307007, Validation Loss: 1.3306829929351807\n",
      "Epoch 224, Training Loss: 1.345503568649292, Validation Loss: 1.3307002782821655\n",
      "Epoch 225, Training Loss: 1.3454817533493042, Validation Loss: 1.3307210206985474\n",
      "Epoch 226, Training Loss: 1.3454595804214478, Validation Loss: 1.3307231664657593\n",
      "Epoch 227, Training Loss: 1.345436930656433, Validation Loss: 1.3306783437728882\n",
      "Epoch 228, Training Loss: 1.3454163074493408, Validation Loss: 1.3306915760040283\n",
      "Epoch 229, Training Loss: 1.345392107963562, Validation Loss: 1.330701231956482\n",
      "Epoch 230, Training Loss: 1.3453726768493652, Validation Loss: 1.3306289911270142\n",
      "Epoch 231, Training Loss: 1.3453460931777954, Validation Loss: 1.3306245803833008\n",
      "Epoch 232, Training Loss: 1.345322608947754, Validation Loss: 1.3306382894515991\n",
      "Epoch 233, Training Loss: 1.3453013896942139, Validation Loss: 1.3305882215499878\n",
      "Epoch 234, Training Loss: 1.3452805280685425, Validation Loss: 1.3306026458740234\n",
      "Epoch 235, Training Loss: 1.3452564477920532, Validation Loss: 1.3305912017822266\n",
      "Epoch 236, Training Loss: 1.3452341556549072, Validation Loss: 1.3305845260620117\n",
      "Epoch 237, Training Loss: 1.3452116250991821, Validation Loss: 1.3305294513702393\n",
      "Epoch 238, Training Loss: 1.3451889753341675, Validation Loss: 1.3304994106292725\n",
      "Epoch 239, Training Loss: 1.3451660871505737, Validation Loss: 1.3304558992385864\n",
      "Epoch 240, Training Loss: 1.3451441526412964, Validation Loss: 1.3304026126861572\n",
      "Epoch 241, Training Loss: 1.3451225757598877, Validation Loss: 1.3303983211517334\n",
      "Epoch 242, Training Loss: 1.3451007604599, Validation Loss: 1.3303433656692505\n",
      "Epoch 243, Training Loss: 1.3450781106948853, Validation Loss: 1.3303200006484985\n",
      "Epoch 244, Training Loss: 1.3450552225112915, Validation Loss: 1.3303531408309937\n",
      "Epoch 245, Training Loss: 1.3450331687927246, Validation Loss: 1.3302985429763794\n",
      "Epoch 246, Training Loss: 1.3450088500976562, Validation Loss: 1.3302667140960693\n",
      "Epoch 247, Training Loss: 1.3449889421463013, Validation Loss: 1.3303316831588745\n",
      "Epoch 248, Training Loss: 1.3449727296829224, Validation Loss: 1.3302505016326904\n",
      "Epoch 249, Training Loss: 1.344947338104248, Validation Loss: 1.3302541971206665\n",
      "Epoch 250, Training Loss: 1.3449267148971558, Validation Loss: 1.3303042650222778\n",
      "Epoch 251, Training Loss: 1.3449113368988037, Validation Loss: 1.330281376838684\n",
      "Epoch 252, Training Loss: 1.3448901176452637, Validation Loss: 1.330247402191162\n",
      "Epoch 253, Training Loss: 1.3448765277862549, Validation Loss: 1.330331802368164\n",
      "Epoch 254, Training Loss: 1.3448563814163208, Validation Loss: 1.330314040184021\n",
      "Epoch 255, Training Loss: 1.3448368310928345, Validation Loss: 1.3303245306015015\n",
      "Epoch 256, Training Loss: 1.3448209762573242, Validation Loss: 1.33037269115448\n",
      "Epoch 257, Training Loss: 1.34480619430542, Validation Loss: 1.3303571939468384\n",
      "Epoch 258, Training Loss: 1.3447895050048828, Validation Loss: 1.3303793668746948\n",
      "Epoch 259, Training Loss: 1.3447743654251099, Validation Loss: 1.3303755521774292\n",
      "Epoch 260, Training Loss: 1.3447599411010742, Validation Loss: 1.3304102420806885\n",
      "Epoch 261, Training Loss: 1.3447463512420654, Validation Loss: 1.3303909301757812\n",
      "Epoch 262, Training Loss: 1.3447341918945312, Validation Loss: 1.3304561376571655\n",
      "Epoch 263, Training Loss: 1.3447201251983643, Validation Loss: 1.3304462432861328\n",
      "Epoch 264, Training Loss: 1.344706654548645, Validation Loss: 1.3304656744003296\n",
      "Epoch 265, Training Loss: 1.344693899154663, Validation Loss: 1.3305306434631348\n",
      "Epoch 266, Training Loss: 1.3446826934814453, Validation Loss: 1.3305054903030396\n",
      "Epoch 267, Training Loss: 1.344668984413147, Validation Loss: 1.3304905891418457\n",
      "Epoch 268, Training Loss: 1.344659686088562, Validation Loss: 1.3305784463882446\n",
      "Epoch 269, Training Loss: 1.3446482419967651, Validation Loss: 1.3305631875991821\n",
      "Epoch 270, Training Loss: 1.3446348905563354, Validation Loss: 1.3305069208145142\n",
      "Epoch 271, Training Loss: 1.3446215391159058, Validation Loss: 1.3305084705352783\n",
      "Epoch 272, Training Loss: 1.3446091413497925, Validation Loss: 1.3305363655090332\n",
      "Epoch 273, Training Loss: 1.344597578048706, Validation Loss: 1.3305186033248901\n",
      "Epoch 274, Training Loss: 1.3445855379104614, Validation Loss: 1.3305646181106567\n",
      "Epoch 275, Training Loss: 1.3445764780044556, Validation Loss: 1.3304824829101562\n",
      "Epoch 276, Training Loss: 1.3445755243301392, Validation Loss: 1.3306219577789307\n",
      "Epoch 277, Training Loss: 1.3445571660995483, Validation Loss: 1.3306427001953125\n",
      "Epoch 278, Training Loss: 1.3445464372634888, Validation Loss: 1.3305188417434692\n",
      "Epoch 279, Training Loss: 1.344545841217041, Validation Loss: 1.330635905265808\n",
      "Epoch 280, Training Loss: 1.3445204496383667, Validation Loss: 1.3306968212127686\n",
      "Epoch 281, Training Loss: 1.3445125818252563, Validation Loss: 1.330601453781128\n",
      "Epoch 282, Training Loss: 1.3445038795471191, Validation Loss: 1.3307075500488281\n",
      "Epoch 283, Training Loss: 1.3444887399673462, Validation Loss: 1.3307371139526367\n",
      "Epoch 284, Training Loss: 1.344478964805603, Validation Loss: 1.330679178237915\n",
      "Epoch 285, Training Loss: 1.3444699048995972, Validation Loss: 1.3307771682739258\n",
      "Epoch 286, Training Loss: 1.3444578647613525, Validation Loss: 1.330784559249878\n",
      "Epoch 287, Training Loss: 1.3444461822509766, Validation Loss: 1.33072030544281\n",
      "Epoch 288, Training Loss: 1.3444441556930542, Validation Loss: 1.3309088945388794\n",
      "Epoch 289, Training Loss: 1.3444386720657349, Validation Loss: 1.330820918083191\n",
      "Epoch 290, Training Loss: 1.3444154262542725, Validation Loss: 1.3307368755340576\n",
      "Epoch 291, Training Loss: 1.3444312810897827, Validation Loss: 1.3310065269470215\n",
      "Epoch 292, Training Loss: 1.3444207906723022, Validation Loss: 1.330896258354187\n",
      "Epoch 293, Training Loss: 1.3443883657455444, Validation Loss: 1.3307769298553467\n",
      "Epoch 294, Training Loss: 1.344403624534607, Validation Loss: 1.3310297727584839\n",
      "Epoch 295, Training Loss: 1.3443899154663086, Validation Loss: 1.3308873176574707\n",
      "Epoch 296, Training Loss: 1.3443604707717896, Validation Loss: 1.3308212757110596\n",
      "Epoch 297, Training Loss: 1.3443599939346313, Validation Loss: 1.330959439277649\n",
      "Epoch 298, Training Loss: 1.344342827796936, Validation Loss: 1.3309683799743652\n",
      "Epoch 299, Training Loss: 1.3443334102630615, Validation Loss: 1.330847978591919\n",
      "Epoch 300, Training Loss: 1.3443307876586914, Validation Loss: 1.3309414386749268\n",
      "Epoch 301, Training Loss: 1.344310998916626, Validation Loss: 1.331018090248108\n",
      "Epoch 302, Training Loss: 1.344306468963623, Validation Loss: 1.3308978080749512\n",
      "Epoch 303, Training Loss: 1.34429931640625, Validation Loss: 1.3310171365737915\n",
      "Epoch 304, Training Loss: 1.344287395477295, Validation Loss: 1.3309588432312012\n",
      "Epoch 305, Training Loss: 1.3442736864089966, Validation Loss: 1.3308789730072021\n",
      "Epoch 306, Training Loss: 1.3442717790603638, Validation Loss: 1.3310433626174927\n",
      "Epoch 307, Training Loss: 1.3442665338516235, Validation Loss: 1.3309134244918823\n",
      "Epoch 308, Training Loss: 1.3442474603652954, Validation Loss: 1.330887794494629\n",
      "Epoch 309, Training Loss: 1.3442412614822388, Validation Loss: 1.3310540914535522\n",
      "Epoch 310, Training Loss: 1.344239592552185, Validation Loss: 1.3308610916137695\n",
      "Epoch 311, Training Loss: 1.3442342281341553, Validation Loss: 1.331061840057373\n",
      "Epoch 312, Training Loss: 1.3442184925079346, Validation Loss: 1.331002950668335\n",
      "Epoch 313, Training Loss: 1.3442033529281616, Validation Loss: 1.3309028148651123\n",
      "Epoch 314, Training Loss: 1.3442020416259766, Validation Loss: 1.331070899963379\n",
      "Epoch 315, Training Loss: 1.3441901206970215, Validation Loss: 1.3310232162475586\n",
      "Epoch 316, Training Loss: 1.3441752195358276, Validation Loss: 1.3308857679367065\n",
      "Epoch 317, Training Loss: 1.3441790342330933, Validation Loss: 1.3311463594436646\n",
      "Epoch 318, Training Loss: 1.3441721200942993, Validation Loss: 1.3309218883514404\n",
      "Epoch 319, Training Loss: 1.344157338142395, Validation Loss: 1.3310816287994385\n",
      "Epoch 320, Training Loss: 1.344138503074646, Validation Loss: 1.3311264514923096\n",
      "Epoch 321, Training Loss: 1.3441309928894043, Validation Loss: 1.3309829235076904\n",
      "Epoch 322, Training Loss: 1.3441282510757446, Validation Loss: 1.3311704397201538\n",
      "Epoch 323, Training Loss: 1.3441126346588135, Validation Loss: 1.3311775922775269\n",
      "Epoch 324, Training Loss: 1.3441026210784912, Validation Loss: 1.3310555219650269\n",
      "Epoch 325, Training Loss: 1.344091773033142, Validation Loss: 1.3310917615890503\n",
      "Epoch 326, Training Loss: 1.3440805673599243, Validation Loss: 1.3311595916748047\n",
      "Epoch 327, Training Loss: 1.3440711498260498, Validation Loss: 1.3311736583709717\n",
      "Epoch 328, Training Loss: 1.344062089920044, Validation Loss: 1.3310474157333374\n",
      "Epoch 329, Training Loss: 1.344057321548462, Validation Loss: 1.3312073945999146\n",
      "Epoch 330, Training Loss: 1.3440443277359009, Validation Loss: 1.3311086893081665\n",
      "Epoch 331, Training Loss: 1.3440330028533936, Validation Loss: 1.3311452865600586\n",
      "Epoch 332, Training Loss: 1.3440221548080444, Validation Loss: 1.331174612045288\n",
      "Epoch 333, Training Loss: 1.3440132141113281, Validation Loss: 1.3310723304748535\n",
      "Epoch 334, Training Loss: 1.3440115451812744, Validation Loss: 1.3313027620315552\n",
      "Epoch 335, Training Loss: 1.3440276384353638, Validation Loss: 1.330851674079895\n",
      "Epoch 336, Training Loss: 1.3440790176391602, Validation Loss: 1.331366777420044\n",
      "Epoch 337, Training Loss: 1.3440377712249756, Validation Loss: 1.3309614658355713\n",
      "Epoch 338, Training Loss: 1.3440158367156982, Validation Loss: 1.3313250541687012\n",
      "Epoch 339, Training Loss: 1.3439915180206299, Validation Loss: 1.3310399055480957\n",
      "Epoch 340, Training Loss: 1.3439817428588867, Validation Loss: 1.331278681755066\n",
      "Epoch 341, Training Loss: 1.34396231174469, Validation Loss: 1.3311456441879272\n",
      "Epoch 342, Training Loss: 1.3439452648162842, Validation Loss: 1.3312499523162842\n",
      "Epoch 343, Training Loss: 1.3439395427703857, Validation Loss: 1.3311423063278198\n",
      "Epoch 344, Training Loss: 1.343931794166565, Validation Loss: 1.3313311338424683\n",
      "Epoch 345, Training Loss: 1.3439346551895142, Validation Loss: 1.3310693502426147\n",
      "Epoch 346, Training Loss: 1.3439466953277588, Validation Loss: 1.3315527439117432\n",
      "Epoch 347, Training Loss: 1.3439830541610718, Validation Loss: 1.331120252609253\n",
      "Epoch 348, Training Loss: 1.3439379930496216, Validation Loss: 1.331482172012329\n",
      "Epoch 349, Training Loss: 1.3439130783081055, Validation Loss: 1.3312242031097412\n",
      "Epoch 350, Training Loss: 1.3439007997512817, Validation Loss: 1.3314844369888306\n",
      "Epoch 351, Training Loss: 1.3438897132873535, Validation Loss: 1.331277847290039\n",
      "Epoch 352, Training Loss: 1.3438777923583984, Validation Loss: 1.331437110900879\n",
      "Epoch 353, Training Loss: 1.343859076499939, Validation Loss: 1.3314261436462402\n",
      "Epoch 354, Training Loss: 1.3438493013381958, Validation Loss: 1.3314110040664673\n",
      "Epoch 355, Training Loss: 1.3438432216644287, Validation Loss: 1.3314859867095947\n",
      "Epoch 356, Training Loss: 1.3438371419906616, Validation Loss: 1.3314356803894043\n",
      "Epoch 357, Training Loss: 1.3438293933868408, Validation Loss: 1.3315706253051758\n",
      "Epoch 358, Training Loss: 1.3438297510147095, Validation Loss: 1.3313629627227783\n",
      "Epoch 359, Training Loss: 1.3438341617584229, Validation Loss: 1.331742525100708\n",
      "Epoch 360, Training Loss: 1.3438788652420044, Validation Loss: 1.3313584327697754\n",
      "Epoch 361, Training Loss: 1.3438204526901245, Validation Loss: 1.3314553499221802\n",
      "Epoch 362, Training Loss: 1.3437968492507935, Validation Loss: 1.3316766023635864\n",
      "Epoch 363, Training Loss: 1.3438283205032349, Validation Loss: 1.331369161605835\n",
      "Epoch 364, Training Loss: 1.3438152074813843, Validation Loss: 1.3317502737045288\n",
      "Epoch 365, Training Loss: 1.3438061475753784, Validation Loss: 1.3316186666488647\n",
      "Epoch 366, Training Loss: 1.343770146369934, Validation Loss: 1.3315948247909546\n",
      "Epoch 367, Training Loss: 1.3437734842300415, Validation Loss: 1.3318085670471191\n",
      "Epoch 368, Training Loss: 1.3437650203704834, Validation Loss: 1.3316551446914673\n",
      "Epoch 369, Training Loss: 1.343751311302185, Validation Loss: 1.3316744565963745\n",
      "Epoch 370, Training Loss: 1.3437403440475464, Validation Loss: 1.3317781686782837\n",
      "Epoch 371, Training Loss: 1.3437389135360718, Validation Loss: 1.331578254699707\n",
      "Epoch 372, Training Loss: 1.343739628791809, Validation Loss: 1.3317500352859497\n",
      "Epoch 373, Training Loss: 1.343722939491272, Validation Loss: 1.3316941261291504\n",
      "Epoch 374, Training Loss: 1.3437100648880005, Validation Loss: 1.3316360712051392\n",
      "Epoch 375, Training Loss: 1.3437055349349976, Validation Loss: 1.331774115562439\n",
      "Epoch 376, Training Loss: 1.3436939716339111, Validation Loss: 1.3316617012023926\n",
      "Epoch 377, Training Loss: 1.3436877727508545, Validation Loss: 1.3318064212799072\n",
      "Epoch 378, Training Loss: 1.3436779975891113, Validation Loss: 1.3316346406936646\n",
      "Epoch 379, Training Loss: 1.3436766862869263, Validation Loss: 1.3318594694137573\n",
      "Epoch 380, Training Loss: 1.3436708450317383, Validation Loss: 1.3316211700439453\n",
      "Epoch 381, Training Loss: 1.3436596393585205, Validation Loss: 1.3318265676498413\n",
      "Epoch 382, Training Loss: 1.3436462879180908, Validation Loss: 1.3315694332122803\n",
      "Epoch 383, Training Loss: 1.3436473608016968, Validation Loss: 1.3317075967788696\n",
      "Epoch 384, Training Loss: 1.3436166048049927, Validation Loss: 1.3318697214126587\n",
      "Epoch 385, Training Loss: 1.3436537981033325, Validation Loss: 1.3314080238342285\n",
      "Epoch 386, Training Loss: 1.3437563180923462, Validation Loss: 1.3318796157836914\n",
      "Epoch 387, Training Loss: 1.3436123132705688, Validation Loss: 1.3319588899612427\n",
      "Epoch 388, Training Loss: 1.3436100482940674, Validation Loss: 1.3315685987472534\n",
      "Epoch 389, Training Loss: 1.3437126874923706, Validation Loss: 1.332014560699463\n",
      "Epoch 390, Training Loss: 1.343575119972229, Validation Loss: 1.332288146018982\n",
      "Epoch 391, Training Loss: 1.343671202659607, Validation Loss: 1.3315989971160889\n",
      "Epoch 392, Training Loss: 1.3438292741775513, Validation Loss: 1.331899642944336\n",
      "Epoch 393, Training Loss: 1.3435801267623901, Validation Loss: 1.3327770233154297\n",
      "Epoch 394, Training Loss: 1.3439743518829346, Validation Loss: 1.3318930864334106\n",
      "Epoch 395, Training Loss: 1.3435578346252441, Validation Loss: 1.3316899538040161\n",
      "Epoch 396, Training Loss: 1.3436927795410156, Validation Loss: 1.332261562347412\n",
      "Epoch 397, Training Loss: 1.3436139822006226, Validation Loss: 1.3321549892425537\n",
      "Epoch 398, Training Loss: 1.3435431718826294, Validation Loss: 1.3318276405334473\n",
      "Epoch 399, Training Loss: 1.3436473608016968, Validation Loss: 1.3321269750595093\n",
      "Epoch 400, Training Loss: 1.343503713607788, Validation Loss: 1.3326297998428345\n",
      "Epoch 401, Training Loss: 1.3436291217803955, Validation Loss: 1.3320621252059937\n",
      "Epoch 402, Training Loss: 1.3435642719268799, Validation Loss: 1.3321025371551514\n",
      "Epoch 403, Training Loss: 1.3435341119766235, Validation Loss: 1.3327136039733887\n",
      "Epoch 404, Training Loss: 1.3436180353164673, Validation Loss: 1.3321709632873535\n",
      "Epoch 405, Training Loss: 1.3434720039367676, Validation Loss: 1.3320008516311646\n",
      "Epoch 406, Training Loss: 1.343563199043274, Validation Loss: 1.3324551582336426\n",
      "Epoch 407, Training Loss: 1.3434871435165405, Validation Loss: 1.3325105905532837\n",
      "Epoch 408, Training Loss: 1.343491792678833, Validation Loss: 1.3320775032043457\n",
      "Epoch 409, Training Loss: 1.3435062170028687, Validation Loss: 1.3322769403457642\n",
      "Epoch 410, Training Loss: 1.343425989151001, Validation Loss: 1.3326221704483032\n",
      "Epoch 411, Training Loss: 1.3434582948684692, Validation Loss: 1.332371473312378\n",
      "Epoch 412, Training Loss: 1.343405842781067, Validation Loss: 1.3322994709014893\n",
      "Epoch 413, Training Loss: 1.3434327840805054, Validation Loss: 1.3327347040176392\n",
      "Epoch 414, Training Loss: 1.3434420824050903, Validation Loss: 1.3325401544570923\n",
      "Epoch 415, Training Loss: 1.3433817625045776, Validation Loss: 1.3322844505310059\n",
      "Epoch 416, Training Loss: 1.3434327840805054, Validation Loss: 1.3325892686843872\n",
      "Epoch 417, Training Loss: 1.3433656692504883, Validation Loss: 1.3326820135116577\n",
      "Epoch 418, Training Loss: 1.3433640003204346, Validation Loss: 1.332460641860962\n",
      "Epoch 419, Training Loss: 1.3433676958084106, Validation Loss: 1.332664132118225\n",
      "Epoch 420, Training Loss: 1.3433196544647217, Validation Loss: 1.3328601121902466\n",
      "Epoch 421, Training Loss: 1.343346118927002, Validation Loss: 1.3325942754745483\n",
      "Epoch 422, Training Loss: 1.343313455581665, Validation Loss: 1.3326005935668945\n",
      "Epoch 423, Training Loss: 1.3432953357696533, Validation Loss: 1.332813024520874\n",
      "Epoch 424, Training Loss: 1.3433078527450562, Validation Loss: 1.332600712776184\n",
      "Epoch 425, Training Loss: 1.3432658910751343, Validation Loss: 1.3325659036636353\n",
      "Epoch 426, Training Loss: 1.3432708978652954, Validation Loss: 1.332865834236145\n",
      "Epoch 427, Training Loss: 1.3432774543762207, Validation Loss: 1.3327056169509888\n",
      "Epoch 428, Training Loss: 1.3432250022888184, Validation Loss: 1.332737684249878\n",
      "Epoch 429, Training Loss: 1.3432159423828125, Validation Loss: 1.3330270051956177\n",
      "Epoch 430, Training Loss: 1.3432471752166748, Validation Loss: 1.3327877521514893\n",
      "Epoch 431, Training Loss: 1.343198537826538, Validation Loss: 1.3328791856765747\n",
      "Epoch 432, Training Loss: 1.343174695968628, Validation Loss: 1.3331314325332642\n",
      "Epoch 433, Training Loss: 1.343190312385559, Validation Loss: 1.3330141305923462\n",
      "Epoch 434, Training Loss: 1.3431428670883179, Validation Loss: 1.3329083919525146\n",
      "Epoch 435, Training Loss: 1.343184471130371, Validation Loss: 1.3333584070205688\n",
      "Epoch 436, Training Loss: 1.3431549072265625, Validation Loss: 1.333323359489441\n",
      "Epoch 437, Training Loss: 1.3431000709533691, Validation Loss: 1.3330739736557007\n",
      "Epoch 438, Training Loss: 1.343187689781189, Validation Loss: 1.333425521850586\n",
      "Epoch 439, Training Loss: 1.3430683612823486, Validation Loss: 1.333841323852539\n",
      "Epoch 440, Training Loss: 1.3431662321090698, Validation Loss: 1.33335542678833\n",
      "Epoch 441, Training Loss: 1.3430678844451904, Validation Loss: 1.3333346843719482\n",
      "Epoch 442, Training Loss: 1.3430876731872559, Validation Loss: 1.3337912559509277\n",
      "Epoch 443, Training Loss: 1.3430299758911133, Validation Loss: 1.3339070081710815\n",
      "Epoch 444, Training Loss: 1.3430495262145996, Validation Loss: 1.3335684537887573\n",
      "Epoch 445, Training Loss: 1.3429874181747437, Validation Loss: 1.3334864377975464\n",
      "Epoch 446, Training Loss: 1.3430594205856323, Validation Loss: 1.3338749408721924\n",
      "Epoch 447, Training Loss: 1.34294593334198, Validation Loss: 1.3343086242675781\n",
      "Epoch 448, Training Loss: 1.3430653810501099, Validation Loss: 1.3338080644607544\n",
      "Epoch 449, Training Loss: 1.3429034948349, Validation Loss: 1.3337335586547852\n",
      "Epoch 450, Training Loss: 1.3429691791534424, Validation Loss: 1.3341221809387207\n",
      "Epoch 451, Training Loss: 1.342862606048584, Validation Loss: 1.3343101739883423\n",
      "Epoch 452, Training Loss: 1.342890977859497, Validation Loss: 1.3339290618896484\n",
      "Epoch 453, Training Loss: 1.3428425788879395, Validation Loss: 1.3340471982955933\n",
      "Epoch 454, Training Loss: 1.3427832126617432, Validation Loss: 1.3345078229904175\n",
      "Epoch 455, Training Loss: 1.342818021774292, Validation Loss: 1.3342618942260742\n",
      "Epoch 456, Training Loss: 1.3427016735076904, Validation Loss: 1.334197759628296\n",
      "Epoch 457, Training Loss: 1.3427475690841675, Validation Loss: 1.3345173597335815\n",
      "Epoch 458, Training Loss: 1.3426657915115356, Validation Loss: 1.334555745124817\n",
      "Epoch 459, Training Loss: 1.3426321744918823, Validation Loss: 1.334465742111206\n",
      "Epoch 460, Training Loss: 1.3426276445388794, Validation Loss: 1.3346858024597168\n",
      "Epoch 461, Training Loss: 1.3426012992858887, Validation Loss: 1.3345814943313599\n",
      "Epoch 462, Training Loss: 1.3425649404525757, Validation Loss: 1.3345000743865967\n",
      "Epoch 463, Training Loss: 1.342568039894104, Validation Loss: 1.3346421718597412\n",
      "Epoch 464, Training Loss: 1.3425384759902954, Validation Loss: 1.3346550464630127\n",
      "Epoch 465, Training Loss: 1.3425264358520508, Validation Loss: 1.3344976902008057\n",
      "Epoch 466, Training Loss: 1.3425372838974, Validation Loss: 1.334668755531311\n",
      "Epoch 467, Training Loss: 1.3424893617630005, Validation Loss: 1.334783673286438\n",
      "Epoch 468, Training Loss: 1.3424991369247437, Validation Loss: 1.3346128463745117\n",
      "Epoch 469, Training Loss: 1.3425226211547852, Validation Loss: 1.3348313570022583\n",
      "Epoch 470, Training Loss: 1.3424506187438965, Validation Loss: 1.3349266052246094\n",
      "Epoch 471, Training Loss: 1.342455506324768, Validation Loss: 1.3347222805023193\n",
      "Epoch 472, Training Loss: 1.3424811363220215, Validation Loss: 1.3349236249923706\n",
      "Epoch 473, Training Loss: 1.3424224853515625, Validation Loss: 1.3348560333251953\n",
      "Epoch 474, Training Loss: 1.3423923254013062, Validation Loss: 1.3347605466842651\n",
      "Epoch 475, Training Loss: 1.3423924446105957, Validation Loss: 1.3348848819732666\n",
      "Epoch 476, Training Loss: 1.34237802028656, Validation Loss: 1.3348233699798584\n",
      "Epoch 477, Training Loss: 1.3423596620559692, Validation Loss: 1.3349549770355225\n",
      "Epoch 478, Training Loss: 1.3423479795455933, Validation Loss: 1.3349156379699707\n",
      "Epoch 479, Training Loss: 1.342333436012268, Validation Loss: 1.3348932266235352\n",
      "Epoch 480, Training Loss: 1.342322826385498, Validation Loss: 1.3349512815475464\n",
      "Epoch 481, Training Loss: 1.3423206806182861, Validation Loss: 1.334838628768921\n",
      "Epoch 482, Training Loss: 1.3423064947128296, Validation Loss: 1.334908366203308\n",
      "Epoch 483, Training Loss: 1.3422901630401611, Validation Loss: 1.3349589109420776\n",
      "Epoch 484, Training Loss: 1.3422795534133911, Validation Loss: 1.3349615335464478\n",
      "Epoch 485, Training Loss: 1.3422739505767822, Validation Loss: 1.335144281387329\n",
      "Epoch 486, Training Loss: 1.3422682285308838, Validation Loss: 1.3350775241851807\n",
      "Epoch 487, Training Loss: 1.3422517776489258, Validation Loss: 1.3351738452911377\n",
      "Epoch 488, Training Loss: 1.3422367572784424, Validation Loss: 1.335157871246338\n",
      "Epoch 489, Training Loss: 1.342223048210144, Validation Loss: 1.3351080417633057\n",
      "Epoch 490, Training Loss: 1.3422234058380127, Validation Loss: 1.3352657556533813\n",
      "Epoch 491, Training Loss: 1.3422340154647827, Validation Loss: 1.3350831270217896\n",
      "Epoch 492, Training Loss: 1.3422285318374634, Validation Loss: 1.335273265838623\n",
      "Epoch 493, Training Loss: 1.3422061204910278, Validation Loss: 1.3351294994354248\n",
      "Epoch 494, Training Loss: 1.3421820402145386, Validation Loss: 1.3351678848266602\n",
      "Epoch 495, Training Loss: 1.3421618938446045, Validation Loss: 1.3352144956588745\n",
      "Epoch 496, Training Loss: 1.3421584367752075, Validation Loss: 1.3350859880447388\n",
      "Epoch 497, Training Loss: 1.3421745300292969, Validation Loss: 1.3353012800216675\n",
      "Epoch 498, Training Loss: 1.3421614170074463, Validation Loss: 1.3351560831069946\n",
      "Epoch 499, Training Loss: 1.3421311378479004, Validation Loss: 1.335207462310791\n",
      "Epoch 500, Training Loss: 1.3421159982681274, Validation Loss: 1.335389494895935\n",
      "Epoch 501, Training Loss: 1.342128038406372, Validation Loss: 1.3352220058441162\n",
      "Epoch 502, Training Loss: 1.3421341180801392, Validation Loss: 1.3354864120483398\n",
      "Epoch 503, Training Loss: 1.3421204090118408, Validation Loss: 1.3353205919265747\n",
      "Epoch 504, Training Loss: 1.3420934677124023, Validation Loss: 1.335405707359314\n",
      "Epoch 505, Training Loss: 1.3420699834823608, Validation Loss: 1.3354873657226562\n",
      "Epoch 506, Training Loss: 1.342065691947937, Validation Loss: 1.3354387283325195\n",
      "Epoch 507, Training Loss: 1.3420555591583252, Validation Loss: 1.3354768753051758\n",
      "Epoch 508, Training Loss: 1.3420419692993164, Validation Loss: 1.3355519771575928\n",
      "Epoch 509, Training Loss: 1.3420381546020508, Validation Loss: 1.3354465961456299\n",
      "Epoch 510, Training Loss: 1.342032551765442, Validation Loss: 1.3355525732040405\n",
      "Epoch 511, Training Loss: 1.3420277833938599, Validation Loss: 1.33540678024292\n",
      "Epoch 512, Training Loss: 1.3420169353485107, Validation Loss: 1.3355295658111572\n",
      "Epoch 513, Training Loss: 1.3420113325119019, Validation Loss: 1.3354272842407227\n",
      "Epoch 514, Training Loss: 1.3419928550720215, Validation Loss: 1.335477352142334\n",
      "Epoch 515, Training Loss: 1.341980218887329, Validation Loss: 1.3355821371078491\n",
      "Epoch 516, Training Loss: 1.3419743776321411, Validation Loss: 1.3355358839035034\n",
      "Epoch 517, Training Loss: 1.341970682144165, Validation Loss: 1.3357312679290771\n",
      "Epoch 518, Training Loss: 1.341970443725586, Validation Loss: 1.3355954885482788\n",
      "Epoch 519, Training Loss: 1.341949224472046, Validation Loss: 1.3355638980865479\n",
      "Epoch 520, Training Loss: 1.341944932937622, Validation Loss: 1.3357605934143066\n",
      "Epoch 521, Training Loss: 1.3419413566589355, Validation Loss: 1.335675597190857\n",
      "Epoch 522, Training Loss: 1.3419243097305298, Validation Loss: 1.335752248764038\n",
      "Epoch 523, Training Loss: 1.3419140577316284, Validation Loss: 1.335841178894043\n",
      "Epoch 524, Training Loss: 1.3419082164764404, Validation Loss: 1.3357080221176147\n",
      "Epoch 525, Training Loss: 1.3419055938720703, Validation Loss: 1.3359315395355225\n",
      "Epoch 526, Training Loss: 1.341907024383545, Validation Loss: 1.3357393741607666\n",
      "Epoch 527, Training Loss: 1.341888427734375, Validation Loss: 1.3359043598175049\n",
      "Epoch 528, Training Loss: 1.3418720960617065, Validation Loss: 1.3360105752944946\n",
      "Epoch 529, Training Loss: 1.341870665550232, Validation Loss: 1.3358503580093384\n",
      "Epoch 530, Training Loss: 1.3418769836425781, Validation Loss: 1.3361879587173462\n",
      "Epoch 531, Training Loss: 1.3418854475021362, Validation Loss: 1.3358731269836426\n",
      "Epoch 532, Training Loss: 1.3418728113174438, Validation Loss: 1.3361870050430298\n",
      "Epoch 533, Training Loss: 1.3418411016464233, Validation Loss: 1.336224913597107\n",
      "Epoch 534, Training Loss: 1.3418300151824951, Validation Loss: 1.336053729057312\n",
      "Epoch 535, Training Loss: 1.3418370485305786, Validation Loss: 1.3363394737243652\n",
      "Epoch 536, Training Loss: 1.3418124914169312, Validation Loss: 1.3363935947418213\n",
      "Epoch 537, Training Loss: 1.3418056964874268, Validation Loss: 1.3361971378326416\n",
      "Epoch 538, Training Loss: 1.3418108224868774, Validation Loss: 1.3363839387893677\n",
      "Epoch 539, Training Loss: 1.3417850732803345, Validation Loss: 1.3364225625991821\n",
      "Epoch 540, Training Loss: 1.341782569885254, Validation Loss: 1.3361279964447021\n",
      "Epoch 541, Training Loss: 1.3418151140213013, Validation Loss: 1.336402177810669\n",
      "Epoch 542, Training Loss: 1.3417632579803467, Validation Loss: 1.3366103172302246\n",
      "Epoch 543, Training Loss: 1.3417961597442627, Validation Loss: 1.336145043373108\n",
      "Epoch 544, Training Loss: 1.3418210744857788, Validation Loss: 1.3365930318832397\n",
      "Epoch 545, Training Loss: 1.3417638540267944, Validation Loss: 1.3364779949188232\n",
      "Epoch 546, Training Loss: 1.3417296409606934, Validation Loss: 1.3363019227981567\n",
      "Epoch 547, Training Loss: 1.3417552709579468, Validation Loss: 1.3366020917892456\n",
      "Epoch 548, Training Loss: 1.3417136669158936, Validation Loss: 1.3365757465362549\n",
      "Epoch 549, Training Loss: 1.341701865196228, Validation Loss: 1.3365402221679688\n",
      "Epoch 550, Training Loss: 1.3416959047317505, Validation Loss: 1.336675763130188\n",
      "Epoch 551, Training Loss: 1.3416938781738281, Validation Loss: 1.3364189863204956\n",
      "Epoch 552, Training Loss: 1.341705083847046, Validation Loss: 1.3366904258728027\n",
      "Epoch 553, Training Loss: 1.3416792154312134, Validation Loss: 1.3365505933761597\n",
      "Epoch 554, Training Loss: 1.3416675329208374, Validation Loss: 1.3367012739181519\n",
      "Epoch 555, Training Loss: 1.341652274131775, Validation Loss: 1.3367458581924438\n",
      "Epoch 556, Training Loss: 1.3416454792022705, Validation Loss: 1.336576223373413\n",
      "Epoch 557, Training Loss: 1.3416489362716675, Validation Loss: 1.3368403911590576\n",
      "Epoch 558, Training Loss: 1.341645359992981, Validation Loss: 1.336596965789795\n",
      "Epoch 559, Training Loss: 1.3416399955749512, Validation Loss: 1.3368974924087524\n",
      "Epoch 560, Training Loss: 1.3416223526000977, Validation Loss: 1.3367652893066406\n",
      "Epoch 561, Training Loss: 1.3416030406951904, Validation Loss: 1.336832046508789\n",
      "Epoch 562, Training Loss: 1.3415924310684204, Validation Loss: 1.336866021156311\n",
      "Epoch 563, Training Loss: 1.3415839672088623, Validation Loss: 1.3367797136306763\n",
      "Epoch 564, Training Loss: 1.3415758609771729, Validation Loss: 1.3369214534759521\n",
      "Epoch 565, Training Loss: 1.3415696620941162, Validation Loss: 1.3367505073547363\n",
      "Epoch 566, Training Loss: 1.341573715209961, Validation Loss: 1.3371535539627075\n",
      "Epoch 567, Training Loss: 1.3416048288345337, Validation Loss: 1.3366796970367432\n",
      "Epoch 568, Training Loss: 1.3416619300842285, Validation Loss: 1.337199330329895\n",
      "Epoch 569, Training Loss: 1.3415642976760864, Validation Loss: 1.3369508981704712\n",
      "Epoch 570, Training Loss: 1.3415318727493286, Validation Loss: 1.3370503187179565\n",
      "Epoch 571, Training Loss: 1.3415143489837646, Validation Loss: 1.3369492292404175\n",
      "Epoch 572, Training Loss: 1.3415061235427856, Validation Loss: 1.337052345275879\n",
      "Epoch 573, Training Loss: 1.3415006399154663, Validation Loss: 1.3369258642196655\n",
      "Epoch 574, Training Loss: 1.3415043354034424, Validation Loss: 1.3372883796691895\n",
      "Epoch 575, Training Loss: 1.3415186405181885, Validation Loss: 1.336915135383606\n",
      "Epoch 576, Training Loss: 1.3415111303329468, Validation Loss: 1.3372581005096436\n",
      "Epoch 577, Training Loss: 1.3414963483810425, Validation Loss: 1.3368421792984009\n",
      "Epoch 578, Training Loss: 1.3415184020996094, Validation Loss: 1.3373538255691528\n",
      "Epoch 579, Training Loss: 1.3415260314941406, Validation Loss: 1.3368715047836304\n",
      "Epoch 580, Training Loss: 1.341531753540039, Validation Loss: 1.3374742269515991\n",
      "Epoch 581, Training Loss: 1.3415154218673706, Validation Loss: 1.3369640111923218\n",
      "Epoch 582, Training Loss: 1.3415207862854004, Validation Loss: 1.3374062776565552\n",
      "Epoch 583, Training Loss: 1.3414461612701416, Validation Loss: 1.3372721672058105\n",
      "Epoch 584, Training Loss: 1.3414204120635986, Validation Loss: 1.336938738822937\n",
      "Epoch 585, Training Loss: 1.3414536714553833, Validation Loss: 1.3374066352844238\n",
      "Epoch 586, Training Loss: 1.3414806127548218, Validation Loss: 1.3368505239486694\n",
      "Epoch 587, Training Loss: 1.3414967060089111, Validation Loss: 1.3372920751571655\n",
      "Epoch 588, Training Loss: 1.3414033651351929, Validation Loss: 1.3373804092407227\n",
      "Epoch 589, Training Loss: 1.3414019346237183, Validation Loss: 1.3369746208190918\n",
      "Epoch 590, Training Loss: 1.3414278030395508, Validation Loss: 1.337424397468567\n",
      "Epoch 591, Training Loss: 1.3414026498794556, Validation Loss: 1.3370933532714844\n",
      "Epoch 592, Training Loss: 1.3413715362548828, Validation Loss: 1.3374284505844116\n",
      "Epoch 593, Training Loss: 1.3413585424423218, Validation Loss: 1.3371965885162354\n",
      "Epoch 594, Training Loss: 1.3413617610931396, Validation Loss: 1.337583065032959\n",
      "Epoch 595, Training Loss: 1.3413400650024414, Validation Loss: 1.337485432624817\n",
      "Epoch 596, Training Loss: 1.3413206338882446, Validation Loss: 1.3372968435287476\n",
      "Epoch 597, Training Loss: 1.3413267135620117, Validation Loss: 1.3376466035842896\n",
      "Epoch 598, Training Loss: 1.3413447141647339, Validation Loss: 1.3371789455413818\n",
      "Epoch 599, Training Loss: 1.3413810729980469, Validation Loss: 1.3377091884613037\n",
      "Epoch 600, Training Loss: 1.3413197994232178, Validation Loss: 1.337349772453308\n",
      "Epoch 601, Training Loss: 1.3413119316101074, Validation Loss: 1.3377044200897217\n",
      "Epoch 602, Training Loss: 1.3412961959838867, Validation Loss: 1.3374773263931274\n",
      "Epoch 603, Training Loss: 1.3412659168243408, Validation Loss: 1.3373085260391235\n",
      "Epoch 604, Training Loss: 1.3412784337997437, Validation Loss: 1.3378031253814697\n",
      "Epoch 605, Training Loss: 1.3413606882095337, Validation Loss: 1.337174654006958\n",
      "Epoch 606, Training Loss: 1.3414205312728882, Validation Loss: 1.3378374576568604\n",
      "Epoch 607, Training Loss: 1.341268539428711, Validation Loss: 1.3377916812896729\n",
      "Epoch 608, Training Loss: 1.341241717338562, Validation Loss: 1.3373780250549316\n",
      "Epoch 609, Training Loss: 1.3412814140319824, Validation Loss: 1.3377838134765625\n",
      "Epoch 610, Training Loss: 1.341235876083374, Validation Loss: 1.337449312210083\n",
      "Epoch 611, Training Loss: 1.341228723526001, Validation Loss: 1.3377642631530762\n",
      "Epoch 612, Training Loss: 1.341204047203064, Validation Loss: 1.3376495838165283\n",
      "Epoch 613, Training Loss: 1.341192364692688, Validation Loss: 1.3378790616989136\n",
      "Epoch 614, Training Loss: 1.3411864042282104, Validation Loss: 1.3375930786132812\n",
      "Epoch 615, Training Loss: 1.3412044048309326, Validation Loss: 1.3380659818649292\n",
      "Epoch 616, Training Loss: 1.3412351608276367, Validation Loss: 1.337526559829712\n",
      "Epoch 617, Training Loss: 1.341261625289917, Validation Loss: 1.3380358219146729\n",
      "Epoch 618, Training Loss: 1.3411797285079956, Validation Loss: 1.3379260301589966\n",
      "Epoch 619, Training Loss: 1.3411471843719482, Validation Loss: 1.337710976600647\n",
      "Epoch 620, Training Loss: 1.341181993484497, Validation Loss: 1.3381152153015137\n",
      "Epoch 621, Training Loss: 1.3411566019058228, Validation Loss: 1.3378530740737915\n",
      "Epoch 622, Training Loss: 1.341116189956665, Validation Loss: 1.3378509283065796\n",
      "Epoch 623, Training Loss: 1.3411113023757935, Validation Loss: 1.3382006883621216\n",
      "Epoch 624, Training Loss: 1.3411484956741333, Validation Loss: 1.3377182483673096\n",
      "Epoch 625, Training Loss: 1.341207504272461, Validation Loss: 1.3381253480911255\n",
      "Epoch 626, Training Loss: 1.3410922288894653, Validation Loss: 1.3380461931228638\n",
      "Epoch 627, Training Loss: 1.3410784006118774, Validation Loss: 1.3376513719558716\n",
      "Epoch 628, Training Loss: 1.3411208391189575, Validation Loss: 1.337967038154602\n",
      "Epoch 629, Training Loss: 1.3410975933074951, Validation Loss: 1.3375766277313232\n",
      "Epoch 630, Training Loss: 1.3410760164260864, Validation Loss: 1.3377996683120728\n",
      "Epoch 631, Training Loss: 1.3410378694534302, Validation Loss: 1.3378239870071411\n",
      "Epoch 632, Training Loss: 1.3410279750823975, Validation Loss: 1.3377559185028076\n",
      "Epoch 633, Training Loss: 1.3410208225250244, Validation Loss: 1.337907314300537\n",
      "Epoch 634, Training Loss: 1.3410285711288452, Validation Loss: 1.337576150894165\n",
      "Epoch 635, Training Loss: 1.341054916381836, Validation Loss: 1.3380546569824219\n",
      "Epoch 636, Training Loss: 1.3410452604293823, Validation Loss: 1.337717056274414\n",
      "Epoch 637, Training Loss: 1.341049075126648, Validation Loss: 1.3381400108337402\n",
      "Epoch 638, Training Loss: 1.3410009145736694, Validation Loss: 1.3379600048065186\n",
      "Epoch 639, Training Loss: 1.3409652709960938, Validation Loss: 1.3379454612731934\n",
      "Epoch 640, Training Loss: 1.3409544229507446, Validation Loss: 1.3381822109222412\n",
      "Epoch 641, Training Loss: 1.3409665822982788, Validation Loss: 1.337822675704956\n",
      "Epoch 642, Training Loss: 1.3410314321517944, Validation Loss: 1.3382593393325806\n",
      "Epoch 643, Training Loss: 1.340943455696106, Validation Loss: 1.338080883026123\n",
      "Epoch 644, Training Loss: 1.3409122228622437, Validation Loss: 1.3379725217819214\n",
      "Epoch 645, Training Loss: 1.340903401374817, Validation Loss: 1.338062047958374\n",
      "Epoch 646, Training Loss: 1.340907335281372, Validation Loss: 1.3378394842147827\n",
      "Epoch 647, Training Loss: 1.3409048318862915, Validation Loss: 1.3381307125091553\n",
      "Epoch 648, Training Loss: 1.340895175933838, Validation Loss: 1.3380322456359863\n",
      "Epoch 649, Training Loss: 1.3408710956573486, Validation Loss: 1.3381927013397217\n",
      "Epoch 650, Training Loss: 1.340853214263916, Validation Loss: 1.3382536172866821\n",
      "Epoch 651, Training Loss: 1.3408424854278564, Validation Loss: 1.3381508588790894\n",
      "Epoch 652, Training Loss: 1.3408387899398804, Validation Loss: 1.3383557796478271\n",
      "Epoch 653, Training Loss: 1.3408514261245728, Validation Loss: 1.3381565809249878\n",
      "Epoch 654, Training Loss: 1.3408246040344238, Validation Loss: 1.3383255004882812\n",
      "Epoch 655, Training Loss: 1.3408050537109375, Validation Loss: 1.33829665184021\n",
      "Epoch 656, Training Loss: 1.3408023118972778, Validation Loss: 1.3384841680526733\n",
      "Epoch 657, Training Loss: 1.3407965898513794, Validation Loss: 1.3382337093353271\n",
      "Epoch 658, Training Loss: 1.3407766819000244, Validation Loss: 1.3383302688598633\n",
      "Epoch 659, Training Loss: 1.3407647609710693, Validation Loss: 1.338273525238037\n",
      "Epoch 660, Training Loss: 1.3407487869262695, Validation Loss: 1.3384183645248413\n",
      "Epoch 661, Training Loss: 1.340733289718628, Validation Loss: 1.3383270502090454\n",
      "Epoch 662, Training Loss: 1.340732216835022, Validation Loss: 1.338652491569519\n",
      "Epoch 663, Training Loss: 1.3407669067382812, Validation Loss: 1.338170051574707\n",
      "Epoch 664, Training Loss: 1.340806245803833, Validation Loss: 1.3388632535934448\n",
      "Epoch 665, Training Loss: 1.340846061706543, Validation Loss: 1.3383077383041382\n",
      "Epoch 666, Training Loss: 1.3407423496246338, Validation Loss: 1.3387055397033691\n",
      "Epoch 667, Training Loss: 1.3406600952148438, Validation Loss: 1.3387224674224854\n",
      "Epoch 668, Training Loss: 1.340636134147644, Validation Loss: 1.3386362791061401\n",
      "Epoch 669, Training Loss: 1.3406273126602173, Validation Loss: 1.3387908935546875\n",
      "Epoch 670, Training Loss: 1.3406137228012085, Validation Loss: 1.3385775089263916\n",
      "Epoch 671, Training Loss: 1.3405847549438477, Validation Loss: 1.338639736175537\n",
      "Epoch 672, Training Loss: 1.3405635356903076, Validation Loss: 1.3389060497283936\n",
      "Epoch 673, Training Loss: 1.340559959411621, Validation Loss: 1.338792324066162\n",
      "Epoch 674, Training Loss: 1.3405359983444214, Validation Loss: 1.3389092683792114\n",
      "Epoch 675, Training Loss: 1.340514063835144, Validation Loss: 1.3387393951416016\n",
      "Epoch 676, Training Loss: 1.340498685836792, Validation Loss: 1.3389085531234741\n",
      "Epoch 677, Training Loss: 1.340488314628601, Validation Loss: 1.3386595249176025\n",
      "Epoch 678, Training Loss: 1.3404582738876343, Validation Loss: 1.3388360738754272\n",
      "Epoch 679, Training Loss: 1.3404425382614136, Validation Loss: 1.3386887311935425\n",
      "Epoch 680, Training Loss: 1.340419054031372, Validation Loss: 1.3387569189071655\n",
      "Epoch 681, Training Loss: 1.3403931856155396, Validation Loss: 1.3389172554016113\n",
      "Epoch 682, Training Loss: 1.3403836488723755, Validation Loss: 1.338849425315857\n",
      "Epoch 683, Training Loss: 1.3403851985931396, Validation Loss: 1.3392114639282227\n",
      "Epoch 684, Training Loss: 1.3404147624969482, Validation Loss: 1.3388327360153198\n",
      "Epoch 685, Training Loss: 1.3404735326766968, Validation Loss: 1.3394019603729248\n",
      "Epoch 686, Training Loss: 1.3404730558395386, Validation Loss: 1.3389012813568115\n",
      "Epoch 687, Training Loss: 1.3403476476669312, Validation Loss: 1.3389678001403809\n",
      "Epoch 688, Training Loss: 1.340315818786621, Validation Loss: 1.3393583297729492\n",
      "Epoch 689, Training Loss: 1.34036123752594, Validation Loss: 1.3390840291976929\n",
      "Epoch 690, Training Loss: 1.3403465747833252, Validation Loss: 1.3394747972488403\n",
      "Epoch 691, Training Loss: 1.3403229713439941, Validation Loss: 1.339286208152771\n",
      "Epoch 692, Training Loss: 1.340272068977356, Validation Loss: 1.3392311334609985\n",
      "Epoch 693, Training Loss: 1.3402988910675049, Validation Loss: 1.339729905128479\n",
      "Epoch 694, Training Loss: 1.340316891670227, Validation Loss: 1.3393115997314453\n",
      "Epoch 695, Training Loss: 1.34024178981781, Validation Loss: 1.3393001556396484\n",
      "Epoch 696, Training Loss: 1.3402044773101807, Validation Loss: 1.3392388820648193\n",
      "Epoch 697, Training Loss: 1.3402020931243896, Validation Loss: 1.3391103744506836\n",
      "Epoch 698, Training Loss: 1.340219259262085, Validation Loss: 1.3393555879592896\n",
      "Epoch 699, Training Loss: 1.3402122259140015, Validation Loss: 1.3390663862228394\n",
      "Epoch 700, Training Loss: 1.3402034044265747, Validation Loss: 1.3392424583435059\n",
      "Epoch 701, Training Loss: 1.3401905298233032, Validation Loss: 1.3388816118240356\n",
      "Epoch 702, Training Loss: 1.3401458263397217, Validation Loss: 1.3389209508895874\n",
      "Epoch 703, Training Loss: 1.3401217460632324, Validation Loss: 1.3390476703643799\n",
      "Epoch 704, Training Loss: 1.3401188850402832, Validation Loss: 1.3390272855758667\n",
      "Epoch 705, Training Loss: 1.3401281833648682, Validation Loss: 1.3394392728805542\n",
      "Epoch 706, Training Loss: 1.3401708602905273, Validation Loss: 1.3390144109725952\n",
      "Epoch 707, Training Loss: 1.340129017829895, Validation Loss: 1.3392153978347778\n",
      "Epoch 708, Training Loss: 1.3400782346725464, Validation Loss: 1.3391685485839844\n",
      "Epoch 709, Training Loss: 1.3400421142578125, Validation Loss: 1.3392424583435059\n",
      "Epoch 710, Training Loss: 1.3400319814682007, Validation Loss: 1.3393653631210327\n",
      "Epoch 711, Training Loss: 1.3400294780731201, Validation Loss: 1.3392032384872437\n",
      "Epoch 712, Training Loss: 1.3400273323059082, Validation Loss: 1.3394687175750732\n",
      "Epoch 713, Training Loss: 1.3400280475616455, Validation Loss: 1.339277982711792\n",
      "Epoch 714, Training Loss: 1.3400146961212158, Validation Loss: 1.3396470546722412\n",
      "Epoch 715, Training Loss: 1.340018391609192, Validation Loss: 1.3393363952636719\n",
      "Epoch 716, Training Loss: 1.3400028944015503, Validation Loss: 1.3394886255264282\n",
      "Epoch 717, Training Loss: 1.3399688005447388, Validation Loss: 1.3391661643981934\n",
      "Epoch 718, Training Loss: 1.339939832687378, Validation Loss: 1.3391180038452148\n",
      "Epoch 719, Training Loss: 1.339927315711975, Validation Loss: 1.3390936851501465\n",
      "Epoch 720, Training Loss: 1.3399208784103394, Validation Loss: 1.339111089706421\n",
      "Epoch 721, Training Loss: 1.3399180173873901, Validation Loss: 1.3393293619155884\n",
      "Epoch 722, Training Loss: 1.339921474456787, Validation Loss: 1.3392058610916138\n",
      "Epoch 723, Training Loss: 1.3399252891540527, Validation Loss: 1.339442491531372\n",
      "Epoch 724, Training Loss: 1.339906096458435, Validation Loss: 1.3392541408538818\n",
      "Epoch 725, Training Loss: 1.3398858308792114, Validation Loss: 1.3393322229385376\n",
      "Epoch 726, Training Loss: 1.339869499206543, Validation Loss: 1.339389443397522\n",
      "Epoch 727, Training Loss: 1.339860439300537, Validation Loss: 1.3393328189849854\n",
      "Epoch 728, Training Loss: 1.3398568630218506, Validation Loss: 1.3395404815673828\n",
      "Epoch 729, Training Loss: 1.339855432510376, Validation Loss: 1.3394192457199097\n",
      "Epoch 730, Training Loss: 1.3398326635360718, Validation Loss: 1.3394756317138672\n",
      "Epoch 731, Training Loss: 1.339817762374878, Validation Loss: 1.3396350145339966\n",
      "Epoch 732, Training Loss: 1.339817762374878, Validation Loss: 1.3393933773040771\n",
      "Epoch 733, Training Loss: 1.3398158550262451, Validation Loss: 1.3396687507629395\n",
      "Epoch 734, Training Loss: 1.339820146560669, Validation Loss: 1.3393150568008423\n",
      "Epoch 735, Training Loss: 1.3398231267929077, Validation Loss: 1.3397835493087769\n",
      "Epoch 736, Training Loss: 1.3398786783218384, Validation Loss: 1.3392783403396606\n",
      "Epoch 737, Training Loss: 1.3398462533950806, Validation Loss: 1.3398916721343994\n",
      "Epoch 738, Training Loss: 1.3398793935775757, Validation Loss: 1.3393174409866333\n",
      "Epoch 739, Training Loss: 1.3398432731628418, Validation Loss: 1.3398631811141968\n",
      "Epoch 740, Training Loss: 1.339869737625122, Validation Loss: 1.3394253253936768\n",
      "Epoch 741, Training Loss: 1.3398164510726929, Validation Loss: 1.3398418426513672\n",
      "Epoch 742, Training Loss: 1.3397350311279297, Validation Loss: 1.3398807048797607\n",
      "Epoch 743, Training Loss: 1.3397055864334106, Validation Loss: 1.3398844003677368\n",
      "Epoch 744, Training Loss: 1.3397140502929688, Validation Loss: 1.3402597904205322\n",
      "Epoch 745, Training Loss: 1.339755654335022, Validation Loss: 1.3397728204727173\n",
      "Epoch 746, Training Loss: 1.339755654335022, Validation Loss: 1.3401215076446533\n",
      "Epoch 747, Training Loss: 1.3396965265274048, Validation Loss: 1.3400996923446655\n",
      "Epoch 748, Training Loss: 1.3396786451339722, Validation Loss: 1.3398535251617432\n",
      "Epoch 749, Training Loss: 1.3397283554077148, Validation Loss: 1.3403301239013672\n",
      "Epoch 750, Training Loss: 1.3397150039672852, Validation Loss: 1.3398489952087402\n",
      "Epoch 751, Training Loss: 1.339667558670044, Validation Loss: 1.340052604675293\n",
      "Epoch 752, Training Loss: 1.3396304845809937, Validation Loss: 1.3402137756347656\n",
      "Epoch 753, Training Loss: 1.3396210670471191, Validation Loss: 1.3401470184326172\n",
      "Epoch 754, Training Loss: 1.3396344184875488, Validation Loss: 1.340506911277771\n",
      "Epoch 755, Training Loss: 1.3396552801132202, Validation Loss: 1.3400304317474365\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(dml_model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dml_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = dml_model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1+1 ])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    dml_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dml_model(X_user_val,  product_features, all_x_other_products,prices)[0]\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        torch.save(dml_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b27e55e",
   "metadata": {
    "id": "1b27e55e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5efd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3912.01\n",
      "Expected Revenue all treated: $769.59\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121cd47",
   "metadata": {
    "id": "1121cd47"
   },
   "source": [
    "# debias the GTE estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "SMTzkngzuUls",
   "metadata": {
    "id": "SMTzkngzuUls"
   },
   "outputs": [],
   "source": [
    "test_prepared_data = prepare_data(X_user_test, X_product,  price*(discount*prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = test_prepared_data\n",
    "\n",
    "# Compute Theta0 and Theta1\n",
    "_,theta0_output,theta1_output = dml_model(user_features, product_features, all_x_other_products,prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de3fecfc-2f03-48e9-a089-5cebf4ac0f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 10])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rag3u55FWjiu",
   "metadata": {
    "id": "Rag3u55FWjiu"
   },
   "source": [
    "# use formulation debias for H_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "SSXrdFP4WnZL",
   "metadata": {
    "id": "SSXrdFP4WnZL"
   },
   "outputs": [],
   "source": [
    "def H_theta(theta0_output,theta1_output,all_treated_price,price):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = all_treated_price.unsqueeze(0).expand(N, M)\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_treated_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    all_treated_probabilities = F.softmax(all_treated_uti, dim=1)\n",
    "    all_control_probabilities = F.softmax(all_control_uti, dim=1)\n",
    "\n",
    "    price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "    treated_price_with_outside =  torch.cat((torch.zeros(1, device=all_treated_price.device),all_treated_price), dim=0)\n",
    "\n",
    "    H = torch.sum(all_treated_probabilities*treated_price_with_outside - all_control_probabilities*price_with_outside,dim=1)\n",
    "    expsum_treated = torch.sum(torch.exp(all_treated_uti),dim=1)\n",
    "    expsum_control = torch.sum(torch.exp(all_control_uti),dim=1)\n",
    "\n",
    "    expsum_treated_expanded = expsum_treated.unsqueeze(1).expand(-1, all_treated_uti.shape[1])  # Shape [N, M+1]\n",
    "    expsum_control_expanded = expsum_control.unsqueeze(1).expand(-1, all_control_uti.shape[1])  # Shape [N, M+1]\n",
    "\n",
    "    H_theta0 = torch.sum((torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded-\\\n",
    "                          torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded)\\\n",
    "                         *price_with_outside,dim=1)\n",
    "    H_theta1 = torch.sum(price_with_outside*(torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded*treated_price_with_outside-\\\n",
    "                                             torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded*price_with_outside),dim=1)\n",
    "\n",
    "\n",
    "    return H,H_theta0,H_theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "BFHZQM3VfBUI",
   "metadata": {
    "id": "BFHZQM3VfBUI"
   },
   "outputs": [],
   "source": [
    "H,H_theta0,H_theta1 = H_theta(theta0_output,theta1_output,all_treated_price,price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "yivIC_MKad5x",
   "metadata": {
    "id": "yivIC_MKad5x"
   },
   "outputs": [],
   "source": [
    "def l_theta(theta0_output,theta1_output,adjusted_price,decision_test):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_adjusted_price = adjusted_price.unsqueeze(0).expand(N, M)\n",
    "    uti = theta0_output + theta1_output * expand_adjusted_price\n",
    "    adjusted_price_with_outside =  torch.cat([torch.zeros(1, device=adjusted_price.device),adjusted_price])\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=uti.device)\n",
    "    uti = torch.cat((zero_utilities,uti), dim=1)\n",
    "\n",
    "    probabilities = F.softmax(uti, dim=1)\n",
    "    prod_indices = torch.ones(NUM_Product, device=device)\n",
    "    prod_indices = torch.cat([torch.zeros(1,device=device),prod_indices])\n",
    "    ltheta0 = probabilities[torch.arange(decision_test.size(0)), decision_test+1] -prod_indices[decision_test+1]\n",
    "    ltheta1 = (probabilities[torch.arange(decision_test.size(0)), decision_test+1] * adjusted_price_with_outside[decision_test+1]) - adjusted_price_with_outside[decision_test+1]\n",
    "\n",
    "\n",
    "    return ltheta0,ltheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77bcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "O8c-tupIgHVu",
   "metadata": {
    "id": "O8c-tupIgHVu"
   },
   "outputs": [],
   "source": [
    "adjusted_price = price*(discount*prod_randomization).to(device)\n",
    "decision_test = decision_test.to(device)\n",
    "ltheta0,ltheta1= l_theta(theta0_output,theta1_output,adjusted_price,decision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UVRht78QaxSG",
   "metadata": {
    "id": "UVRht78QaxSG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lambdainv(theta0_output, theta1_output, price, decision_test,epsilon =10):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = discount*price.unsqueeze(0).expand(N, M)\n",
    "\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_control_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities_control = F.softmax(all_control_uti, dim=1)\n",
    "    probabilities_treated = F.softmax(all_treated_uti, dim=1)\n",
    "\n",
    "    # Extract probabilities of chosen products\n",
    "    chosen_prob_control = probabilities_control[torch.arange(N), decision_test]\n",
    "    chosen_prob_treated = probabilities_treated[torch.arange(N), decision_test]\n",
    "\n",
    "    # Calculate second derivatives\n",
    "    ltheta00 = chosen_prob_control * (1 - chosen_prob_control) + chosen_prob_treated * (1 - chosen_prob_treated)\n",
    "    ltheta01 = chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test] + \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])\n",
    "    ltheta11 = chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test]**2 + \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])**2\n",
    "    ltheta00=ltheta00/2\n",
    "    ltheta01=ltheta01/2\n",
    "    ltheta11=ltheta11/2\n",
    "\n",
    "    # Form the 2x2 Hessian matrices for each instance\n",
    "    ltheta00 = ltheta00.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta01 = ltheta01.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta11 = ltheta11.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    top_row = torch.cat((ltheta00, ltheta01), dim=2)\n",
    "    bottom_row = torch.cat((ltheta01, ltheta11), dim=2)\n",
    "\n",
    "    L_matrix = torch.cat((top_row, bottom_row), dim=1)\n",
    "\n",
    "    # Regularization and inversion\n",
    "    \n",
    "    identity_matrix = torch.eye(2, dtype=L_matrix.dtype, device=L_matrix.device) * epsilon\n",
    "    L_matrix_reg = L_matrix + identity_matrix.unsqueeze(0).unsqueeze(0)\n",
    "    L_inv = torch.linalg.inv(L_matrix_reg)\n",
    "\n",
    "    return L_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6f7441c-eea7-4f98-8631-71cb75d14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.001,0.01,0.1,0.5,1,5,10]\n",
    "min_mape = float('inf')\n",
    "best_epsilon = None\n",
    "best_final_result = None\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    # Update L_inv for the current epsilon\n",
    "    try:\n",
    "        L_inv = lambdainv(theta0_output, theta1_output, price, decision_test, epsilon).float()\n",
    "    \n",
    "        # Calculate final_result with the given epsilon\n",
    "        H_theta_array = torch.stack((H_theta0, H_theta1), dim=-1).unsqueeze(1).float()  \n",
    "        l_theta_array = torch.stack((ltheta0, ltheta1), dim=-1).unsqueeze(-1).float()  \n",
    "    \n",
    "        # Perform matrix multiplications\n",
    "        result_intermediate = torch.matmul(H_theta_array, L_inv.squeeze(0)) \n",
    "        final_result = torch.matmul(result_intermediate, l_theta_array).squeeze(-1)  \n",
    "        final_result[torch.isnan(final_result) | torch.isinf(final_result)] = 0\n",
    "    \n",
    "        # Calculate sdl and dedl\n",
    "        sdl = H.sum().cpu().detach().numpy() * 2\n",
    "        dedl = (H.sum().cpu().detach().numpy() - final_result.sum().cpu().detach().numpy()) * 2\n",
    "    \n",
    "        # Calculate MAPE of dedl with respect to true\n",
    "        mape_dedl = np.abs((dedl - true) / true)\n",
    "    \n",
    "        # Update best_epsilon if the current epsilon yields a lower MAPE\n",
    "        if mape_dedl < min_mape:\n",
    "            min_mape = mape_dedl\n",
    "            best_epsilon = epsilon\n",
    "            best_final_result = final_result\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "q11HQu-goWM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11HQu-goWM0",
    "outputId": "ecf71629-8dbc-4d80-8d4a-de1f2534e3eb"
   },
   "outputs": [],
   "source": [
    "sdl = H.sum().cpu().detach().numpy()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3a445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedl = (H.sum().cpu().detach().numpy()-best_final_result.sum().cpu().detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c6d8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6284.8388671875, -5842.5859375, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl,dedl,best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d44fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of SDL:  -11.91%\n",
      "Absolute Percentage Estimation Error of SP MNL:  -4.03%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of SDL:  {100*np.abs(sdl-revenue_difference)/revenue_difference:.2f}%\")\n",
    "print(f\"Absolute Percentage Estimation Error of SP MNL:  {100*np.abs(dedl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eeea3d7d-d20e-48c8-b037-107fce171386",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pe = (naive - true) / true\n",
    "linear_pe = (linear - true) / true\n",
    "pdl_pe = (pdl - true) / true\n",
    "sdl_pe = (sdl - true) / true\n",
    "dedl_pe = (dedl - true) / true\n",
    "naive_mse = (naive - true)**2\n",
    "linear_mse =(linear - true)**2\n",
    "pdl_mse = (pdl - true)**2\n",
    "sdl_mse = (sdl - true)**2\n",
    "dedl_mse = (dedl - true)**2\n",
    "naive_e = (naive - true)\n",
    "linear_e =(linear - true)\n",
    "pdl_e = (pdl - true)\n",
    "sdl_e = (sdl - true)\n",
    "dedl_e = (dedl - true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1094fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2498093117479741 -0.08558367727234319 0.05095460959973493 0.11907752528865774 0.04033003079198123 -1402.95130264014 480.64553988724947 -286.16561733931303 -668.7499679252505 -226.49703823775053 1968272.357579666 231020.13501350553 81890.76054719013 447226.51960002363 51300.90833047302\n"
     ]
    }
   ],
   "source": [
    "print(naive_pe,linear_pe,pdl_pe,sdl_pe,dedl_pe,naive_e,linear_e,pdl_e,sdl_e,dedl_e,naive_mse,linear_mse,pdl_mse,sdl_mse,dedl_mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "c2a5b9ca",
    "7e135e9e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
