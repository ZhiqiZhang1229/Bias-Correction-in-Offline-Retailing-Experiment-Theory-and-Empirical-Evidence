{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27192f",
   "metadata": {
    "id": "6c27192f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14efbb6",
   "metadata": {
    "id": "f14efbb6"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56819b49-b3bb-4d32-ad88-a458e7ee142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ff42b4-0dd7-4555-bfbc-c1d6a7c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Product = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42efd20-8c8f-4675-aa1a-9a2534ee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_percentage = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce9a174-68e6-47f3-ad2f-966cc2698e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc24047e-fa23-40ae-b761-5f0d1fbe595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338e9f4b-a5a7-47a9-b06d-5216d077594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083308b",
   "metadata": {
    "id": "9083308b"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "USER_Cont_FEATURES = 2*user_continuous_feature_multiplier\n",
    "USER_Dicr_FEATURES = 3\n",
    "\n",
    "Product_Cont_FEATURES = 3*prod_continuous_feature_multiplier\n",
    "Product_Dicr_FEATURES = 2\n",
    "OUTSIDE_OPTION_UTILITY = 0\n",
    "utilities = torch.zeros(NUM_USER, NUM_Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442d9dd4",
   "metadata": {
    "id": "442d9dd4"
   },
   "outputs": [],
   "source": [
    "def generate_features(N, C, D):\n",
    "    continuous_features = np.zeros((N, C))\n",
    "    for i in range(C):\n",
    "        continuous_features[:, i] = np.random.uniform(0,1,size=N)\n",
    "    binary_features = np.random.randint(0, 2, (N, D))\n",
    "    return np.hstack((continuous_features, binary_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84b940",
   "metadata": {
    "id": "cb84b940"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NonparametricUtilityDNN(nn.Module):\n",
    "    def __init__(self, user_features, product_features):\n",
    "        super(NonparametricUtilityDNN, self).__init__()\n",
    "        \n",
    "        self.original_dim = user_features + product_features + 1\n",
    "        expanded_dim = (self.original_dim - 1) + 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(expanded_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        nn.init.uniform_(self.fc1.weight, -2, 2)\n",
    "        nn.init.uniform_(self.fc2.weight, -2, 2)\n",
    "        nn.init.uniform_(self.fc3.weight, -1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = x[:, :-1] \n",
    "        \n",
    "\n",
    "        price = x[:, -1:] \n",
    "\n",
    "        p_sq = torch.pow(price, 2)       # Quadratic\n",
    "        p_quad = torch.pow(price, 4)     # Quartic (Explosive growth)\n",
    "        p_log = torch.log(torch.abs(price) + 1e-6) # Logarithmic (Decaying growth)\n",
    "\n",
    "        x_new = torch.cat([feats, price, p_sq, p_quad, p_log], dim=1)\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x_new))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        \n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def utility_model_batched_nonparametric(x_user, X_product, price, user_randomization, \n",
    "                                        prod_randomization, dnn_model, gumbel_noise, \n",
    "                                        discount, batch_size=10):\n",
    "    \n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)\n",
    "\n",
    "    # Convert numpy arrays to tensors if necessary\n",
    "    if isinstance(user_randomization, np.ndarray):\n",
    "        user_randomization = torch.from_numpy(user_randomization).to(torch.bool)\n",
    "    if isinstance(prod_randomization, np.ndarray):\n",
    "        prod_randomization = torch.from_numpy(prod_randomization).to(torch.bool)\n",
    "    if isinstance(price, np.ndarray):\n",
    "        price = torch.from_numpy(price).float()\n",
    "\n",
    "    # Iterate over users in batches\n",
    "    for i in range(0, num_users, batch_size):\n",
    "        batch_end = min(i + batch_size, num_users)\n",
    "        batch_indices = slice(i, batch_end)\n",
    "        current_batch_size = batch_end - i\n",
    "\n",
    "\n",
    "        batch_user_features = x_user[batch_indices].unsqueeze(1).expand(-1, num_products, -1)\n",
    "        batch_prod_features = X_product.unsqueeze(0).expand(current_batch_size, -1, -1)\n",
    "        batch_price = price.unsqueeze(0).expand(current_batch_size, -1).unsqueeze(-1) \n",
    "        batch_user_treatment = user_randomization[batch_indices].unsqueeze(1).expand(-1, num_products)\n",
    "        batch_prod_treatment = prod_randomization.unsqueeze(0).expand(current_batch_size, -1)\n",
    "        is_treated = batch_user_treatment | batch_prod_treatment\n",
    "        \n",
    "\n",
    "        batch_adjusted_price = torch.where(\n",
    "            is_treated.unsqueeze(-1), \n",
    "            batch_price * discount, \n",
    "            batch_price\n",
    "        )\n",
    "        combined_features = torch.cat((batch_user_features, batch_prod_features, batch_adjusted_price), dim=2)\n",
    "        flat_features = combined_features.view(-1, combined_features.shape[-1])\n",
    "        utility_preds = dnn_model(flat_features)\n",
    "        utility_deterministic = utility_preds.view(current_batch_size, num_products)\n",
    "        \n",
    "        # Add noise\n",
    "        batch_utilities = utility_deterministic + gumbel_noise[batch_indices]\n",
    "\n",
    "        max_utilities, chosen_products = torch.max(batch_utilities, dim=1)\n",
    "\n",
    "        outside_option = -1 * torch.ones_like(chosen_products, dtype=torch.long)\n",
    "        decisions[batch_indices] = torch.where(max_utilities > 0, chosen_products, outside_option)\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e26f6da",
   "metadata": {
    "id": "0e26f6da"
   },
   "outputs": [],
   "source": [
    "def utility_model(x_user, X_product, price, user_randomization, prod_randomization,pair_utility_model,price_sensitivity_model,gumbel_noise):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    \n",
    "\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    for i in range(num_users):\n",
    "\n",
    "        for j in range(num_products):\n",
    "            # Determine if the user and product are in the treatment group\n",
    "            is_user_treated = (user_randomization[i] == 1)\n",
    "            is_product_treated = (prod_randomization[j] == 1)\n",
    "\n",
    "            # Adjust price based on the experiment conditions\n",
    "            adjusted_price = price[j] * discount if is_user_treated or is_product_treated else price[j]\n",
    "            combined_features = torch.cat((x_user[i], X_product[j]), 0)\n",
    "            utility_from_dnn = pair_utility_model(combined_features)\n",
    "            price_effect = price_sensitivities[i] * adjusted_price\n",
    "\n",
    "            utilities[i, j] = utility_from_dnn - price_effect + gumbel_noise[i,j]\n",
    "\n",
    "    return utility_from_dnn,price_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22bf01fa",
   "metadata": {
    "id": "22bf01fa"
   },
   "outputs": [],
   "source": [
    "def make_decision(utilities):\n",
    "    num_users = utilities.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  \n",
    "    for i in range(num_users):\n",
    "        max_utility, chosen_product = torch.max(utilities[i], dim=0)\n",
    "\n",
    "        # Compare the maximum utility with the outside option (utility = 0)\n",
    "        if max_utility <= 0:\n",
    "            decisions[i] = -1 \n",
    "        else:\n",
    "            decisions[i] = chosen_product \n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b9061",
   "metadata": {
    "id": "3a7b9061"
   },
   "outputs": [],
   "source": [
    "def calculate_revenue(decisions, prices):\n",
    "    total_revenue = 0.0\n",
    "\n",
    "    # Iterate over each decision and add the corresponding product price to total revenue\n",
    "    for i, decision in enumerate(decisions):\n",
    "        if decision != -1:  # Check if the decision is not the outside option\n",
    "            total_revenue += prices[decision].item()  # Add the price of the chosen product\n",
    "\n",
    "    return total_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be800ea9",
   "metadata": {
    "id": "be800ea9"
   },
   "outputs": [],
   "source": [
    "X_user = generate_features(NUM_USER,USER_Cont_FEATURES, USER_Dicr_FEATURES)\n",
    "X_product = generate_features(NUM_Product, Product_Cont_FEATURES, Product_Dicr_FEATURES)\n",
    "price = np.random.uniform(0.5 ,1, NUM_Product)\n",
    "\n",
    "X_user = torch.from_numpy(X_user).float()\n",
    "X_product = torch.from_numpy(X_product).float()\n",
    "price = torch.from_numpy(price).float()\n",
    "gumbel_dist = torch.distributions.Gumbel(0, 1)\n",
    "gumbel_noise = gumbel_dist.sample((NUM_USER, NUM_Product))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97381ec",
   "metadata": {
    "id": "c97381ec"
   },
   "source": [
    "# GTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c9652",
   "metadata": {
    "id": "c74c9652"
   },
   "source": [
    "## All treated scenario: all products are discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8620e2f7",
   "metadata": {
    "id": "8620e2f7"
   },
   "outputs": [],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a69429d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a69429d2",
    "outputId": "da77f021-0faf-424d-91be-088437608bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([2, 2, 5,  ..., 2, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "user_dim = X_user.shape[1]\n",
    "product_dim = X_product.shape[1]\n",
    "model = NonparametricUtilityDNN(user_dim, product_dim)\n",
    "decisions_all_treat = utility_model_batched_nonparametric(\n",
    "    x_user=X_user,\n",
    "    X_product=X_product,\n",
    "    price=price,\n",
    "    user_randomization=user_randomization,\n",
    "    prod_randomization=prod_randomization,\n",
    "    dnn_model=model,  \n",
    "    gumbel_noise=gumbel_noise,\n",
    "    discount=discount, \n",
    "    \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b7df52-fdc0-46a9-ad64-9119957d6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "all_num_unique = torch.unique(decisions_all_treat).numel()\n",
    "print(all_num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8748518-f99d-47ae-a031-7b89fbbfa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n",
      "tensor(773)\n",
      "tensor(145)\n",
      "tensor(5127)\n",
      "tensor(1139)\n",
      "tensor(921)\n",
      "tensor(179)\n",
      "tensor(136)\n",
      "tensor(1043)\n",
      "tensor(410)\n",
      "tensor(116)\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1,10):\n",
    "    print(torch.sum(decisions_all_treat==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59995c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "f59995c9",
    "outputId": "b6b193fc-3ab4-4e76-8096-49dfeae0a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue from sales when all products are discounted: $1654.57\n"
     ]
    }
   ],
   "source": [
    "total_revenue_all_treated = calculate_revenue(decisions_all_treat, price*discount)\n",
    "print(f\"Total revenue from sales when all products are discounted: ${total_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86abbc5",
   "metadata": {},
   "source": [
    "## All control scenario: all products remain the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34e8b6df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "34e8b6df",
    "outputId": "846dbf7a-920f-4c54-d79f-1b527ec86ff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([8, 3, 8,  ..., 3, 7, 8])\n",
      "Total Revenue from Sales: $6145.63\n"
     ]
    }
   ],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "\n",
    "decisions_all_control = utility_model_batched_nonparametric(\n",
    "    x_user=X_user,\n",
    "    X_product=X_product,\n",
    "    price=price,\n",
    "    user_randomization=user_randomization,\n",
    "    prod_randomization=prod_randomization,\n",
    "    dnn_model=model,  \n",
    "    gumbel_noise=gumbel_noise,\n",
    "    discount=discount, \n",
    "    \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_control)\n",
    "total_revenue_all_control = calculate_revenue(decisions_all_control, price)\n",
    "print(f\"Total Revenue from Sales: ${total_revenue_all_control:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3c0837",
   "metadata": {
    "id": "2e3c0837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (ALLTreated - ALLControl): $-4491.07\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = total_revenue_all_treated - total_revenue_all_control\n",
    "print(f\"Revenue Difference (ALLTreated - ALLControl): ${revenue_difference:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/total_revenue_all_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Sp6U7JOV1vEi",
   "metadata": {
    "id": "Sp6U7JOV1vEi"
   },
   "outputs": [],
   "source": [
    "true = revenue_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec3bca",
   "metadata": {
    "id": "d0ec3bca"
   },
   "source": [
    "## product randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cbbd718",
   "metadata": {
    "id": "4cbbd718"
   },
   "outputs": [],
   "source": [
    "def calculate_product_revenue(decisions, prices, prod_randomization):\n",
    "    revenue_treated = 0.0\n",
    "    revenue_control = 0.0\n",
    "\n",
    "    # Iterate over each user's decision\n",
    "    for user_index, decision in enumerate(decisions):\n",
    "        if decision != -1:  # If the user chose a product\n",
    "            product_price = prices[decision].item()  # Get the price of the chosen product\n",
    "\n",
    "            # Check if the product was in the treatment or control group\n",
    "            if prod_randomization[decision] == 1:\n",
    "                revenue_treated += product_price\n",
    "            else:\n",
    "                revenue_control += product_price\n",
    "\n",
    "    return revenue_treated, revenue_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "402ee3f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402ee3f8",
    "outputId": "17e270ca-c5e5-4e92-fd2f-2c675eb2fbf8"
   },
   "outputs": [],
   "source": [
    "utilities = torch.zeros(NUM_USER, NUM_Product)\n",
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1-treatment_percentage, treatment_percentage])\n",
    "# prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, ])\n",
    "decisions_product_randomization  = utility_model_batched_nonparametric(\n",
    "    x_user=X_user,\n",
    "    X_product=X_product,\n",
    "    price=price,\n",
    "    user_randomization=user_randomization,\n",
    "    prod_randomization=prod_randomization,\n",
    "    dnn_model=model,  \n",
    "    gumbel_noise=gumbel_noise,\n",
    "    discount=discount, \n",
    "    \n",
    "    batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "022d9af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022d9af1",
    "outputId": "9f338893-e1f9-4c6a-945c-408a2e5b6f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue from Treated Products: $296.55\n",
      "Revenue from Control Products: $5186.63\n",
      "Revenue Difference (Treated - Control) by naive DIM: $-9780.17\n"
     ]
    }
   ],
   "source": [
    "revenue_treated, revenue_control = calculate_product_revenue(decisions_product_randomization, price-price*(1-discount)*prod_randomization, prod_randomization)\n",
    "naive = revenue_treated/treatment_percentage - revenue_control/(1-treatment_percentage)\n",
    "print(f\"Revenue from Treated Products: ${revenue_treated:.2f}\")\n",
    "print(f\"Revenue from Control Products: ${revenue_control:.2f}\")\n",
    "print(f\"Revenue Difference (Treated - Control) by naive DIM: ${naive:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/revenue_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b9eb",
   "metadata": {},
   "source": [
    "## Prepare training and testing data given experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "wUgBFRaYHK7-",
   "metadata": {
    "id": "wUgBFRaYHK7-"
   },
   "outputs": [],
   "source": [
    "X_user_1, X_user_2, decision_1, decision_2 = train_test_split(\n",
    "X_user, decisions_product_randomization, test_size=1/2, random_state=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bd5975d-53cb-45c6-90e2-c36e313515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\n",
    "    'features': X_user_1,\n",
    "    'labels': decision_1\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    'features': X_user_2,\n",
    "    'labels': decision_2\n",
    "}\n",
    "\n",
    "# Flag to switch between training and test set\n",
    "use_train_set = False  # Set to False for the test set\n",
    "\n",
    "# Function to get the current active dataset\n",
    "def get_active_dataset(use_train):\n",
    "    return train_set if use_train else test_set\n",
    "def get_test_dataset(use_train):\n",
    "    return test_set if use_train else train_set\n",
    "# Retrieve the current dataset based on the flag\n",
    "current_dataset = get_active_dataset(use_train_set)\n",
    "X_user_train = current_dataset['features']\n",
    "decision_train = current_dataset['labels']\n",
    "X_user_test = get_test_dataset(use_train_set)['features']\n",
    "decision_test =  get_test_dataset(use_train_set)['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320887a",
   "metadata": {
    "id": "d320887a"
   },
   "source": [
    "# use simple MNL structural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b14cd096",
   "metadata": {
    "id": "b14cd096"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearMNLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearMNLModel, self).__init__()\n",
    "        # Initialize parameters for user and product features\n",
    "        self.beta_user = nn.Parameter(torch.randn(user_feature_dim))\n",
    "        self.beta_product = nn.Parameter(torch.randn(product_feature_dim))\n",
    "        self.beta_price = nn.Parameter(torch.tensor(-1.0)) \n",
    "\n",
    "    def forward(self, x_user, X_product, price, user_randomization, prod_randomization):\n",
    "        N, M = x_user.shape[0], X_product.shape[0]\n",
    "\n",
    "        # Expand user and product features to create a [N, M, F] shaped tensor for each\n",
    "        x_user_expanded = x_user.unsqueeze(1).expand(-1, M, -1).detach()\n",
    "        X_product_expanded = X_product.unsqueeze(0).expand(N, -1, -1).detach()\n",
    "\n",
    "\n",
    "        # Calculate linear utility from features\n",
    "        utility_user = torch.sum(x_user_expanded * self.beta_user, dim=2)\n",
    "        utility_product = torch.sum(X_product_expanded * self.beta_product, dim=2)\n",
    "\n",
    "        # Adjust prices based on randomization\n",
    "        adjusted_price = torch.where(\n",
    "             prod_randomization.unsqueeze(0),\n",
    "            price * discount,  \n",
    "            price\n",
    "        )\n",
    "\n",
    "        # Calculate utility from price, properly expanding its dimension\n",
    "        utility_price = adjusted_price * self.beta_price  # [M]\n",
    "        utility_price = utility_price.expand(N, M)  # [N, M]\n",
    "\n",
    "        # Total utility including features and price\n",
    "        total_utility = utility_user + utility_product + utility_price\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=total_utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities,total_utility), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d027617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearMNLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a74e297a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74e297a",
    "outputId": "d9b4de3b-2c2d-452e-b491-1b217ba2aaf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.484348773956299\n",
      "Epoch 1000, Loss: 1.7609167098999023\n",
      "Epoch 2000, Loss: 1.7473889589309692\n",
      "Epoch 3000, Loss: 1.7471544742584229\n",
      "Epoch 4000, Loss: 1.7470839023590088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    utilities = model(X_user_train, X_product, price, user_randomization, prod_randomization)\n",
    "    choice_probabilities = nn.functional.log_softmax(utilities, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train+1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da3dbeb5-c326-49f2-95d1-948ed3fbb443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.8286,  -2.3516,  -3.7096,  ...,  -2.2660,  -3.3253,  -4.1584],\n",
       "        [-13.4083,  -2.3516,  -3.7096,  ...,  -2.2660,  -3.3253,  -4.1584],\n",
       "        [-14.9134,  -2.3516,  -3.7096,  ...,  -2.2660,  -3.3253,  -4.1584],\n",
       "        ...,\n",
       "        [-17.3158,  -2.3516,  -3.7096,  ...,  -2.2660,  -3.3253,  -4.1584],\n",
       "        [ -9.7339,  -2.3517,  -3.7096,  ...,  -2.2660,  -3.3254,  -4.1585],\n",
       "        [-14.9100,  -2.3516,  -3.7096,  ...,  -2.2660,  -3.3253,  -4.1584]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f78d4b21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78d4b21",
    "outputId": "6078e9c0-ce87-4f8e-dd3d-88c3cfb2a616"
   },
   "outputs": [],
   "source": [
    "beta_price_est = model.beta_price.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ac97d98-d381-4a84-82c7-aa6821c3aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9523107\n"
     ]
    }
   ],
   "source": [
    "print(beta_price_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e4df3c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e4df3c1",
    "outputId": "709d1125-67b8-459d-d512-a95764796126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([4.1624, 5.1737, 4.1927, 2.4627, 3.8682], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1783523",
   "metadata": {
    "id": "a1783523"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17eb38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17eb38d3",
    "outputId": "df4ab9ed-52f4-4538-87a5-66932ad5f9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue: $3479.85\n",
      "Expected Revenue: $713.76\n"
     ]
    }
   ],
   "source": [
    "all_product_control = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "all_product_treated = np.random.choice([0,1], NUM_Product, p=[0, 1])\n",
    "all_product_control = torch.from_numpy(all_product_control).to(X_user_train.device).bool()\n",
    "all_product_treated = torch.from_numpy(all_product_treated).to(X_user_train.device).bool()\n",
    "\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_control)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "expected_revenue = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue.item():.2f}\")\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_treated)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)*discount\n",
    "expected_revenue_treated = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue_treated.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "P7Z_BF2C1kdj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7Z_BF2C1kdj",
    "outputId": "6998bb85-85b3-4cf8-da34-4ecded076114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (Treated - Control) by Linear MNL: $-5532.17\n",
      "Absolute Percentage Estimation Error of Linear MNL:  -23.18%\n"
     ]
    }
   ],
   "source": [
    "linear = (expected_revenue_treated-expected_revenue).cpu().detach().numpy()\n",
    "linear = linear*2\n",
    "print(f\"Revenue Difference (Treated - Control) by Linear MNL: ${linear:.2f}\")\n",
    "print(f\"Absolute Percentage Estimation Error of Linear MNL:  {100*np.abs(linear-revenue_difference)/revenue_difference:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab4aae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2899bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "134acfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39864359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(PDLModel, self).__init__()\n",
    "        # Combined feature dimension includes product features, price, and user features, as well as other products' features and prices\n",
    "        total_feature_dim = user_feature_dim + 2*product_feature_dim + 1  # +1 for price\n",
    "\n",
    "        # Single neural network to process the combined features\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1) \n",
    "        )\n",
    "            # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_user, x_product, x_other_products,prices):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                        x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        aggregated_other_features.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        prices.view(1, -1, 1).expand(N, -1, -1)),\n",
    "                                        dim=2)\n",
    "   \n",
    "\n",
    "        # Compute utility for each combined feature set\n",
    "        utilities = self.network(combined_features).squeeze(-1)\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=utilities.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utilities), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a160035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdlmodel = PDLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1ceb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.379567861557007, Validation Loss: 2.373211622238159\n",
      "Epoch 2, Training Loss: 2.3734123706817627, Validation Loss: 2.3674166202545166\n",
      "Epoch 3, Training Loss: 2.367734432220459, Validation Loss: 2.3608593940734863\n",
      "Epoch 4, Training Loss: 2.361192226409912, Validation Loss: 2.3526690006256104\n",
      "Epoch 5, Training Loss: 2.353121757507324, Validation Loss: 2.342992067337036\n",
      "Epoch 6, Training Loss: 2.3436036109924316, Validation Loss: 2.3311798572540283\n",
      "Epoch 7, Training Loss: 2.3320746421813965, Validation Loss: 2.316425323486328\n",
      "Epoch 8, Training Loss: 2.317845582962036, Validation Loss: 2.298616409301758\n",
      "Epoch 9, Training Loss: 2.300743818283081, Validation Loss: 2.2770962715148926\n",
      "Epoch 10, Training Loss: 2.2800607681274414, Validation Loss: 2.251371145248413\n",
      "Epoch 11, Training Loss: 2.255384683609009, Validation Loss: 2.2240612506866455\n",
      "Epoch 12, Training Loss: 2.2291648387908936, Validation Loss: 2.1943764686584473\n",
      "Epoch 13, Training Loss: 2.200651168823242, Validation Loss: 2.160393476486206\n",
      "Epoch 14, Training Loss: 2.168208360671997, Validation Loss: 2.1212568283081055\n",
      "Epoch 15, Training Loss: 2.1307342052459717, Validation Loss: 2.0811896324157715\n",
      "Epoch 16, Training Loss: 2.091933250427246, Validation Loss: 2.039520263671875\n",
      "Epoch 17, Training Loss: 2.05188250541687, Validation Loss: 1.994499921798706\n",
      "Epoch 18, Training Loss: 2.0097451210021973, Validation Loss: 1.9516987800598145\n",
      "Epoch 19, Training Loss: 1.9692306518554688, Validation Loss: 1.9135682582855225\n",
      "Epoch 20, Training Loss: 1.9334220886230469, Validation Loss: 1.8829514980316162\n",
      "Epoch 21, Training Loss: 1.9057608842849731, Validation Loss: 1.860867977142334\n",
      "Epoch 22, Training Loss: 1.8863565921783447, Validation Loss: 1.845207691192627\n",
      "Epoch 23, Training Loss: 1.8725816011428833, Validation Loss: 1.8281071186065674\n",
      "Epoch 24, Training Loss: 1.8542895317077637, Validation Loss: 1.817409634590149\n",
      "Epoch 25, Training Loss: 1.842925786972046, Validation Loss: 1.800815224647522\n",
      "Epoch 26, Training Loss: 1.8284783363342285, Validation Loss: 1.7882635593414307\n",
      "Epoch 27, Training Loss: 1.8184555768966675, Validation Loss: 1.775643229484558\n",
      "Epoch 28, Training Loss: 1.8050962686538696, Validation Loss: 1.772202968597412\n",
      "Epoch 29, Training Loss: 1.7994264364242554, Validation Loss: 1.7635763883590698\n",
      "Epoch 30, Training Loss: 1.7915282249450684, Validation Loss: 1.75735342502594\n",
      "Epoch 31, Training Loss: 1.7875910997390747, Validation Loss: 1.7499462366104126\n",
      "Epoch 32, Training Loss: 1.7798800468444824, Validation Loss: 1.745952844619751\n",
      "Epoch 33, Training Loss: 1.7738429307937622, Validation Loss: 1.7345101833343506\n",
      "Epoch 34, Training Loss: 1.7634968757629395, Validation Loss: 1.725980281829834\n",
      "Epoch 35, Training Loss: 1.7562576532363892, Validation Loss: 1.7151778936386108\n",
      "Epoch 36, Training Loss: 1.7448045015335083, Validation Loss: 1.7068182229995728\n",
      "Epoch 37, Training Loss: 1.7354289293289185, Validation Loss: 1.6953462362289429\n",
      "Epoch 38, Training Loss: 1.7238107919692993, Validation Loss: 1.6887319087982178\n",
      "Epoch 39, Training Loss: 1.717435598373413, Validation Loss: 1.6808561086654663\n",
      "Epoch 40, Training Loss: 1.7089645862579346, Validation Loss: 1.6747568845748901\n",
      "Epoch 41, Training Loss: 1.7025290727615356, Validation Loss: 1.6648352146148682\n",
      "Epoch 42, Training Loss: 1.6931402683258057, Validation Loss: 1.6558246612548828\n",
      "Epoch 43, Training Loss: 1.6842551231384277, Validation Loss: 1.644718050956726\n",
      "Epoch 44, Training Loss: 1.672824740409851, Validation Loss: 1.6357934474945068\n",
      "Epoch 45, Training Loss: 1.6646859645843506, Validation Loss: 1.6252145767211914\n",
      "Epoch 46, Training Loss: 1.6551852226257324, Validation Loss: 1.6156880855560303\n",
      "Epoch 47, Training Loss: 1.6462997198104858, Validation Loss: 1.608502745628357\n",
      "Epoch 48, Training Loss: 1.639215111732483, Validation Loss: 1.598362922668457\n",
      "Epoch 49, Training Loss: 1.629217267036438, Validation Loss: 1.591431975364685\n",
      "Epoch 50, Training Loss: 1.6216938495635986, Validation Loss: 1.583046793937683\n",
      "Epoch 51, Training Loss: 1.6123403310775757, Validation Loss: 1.5736844539642334\n",
      "Epoch 52, Training Loss: 1.602027416229248, Validation Loss: 1.5672940015792847\n",
      "Epoch 53, Training Loss: 1.5946227312088013, Validation Loss: 1.5592957735061646\n",
      "Epoch 54, Training Loss: 1.584978461265564, Validation Loss: 1.552662968635559\n",
      "Epoch 55, Training Loss: 1.5768400430679321, Validation Loss: 1.544577956199646\n",
      "Epoch 56, Training Loss: 1.5684882402420044, Validation Loss: 1.5358340740203857\n",
      "Epoch 57, Training Loss: 1.5588325262069702, Validation Loss: 1.529184341430664\n",
      "Epoch 58, Training Loss: 1.550869345664978, Validation Loss: 1.5201499462127686\n",
      "Epoch 59, Training Loss: 1.542186975479126, Validation Loss: 1.5124897956848145\n",
      "Epoch 60, Training Loss: 1.5335454940795898, Validation Loss: 1.5066635608673096\n",
      "Epoch 61, Training Loss: 1.5266143083572388, Validation Loss: 1.5010372400283813\n",
      "Epoch 62, Training Loss: 1.5209485292434692, Validation Loss: 1.4972307682037354\n",
      "Epoch 63, Training Loss: 1.5151567459106445, Validation Loss: 1.491755723953247\n",
      "Epoch 64, Training Loss: 1.509021282196045, Validation Loss: 1.4870741367340088\n",
      "Epoch 65, Training Loss: 1.5030180215835571, Validation Loss: 1.482921838760376\n",
      "Epoch 66, Training Loss: 1.4978560209274292, Validation Loss: 1.4806190729141235\n",
      "Epoch 67, Training Loss: 1.493237853050232, Validation Loss: 1.4759761095046997\n",
      "Epoch 68, Training Loss: 1.4881647825241089, Validation Loss: 1.4729409217834473\n",
      "Epoch 69, Training Loss: 1.483227252960205, Validation Loss: 1.470510721206665\n",
      "Epoch 70, Training Loss: 1.4796465635299683, Validation Loss: 1.4670357704162598\n",
      "Epoch 71, Training Loss: 1.4764657020568848, Validation Loss: 1.4651440382003784\n",
      "Epoch 72, Training Loss: 1.472591757774353, Validation Loss: 1.461527943611145\n",
      "Epoch 73, Training Loss: 1.4687730073928833, Validation Loss: 1.4577138423919678\n",
      "Epoch 74, Training Loss: 1.465665578842163, Validation Loss: 1.4573769569396973\n",
      "Epoch 75, Training Loss: 1.4628230333328247, Validation Loss: 1.4520562887191772\n",
      "Epoch 76, Training Loss: 1.4595707654953003, Validation Loss: 1.4508326053619385\n",
      "Epoch 77, Training Loss: 1.4556148052215576, Validation Loss: 1.448072075843811\n",
      "Epoch 78, Training Loss: 1.452204704284668, Validation Loss: 1.4458733797073364\n",
      "Epoch 79, Training Loss: 1.4494999647140503, Validation Loss: 1.4472429752349854\n",
      "Epoch 80, Training Loss: 1.4477298259735107, Validation Loss: 1.442578911781311\n",
      "Epoch 81, Training Loss: 1.448382019996643, Validation Loss: 1.450089931488037\n",
      "Epoch 82, Training Loss: 1.448243260383606, Validation Loss: 1.4377447366714478\n",
      "Epoch 83, Training Loss: 1.445779800415039, Validation Loss: 1.435646653175354\n",
      "Epoch 84, Training Loss: 1.4372307062149048, Validation Loss: 1.4342164993286133\n",
      "Epoch 85, Training Loss: 1.4357701539993286, Validation Loss: 1.4286960363388062\n",
      "Epoch 86, Training Loss: 1.4377285242080688, Validation Loss: 1.4272129535675049\n",
      "Epoch 87, Training Loss: 1.430863857269287, Validation Loss: 1.4252463579177856\n",
      "Epoch 88, Training Loss: 1.429300308227539, Validation Loss: 1.420198917388916\n",
      "Epoch 89, Training Loss: 1.4307194948196411, Validation Loss: 1.4188551902770996\n",
      "Epoch 90, Training Loss: 1.4247649908065796, Validation Loss: 1.4181607961654663\n",
      "Epoch 91, Training Loss: 1.4236538410186768, Validation Loss: 1.4130265712738037\n",
      "Epoch 92, Training Loss: 1.42391836643219, Validation Loss: 1.4116166830062866\n",
      "Epoch 93, Training Loss: 1.4189378023147583, Validation Loss: 1.413521409034729\n",
      "Epoch 94, Training Loss: 1.4190024137496948, Validation Loss: 1.4073292016983032\n",
      "Epoch 95, Training Loss: 1.4189351797103882, Validation Loss: 1.4073140621185303\n",
      "Epoch 96, Training Loss: 1.4144023656845093, Validation Loss: 1.4092127084732056\n",
      "Epoch 97, Training Loss: 1.414311408996582, Validation Loss: 1.4048274755477905\n",
      "Epoch 98, Training Loss: 1.4152921438217163, Validation Loss: 1.4061814546585083\n",
      "Epoch 99, Training Loss: 1.4108431339263916, Validation Loss: 1.4047719240188599\n",
      "Epoch 100, Training Loss: 1.4092756509780884, Validation Loss: 1.4024560451507568\n",
      "Epoch 101, Training Loss: 1.4101884365081787, Validation Loss: 1.4045493602752686\n",
      "Epoch 102, Training Loss: 1.4073989391326904, Validation Loss: 1.4015272855758667\n",
      "Epoch 103, Training Loss: 1.4052441120147705, Validation Loss: 1.4003732204437256\n",
      "Epoch 104, Training Loss: 1.405243992805481, Validation Loss: 1.404437780380249\n",
      "Epoch 105, Training Loss: 1.4046822786331177, Validation Loss: 1.3996416330337524\n",
      "Epoch 106, Training Loss: 1.4027035236358643, Validation Loss: 1.4001235961914062\n",
      "Epoch 107, Training Loss: 1.40105402469635, Validation Loss: 1.4003347158432007\n",
      "Epoch 108, Training Loss: 1.4002747535705566, Validation Loss: 1.3991508483886719\n",
      "Epoch 109, Training Loss: 1.3998483419418335, Validation Loss: 1.4021040201187134\n",
      "Epoch 110, Training Loss: 1.399306058883667, Validation Loss: 1.39884352684021\n",
      "Epoch 111, Training Loss: 1.3982182741165161, Validation Loss: 1.400801181793213\n",
      "Epoch 112, Training Loss: 1.3971996307373047, Validation Loss: 1.3980423212051392\n",
      "Epoch 113, Training Loss: 1.3961150646209717, Validation Loss: 1.3997985124588013\n",
      "Epoch 114, Training Loss: 1.3952233791351318, Validation Loss: 1.3972358703613281\n",
      "Epoch 115, Training Loss: 1.3946164846420288, Validation Loss: 1.402498722076416\n",
      "Epoch 116, Training Loss: 1.395369052886963, Validation Loss: 1.3978796005249023\n",
      "Epoch 117, Training Loss: 1.4001026153564453, Validation Loss: 1.4186116456985474\n",
      "Epoch 118, Training Loss: 1.4057888984680176, Validation Loss: 1.4044758081436157\n",
      "Epoch 119, Training Loss: 1.4099208116531372, Validation Loss: 1.4024664163589478\n",
      "Epoch 120, Training Loss: 1.393983006477356, Validation Loss: 1.3998523950576782\n",
      "Epoch 121, Training Loss: 1.3919644355773926, Validation Loss: 1.3978458642959595\n",
      "Epoch 122, Training Loss: 1.4021944999694824, Validation Loss: 1.4050569534301758\n",
      "Epoch 123, Training Loss: 1.3944756984710693, Validation Loss: 1.39168119430542\n",
      "Epoch 124, Training Loss: 1.3874441385269165, Validation Loss: 1.390595555305481\n",
      "Epoch 125, Training Loss: 1.3924460411071777, Validation Loss: 1.4028462171554565\n",
      "Epoch 126, Training Loss: 1.3918282985687256, Validation Loss: 1.3875164985656738\n",
      "Epoch 127, Training Loss: 1.3851197957992554, Validation Loss: 1.3863639831542969\n",
      "Epoch 128, Training Loss: 1.386994481086731, Validation Loss: 1.4000705480575562\n",
      "Epoch 129, Training Loss: 1.389206886291504, Validation Loss: 1.3841166496276855\n",
      "Epoch 130, Training Loss: 1.3835456371307373, Validation Loss: 1.3834346532821655\n",
      "Epoch 131, Training Loss: 1.3822791576385498, Validation Loss: 1.393784999847412\n",
      "Epoch 132, Training Loss: 1.3845109939575195, Validation Loss: 1.3817708492279053\n",
      "Epoch 133, Training Loss: 1.3815912008285522, Validation Loss: 1.3821403980255127\n",
      "Epoch 134, Training Loss: 1.3792587518692017, Validation Loss: 1.3907934427261353\n",
      "Epoch 135, Training Loss: 1.381481409072876, Validation Loss: 1.380052924156189\n",
      "Epoch 136, Training Loss: 1.3811630010604858, Validation Loss: 1.3834311962127686\n",
      "Epoch 137, Training Loss: 1.37716543674469, Validation Loss: 1.3870655298233032\n",
      "Epoch 138, Training Loss: 1.3780832290649414, Validation Loss: 1.378006935119629\n",
      "Epoch 139, Training Loss: 1.3798142671585083, Validation Loss: 1.3845056295394897\n",
      "Epoch 140, Training Loss: 1.3760906457901, Validation Loss: 1.382106065750122\n",
      "Epoch 141, Training Loss: 1.374667763710022, Validation Loss: 1.3760520219802856\n",
      "Epoch 142, Training Loss: 1.376563310623169, Validation Loss: 1.3854275941848755\n",
      "Epoch 143, Training Loss: 1.375195026397705, Validation Loss: 1.3775098323822021\n",
      "Epoch 144, Training Loss: 1.372640609741211, Validation Loss: 1.376159906387329\n",
      "Epoch 145, Training Loss: 1.3732333183288574, Validation Loss: 1.3844764232635498\n",
      "Epoch 146, Training Loss: 1.373557686805725, Validation Loss: 1.3747450113296509\n",
      "Epoch 147, Training Loss: 1.372570514678955, Validation Loss: 1.3770208358764648\n",
      "Epoch 148, Training Loss: 1.3701621294021606, Validation Loss: 1.3808674812316895\n",
      "Epoch 149, Training Loss: 1.370890736579895, Validation Loss: 1.3727737665176392\n",
      "Epoch 150, Training Loss: 1.3706822395324707, Validation Loss: 1.3799136877059937\n",
      "Epoch 151, Training Loss: 1.370341420173645, Validation Loss: 1.373927116394043\n",
      "Epoch 152, Training Loss: 1.3682535886764526, Validation Loss: 1.373630404472351\n",
      "Epoch 153, Training Loss: 1.367854118347168, Validation Loss: 1.3785479068756104\n",
      "Epoch 154, Training Loss: 1.3682318925857544, Validation Loss: 1.3704372644424438\n",
      "Epoch 155, Training Loss: 1.3684368133544922, Validation Loss: 1.3798437118530273\n",
      "Epoch 156, Training Loss: 1.368151068687439, Validation Loss: 1.3698482513427734\n",
      "Epoch 157, Training Loss: 1.3670204877853394, Validation Loss: 1.3739097118377686\n",
      "Epoch 158, Training Loss: 1.3655154705047607, Validation Loss: 1.37263023853302\n",
      "Epoch 159, Training Loss: 1.3647408485412598, Validation Loss: 1.3693941831588745\n",
      "Epoch 160, Training Loss: 1.3650346994400024, Validation Loss: 1.377741813659668\n",
      "Epoch 161, Training Loss: 1.3657610416412354, Validation Loss: 1.3675529956817627\n",
      "Epoch 162, Training Loss: 1.3666322231292725, Validation Loss: 1.3778479099273682\n",
      "Epoch 163, Training Loss: 1.3651832342147827, Validation Loss: 1.3668370246887207\n",
      "Epoch 164, Training Loss: 1.364145278930664, Validation Loss: 1.3733618259429932\n",
      "Epoch 165, Training Loss: 1.3624324798583984, Validation Loss: 1.3671907186508179\n",
      "Epoch 166, Training Loss: 1.3612614870071411, Validation Loss: 1.368900179862976\n",
      "Epoch 167, Training Loss: 1.3604042530059814, Validation Loss: 1.3686211109161377\n",
      "Epoch 168, Training Loss: 1.3599052429199219, Validation Loss: 1.3658900260925293\n",
      "Epoch 169, Training Loss: 1.3594924211502075, Validation Loss: 1.3696987628936768\n",
      "Epoch 170, Training Loss: 1.3592902421951294, Validation Loss: 1.363525152206421\n",
      "Epoch 171, Training Loss: 1.3593062162399292, Validation Loss: 1.371178388595581\n",
      "Epoch 172, Training Loss: 1.3592076301574707, Validation Loss: 1.3618882894515991\n",
      "Epoch 173, Training Loss: 1.3599607944488525, Validation Loss: 1.3734945058822632\n",
      "Epoch 174, Training Loss: 1.3598911762237549, Validation Loss: 1.361171007156372\n",
      "Epoch 175, Training Loss: 1.361301302909851, Validation Loss: 1.3754684925079346\n",
      "Epoch 176, Training Loss: 1.3609338998794556, Validation Loss: 1.3606170415878296\n",
      "Epoch 177, Training Loss: 1.3618934154510498, Validation Loss: 1.3728104829788208\n",
      "Epoch 178, Training Loss: 1.3590660095214844, Validation Loss: 1.358302116394043\n",
      "Epoch 179, Training Loss: 1.355855941772461, Validation Loss: 1.3608125448226929\n",
      "Epoch 180, Training Loss: 1.3537845611572266, Validation Loss: 1.3652490377426147\n",
      "Epoch 181, Training Loss: 1.3545781373977661, Validation Loss: 1.356982946395874\n",
      "Epoch 182, Training Loss: 1.3565922975540161, Validation Loss: 1.3709520101547241\n",
      "Epoch 183, Training Loss: 1.356896162033081, Validation Loss: 1.3559890985488892\n",
      "Epoch 184, Training Loss: 1.3552374839782715, Validation Loss: 1.3615286350250244\n",
      "Epoch 185, Training Loss: 1.3520983457565308, Validation Loss: 1.358811855316162\n",
      "Epoch 186, Training Loss: 1.3510115146636963, Validation Loss: 1.3546397686004639\n",
      "Epoch 187, Training Loss: 1.35163414478302, Validation Loss: 1.3627020120620728\n",
      "Epoch 188, Training Loss: 1.3515788316726685, Validation Loss: 1.3536585569381714\n",
      "Epoch 189, Training Loss: 1.3507083654403687, Validation Loss: 1.358759880065918\n",
      "Epoch 190, Training Loss: 1.3492588996887207, Validation Loss: 1.354369878768921\n",
      "Epoch 191, Training Loss: 1.3481312990188599, Validation Loss: 1.3533554077148438\n",
      "Epoch 192, Training Loss: 1.3475518226623535, Validation Loss: 1.3578816652297974\n",
      "Epoch 193, Training Loss: 1.3476946353912354, Validation Loss: 1.3507441282272339\n",
      "Epoch 194, Training Loss: 1.3477513790130615, Validation Loss: 1.357585072517395\n",
      "Epoch 195, Training Loss: 1.3466427326202393, Validation Loss: 1.349485158920288\n",
      "Epoch 196, Training Loss: 1.3453049659729004, Validation Loss: 1.3521541357040405\n",
      "Epoch 197, Training Loss: 1.344230055809021, Validation Loss: 1.3505927324295044\n",
      "Epoch 198, Training Loss: 1.3435269594192505, Validation Loss: 1.347158670425415\n",
      "Epoch 199, Training Loss: 1.3432230949401855, Validation Loss: 1.3522361516952515\n",
      "Epoch 200, Training Loss: 1.3431706428527832, Validation Loss: 1.344529628753662\n",
      "Epoch 201, Training Loss: 1.34247887134552, Validation Loss: 1.3487415313720703\n",
      "Epoch 202, Training Loss: 1.3413302898406982, Validation Loss: 1.3447662591934204\n",
      "Epoch 203, Training Loss: 1.3402414321899414, Validation Loss: 1.344055414199829\n",
      "Epoch 204, Training Loss: 1.33966064453125, Validation Loss: 1.347527265548706\n",
      "Epoch 205, Training Loss: 1.3394603729248047, Validation Loss: 1.340570092201233\n",
      "Epoch 206, Training Loss: 1.3393079042434692, Validation Loss: 1.3483366966247559\n",
      "Epoch 207, Training Loss: 1.3388084173202515, Validation Loss: 1.3396687507629395\n",
      "Epoch 208, Training Loss: 1.337902545928955, Validation Loss: 1.3451728820800781\n",
      "Epoch 209, Training Loss: 1.336804986000061, Validation Loss: 1.3396797180175781\n",
      "Epoch 210, Training Loss: 1.3360753059387207, Validation Loss: 1.3453154563903809\n",
      "Epoch 211, Training Loss: 1.335594892501831, Validation Loss: 1.3384748697280884\n",
      "Epoch 212, Training Loss: 1.3353266716003418, Validation Loss: 1.3479138612747192\n",
      "Epoch 213, Training Loss: 1.335389494895935, Validation Loss: 1.337759017944336\n",
      "Epoch 214, Training Loss: 1.3351408243179321, Validation Loss: 1.348132610321045\n",
      "Epoch 215, Training Loss: 1.3346645832061768, Validation Loss: 1.337134599685669\n",
      "Epoch 216, Training Loss: 1.3335398435592651, Validation Loss: 1.3440215587615967\n",
      "Epoch 217, Training Loss: 1.332351803779602, Validation Loss: 1.3390390872955322\n",
      "Epoch 218, Training Loss: 1.3313392400741577, Validation Loss: 1.3394713401794434\n",
      "Epoch 219, Training Loss: 1.330653429031372, Validation Loss: 1.3408498764038086\n",
      "Epoch 220, Training Loss: 1.3302522897720337, Validation Loss: 1.3375396728515625\n",
      "Epoch 221, Training Loss: 1.3300528526306152, Validation Loss: 1.343974232673645\n",
      "Epoch 222, Training Loss: 1.32999849319458, Validation Loss: 1.3343206644058228\n",
      "Epoch 223, Training Loss: 1.3304119110107422, Validation Loss: 1.3447412252426147\n",
      "Epoch 224, Training Loss: 1.329511284828186, Validation Loss: 1.3342503309249878\n",
      "Epoch 225, Training Loss: 1.3290479183197021, Validation Loss: 1.3425970077514648\n",
      "Epoch 226, Training Loss: 1.3281176090240479, Validation Loss: 1.3343040943145752\n",
      "Epoch 227, Training Loss: 1.3271613121032715, Validation Loss: 1.3381681442260742\n",
      "Epoch 228, Training Loss: 1.3262819051742554, Validation Loss: 1.3370747566223145\n",
      "Epoch 229, Training Loss: 1.3257627487182617, Validation Loss: 1.3336641788482666\n",
      "Epoch 230, Training Loss: 1.3255923986434937, Validation Loss: 1.3409948348999023\n",
      "Epoch 231, Training Loss: 1.325761079788208, Validation Loss: 1.330849051475525\n",
      "Epoch 232, Training Loss: 1.3265808820724487, Validation Loss: 1.3437392711639404\n",
      "Epoch 233, Training Loss: 1.326258659362793, Validation Loss: 1.3292057514190674\n",
      "Epoch 234, Training Loss: 1.3264394998550415, Validation Loss: 1.3424632549285889\n",
      "Epoch 235, Training Loss: 1.3255187273025513, Validation Loss: 1.3283637762069702\n",
      "Epoch 236, Training Loss: 1.3248728513717651, Validation Loss: 1.3367975950241089\n",
      "Epoch 237, Training Loss: 1.3231582641601562, Validation Loss: 1.3301851749420166\n",
      "Epoch 238, Training Loss: 1.3219637870788574, Validation Loss: 1.3297498226165771\n",
      "Epoch 239, Training Loss: 1.3215210437774658, Validation Loss: 1.3343169689178467\n",
      "Epoch 240, Training Loss: 1.3216805458068848, Validation Loss: 1.3266258239746094\n",
      "Epoch 241, Training Loss: 1.3216874599456787, Validation Loss: 1.3350082635879517\n",
      "Epoch 242, Training Loss: 1.3213788270950317, Validation Loss: 1.325589656829834\n",
      "Epoch 243, Training Loss: 1.3206275701522827, Validation Loss: 1.3308582305908203\n",
      "Epoch 244, Training Loss: 1.3197318315505981, Validation Loss: 1.3263741731643677\n",
      "Epoch 245, Training Loss: 1.3191189765930176, Validation Loss: 1.326850414276123\n",
      "Epoch 246, Training Loss: 1.3186825513839722, Validation Loss: 1.328553557395935\n",
      "Epoch 247, Training Loss: 1.318497896194458, Validation Loss: 1.324220061302185\n",
      "Epoch 248, Training Loss: 1.3183144330978394, Validation Loss: 1.3297916650772095\n",
      "Epoch 249, Training Loss: 1.318174958229065, Validation Loss: 1.3228201866149902\n",
      "Epoch 250, Training Loss: 1.3179104328155518, Validation Loss: 1.3286360502243042\n",
      "Epoch 251, Training Loss: 1.3175137042999268, Validation Loss: 1.322611927986145\n",
      "Epoch 252, Training Loss: 1.3169316053390503, Validation Loss: 1.3259919881820679\n",
      "Epoch 253, Training Loss: 1.3165885210037231, Validation Loss: 1.3218579292297363\n",
      "Epoch 254, Training Loss: 1.3165125846862793, Validation Loss: 1.327430248260498\n",
      "Epoch 255, Training Loss: 1.3164557218551636, Validation Loss: 1.3201254606246948\n",
      "Epoch 256, Training Loss: 1.316222906112671, Validation Loss: 1.32588791847229\n",
      "Epoch 257, Training Loss: 1.3156306743621826, Validation Loss: 1.3205344676971436\n",
      "Epoch 258, Training Loss: 1.3153163194656372, Validation Loss: 1.3245420455932617\n",
      "Epoch 259, Training Loss: 1.3152172565460205, Validation Loss: 1.3228120803833008\n",
      "Epoch 260, Training Loss: 1.314530849456787, Validation Loss: 1.3197898864746094\n",
      "Epoch 261, Training Loss: 1.315300703048706, Validation Loss: 1.323917031288147\n",
      "Epoch 262, Training Loss: 1.3153446912765503, Validation Loss: 1.3234434127807617\n",
      "Epoch 263, Training Loss: 1.3156918287277222, Validation Loss: 1.31915283203125\n",
      "Epoch 264, Training Loss: 1.3137563467025757, Validation Loss: 1.3197181224822998\n",
      "Epoch 265, Training Loss: 1.3156085014343262, Validation Loss: 1.320214033126831\n",
      "Epoch 266, Training Loss: 1.3144187927246094, Validation Loss: 1.325791358947754\n",
      "Epoch 267, Training Loss: 1.3165239095687866, Validation Loss: 1.3166433572769165\n",
      "Epoch 268, Training Loss: 1.3135826587677002, Validation Loss: 1.3210777044296265\n",
      "Epoch 269, Training Loss: 1.3151488304138184, Validation Loss: 1.3182120323181152\n",
      "Epoch 270, Training Loss: 1.3119689226150513, Validation Loss: 1.3177361488342285\n",
      "Epoch 271, Training Loss: 1.3133596181869507, Validation Loss: 1.3214442729949951\n",
      "Epoch 272, Training Loss: 1.3126362562179565, Validation Loss: 1.3138611316680908\n",
      "Epoch 273, Training Loss: 1.3109352588653564, Validation Loss: 1.3152505159378052\n",
      "Epoch 274, Training Loss: 1.3115955591201782, Validation Loss: 1.318373680114746\n",
      "Epoch 275, Training Loss: 1.3102141618728638, Validation Loss: 1.3142244815826416\n",
      "Epoch 276, Training Loss: 1.3105430603027344, Validation Loss: 1.3171628713607788\n",
      "Epoch 277, Training Loss: 1.3092639446258545, Validation Loss: 1.3167409896850586\n",
      "Epoch 278, Training Loss: 1.3083745241165161, Validation Loss: 1.3122681379318237\n",
      "Epoch 279, Training Loss: 1.3090189695358276, Validation Loss: 1.3191577196121216\n",
      "Epoch 280, Training Loss: 1.3081916570663452, Validation Loss: 1.310159683227539\n",
      "Epoch 281, Training Loss: 1.307871699333191, Validation Loss: 1.3156590461730957\n",
      "Epoch 282, Training Loss: 1.3060688972473145, Validation Loss: 1.3108505010604858\n",
      "Epoch 283, Training Loss: 1.3046460151672363, Validation Loss: 1.3089916706085205\n",
      "Epoch 284, Training Loss: 1.3042954206466675, Validation Loss: 1.3156540393829346\n",
      "Epoch 285, Training Loss: 1.3044915199279785, Validation Loss: 1.3056477308273315\n",
      "Epoch 286, Training Loss: 1.3048591613769531, Validation Loss: 1.3159443140029907\n",
      "Epoch 287, Training Loss: 1.3034980297088623, Validation Loss: 1.3064213991165161\n",
      "Epoch 288, Training Loss: 1.301690936088562, Validation Loss: 1.306365728378296\n",
      "Epoch 289, Training Loss: 1.3009923696517944, Validation Loss: 1.3106633424758911\n",
      "Epoch 290, Training Loss: 1.3006236553192139, Validation Loss: 1.304528832435608\n",
      "Epoch 291, Training Loss: 1.3007582426071167, Validation Loss: 1.311244010925293\n",
      "Epoch 292, Training Loss: 1.3001726865768433, Validation Loss: 1.3036901950836182\n",
      "Epoch 293, Training Loss: 1.2993361949920654, Validation Loss: 1.3075799942016602\n",
      "Epoch 294, Training Loss: 1.2983464002609253, Validation Loss: 1.3045703172683716\n",
      "Epoch 295, Training Loss: 1.2975101470947266, Validation Loss: 1.306771159172058\n",
      "Epoch 296, Training Loss: 1.2968554496765137, Validation Loss: 1.3075840473175049\n",
      "Epoch 297, Training Loss: 1.2963837385177612, Validation Loss: 1.3052852153778076\n",
      "Epoch 298, Training Loss: 1.2960697412490845, Validation Loss: 1.3108738660812378\n",
      "Epoch 299, Training Loss: 1.2958284616470337, Validation Loss: 1.304496169090271\n",
      "Epoch 300, Training Loss: 1.2955378293991089, Validation Loss: 1.3108878135681152\n",
      "Epoch 301, Training Loss: 1.295180082321167, Validation Loss: 1.302377462387085\n",
      "Epoch 302, Training Loss: 1.2955045700073242, Validation Loss: 1.3159528970718384\n",
      "Epoch 303, Training Loss: 1.2961028814315796, Validation Loss: 1.300851583480835\n",
      "Epoch 304, Training Loss: 1.296775221824646, Validation Loss: 1.3222746849060059\n",
      "Epoch 305, Training Loss: 1.2978971004486084, Validation Loss: 1.3006936311721802\n",
      "Epoch 306, Training Loss: 1.299532175064087, Validation Loss: 1.325743556022644\n",
      "Epoch 307, Training Loss: 1.2981655597686768, Validation Loss: 1.300586462020874\n",
      "Epoch 308, Training Loss: 1.2967897653579712, Validation Loss: 1.3150025606155396\n",
      "Epoch 309, Training Loss: 1.2933155298233032, Validation Loss: 1.3057548999786377\n",
      "Epoch 310, Training Loss: 1.290868878364563, Validation Loss: 1.3028937578201294\n",
      "Epoch 311, Training Loss: 1.291107177734375, Validation Loss: 1.3170642852783203\n",
      "Epoch 312, Training Loss: 1.293329119682312, Validation Loss: 1.2998714447021484\n",
      "Epoch 313, Training Loss: 1.2937726974487305, Validation Loss: 1.3176910877227783\n",
      "Epoch 314, Training Loss: 1.2927656173706055, Validation Loss: 1.300406575202942\n",
      "Epoch 315, Training Loss: 1.2895922660827637, Validation Loss: 1.304208755493164\n",
      "Epoch 316, Training Loss: 1.2883012294769287, Validation Loss: 1.3098231554031372\n",
      "Epoch 317, Training Loss: 1.288827896118164, Validation Loss: 1.299122929573059\n",
      "Epoch 318, Training Loss: 1.2904045581817627, Validation Loss: 1.3192298412322998\n",
      "Epoch 319, Training Loss: 1.2921910285949707, Validation Loss: 1.2988201379776\n",
      "Epoch 320, Training Loss: 1.2936427593231201, Validation Loss: 1.3188767433166504\n",
      "Epoch 321, Training Loss: 1.291650414466858, Validation Loss: 1.2978203296661377\n",
      "Epoch 322, Training Loss: 1.289447546005249, Validation Loss: 1.3075015544891357\n",
      "Epoch 323, Training Loss: 1.286458134651184, Validation Loss: 1.306342363357544\n",
      "Epoch 324, Training Loss: 1.2857556343078613, Validation Loss: 1.2997024059295654\n",
      "Epoch 325, Training Loss: 1.2871737480163574, Validation Loss: 1.31802499294281\n",
      "Epoch 326, Training Loss: 1.2897673845291138, Validation Loss: 1.3001384735107422\n",
      "Epoch 327, Training Loss: 1.2912451028823853, Validation Loss: 1.3170909881591797\n",
      "Epoch 328, Training Loss: 1.2891769409179688, Validation Loss: 1.2976890802383423\n",
      "Epoch 329, Training Loss: 1.287596344947815, Validation Loss: 1.3101556301116943\n",
      "Epoch 330, Training Loss: 1.285298466682434, Validation Loss: 1.3016440868377686\n",
      "Epoch 331, Training Loss: 1.2834070920944214, Validation Loss: 1.2997390031814575\n",
      "Epoch 332, Training Loss: 1.2839877605438232, Validation Loss: 1.3101741075515747\n",
      "Epoch 333, Training Loss: 1.2843804359436035, Validation Loss: 1.2997466325759888\n",
      "Epoch 334, Training Loss: 1.2863258123397827, Validation Loss: 1.3128817081451416\n",
      "Epoch 335, Training Loss: 1.2853933572769165, Validation Loss: 1.2963601350784302\n",
      "Epoch 336, Training Loss: 1.2875115871429443, Validation Loss: 1.3137551546096802\n",
      "Epoch 337, Training Loss: 1.285658836364746, Validation Loss: 1.30088210105896\n",
      "Epoch 338, Training Loss: 1.2849171161651611, Validation Loss: 1.3049769401550293\n",
      "Epoch 339, Training Loss: 1.2816877365112305, Validation Loss: 1.3021645545959473\n",
      "Epoch 340, Training Loss: 1.2824912071228027, Validation Loss: 1.2984507083892822\n",
      "Epoch 341, Training Loss: 1.2819771766662598, Validation Loss: 1.3127937316894531\n",
      "Epoch 342, Training Loss: 1.283873438835144, Validation Loss: 1.2989146709442139\n",
      "Epoch 343, Training Loss: 1.2856377363204956, Validation Loss: 1.313971996307373\n",
      "Epoch 344, Training Loss: 1.2848198413848877, Validation Loss: 1.2966240644454956\n",
      "Epoch 345, Training Loss: 1.2834497690200806, Validation Loss: 1.3088823556900024\n",
      "Epoch 346, Training Loss: 1.2811636924743652, Validation Loss: 1.2992303371429443\n",
      "Epoch 347, Training Loss: 1.2804862260818481, Validation Loss: 1.3037641048431396\n",
      "Epoch 348, Training Loss: 1.2795428037643433, Validation Loss: 1.2994582653045654\n",
      "Epoch 349, Training Loss: 1.2790555953979492, Validation Loss: 1.301891565322876\n",
      "Epoch 350, Training Loss: 1.2785563468933105, Validation Loss: 1.3015071153640747\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(pdlmodel.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pdlmodel.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = pdlmodel(user_features, product_features, all_x_other_products,prices)\n",
    "    choice_probabilities = F.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]),decision_train1+1])\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    pdlmodel.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = pdlmodel(X_user_val,  product_features, all_x_other_products,prices)\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        # torch.save(pdlmodel.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b5374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c915a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3357.73\n",
      "Expected Revenue all treated: $680.15\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17cb1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = (expected_revenue_all_treated-expected_revenue_all_control)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a4df632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of PDL:  -19.24%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(pdl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c900cf",
   "metadata": {
    "id": "63c900cf"
   },
   "source": [
    "# use dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecd9bca0-6191-4297-8452-7f8af22d5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityEstimator(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(UtilityEstimator, self).__init__()\n",
    "        \n",
    "        # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "        self.theta0 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        # Output layer for Theta1 (takes xi, zj, z-j)\n",
    "        self.theta1 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_user, x_product, x_other_products,price):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "    \n",
    "\n",
    "        # Combine features for Theta0\n",
    "        \n",
    "        combined_features_theta =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                               x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                               aggregated_other_features.unsqueeze(0).expand(N, -1, -1)),\n",
    "                                                 dim=2)\n",
    "        theta0_output = self.theta0(combined_features_theta).squeeze(-1)\n",
    "        theta1_output = self.theta1(combined_features_theta).squeeze(-1)\n",
    "        \n",
    "        price = price.unsqueeze(-1)  \n",
    "        utility = theta0_output + theta1_output * price.squeeze(-1)\n",
    "\n",
    "        # Include the outside option (utility = 0)\n",
    "        zero_utilities = torch.zeros(x_user.shape[0], 1, device=utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utility), dim=1)\n",
    "        \n",
    "        return utilities_with_outside,theta0_output,theta1_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf77eeb6",
   "metadata": {
    "id": "cf77eeb6"
   },
   "outputs": [],
   "source": [
    "dml_model = UtilityEstimator(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "QT_wrrh3rIws",
   "metadata": {
    "id": "QT_wrrh3rIws"
   },
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b522ed-0c36-4199-a309-74f71aece365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0ff32e5-5c64-49ad-bbf0-f6aa2a53d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b386e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b386e12",
    "outputId": "aab71afc-5217-4c73-eba6-26cb31bed335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.397836923599243, Validation Loss: 2.379599094390869\n",
      "Epoch 2, Training Loss: 2.3763084411621094, Validation Loss: 2.3584401607513428\n",
      "Epoch 3, Training Loss: 2.3553483486175537, Validation Loss: 2.3401260375976562\n",
      "Epoch 4, Training Loss: 2.337233781814575, Validation Loss: 2.3230464458465576\n",
      "Epoch 5, Training Loss: 2.320643186569214, Validation Loss: 2.3053343296051025\n",
      "Epoch 6, Training Loss: 2.303337335586548, Validation Loss: 2.285661220550537\n",
      "Epoch 7, Training Loss: 2.2839224338531494, Validation Loss: 2.2632639408111572\n",
      "Epoch 8, Training Loss: 2.262124538421631, Validation Loss: 2.2373857498168945\n",
      "Epoch 9, Training Loss: 2.237248659133911, Validation Loss: 2.2089905738830566\n",
      "Epoch 10, Training Loss: 2.2103540897369385, Validation Loss: 2.1780762672424316\n",
      "Epoch 11, Training Loss: 2.1808104515075684, Validation Loss: 2.143239736557007\n",
      "Epoch 12, Training Loss: 2.147589683532715, Validation Loss: 2.1067798137664795\n",
      "Epoch 13, Training Loss: 2.1134324073791504, Validation Loss: 2.069239854812622\n",
      "Epoch 14, Training Loss: 2.078059434890747, Validation Loss: 2.0324175357818604\n",
      "Epoch 15, Training Loss: 2.043544292449951, Validation Loss: 1.9967596530914307\n",
      "Epoch 16, Training Loss: 2.0093393325805664, Validation Loss: 1.9629849195480347\n",
      "Epoch 17, Training Loss: 1.976806402206421, Validation Loss: 1.9311063289642334\n",
      "Epoch 18, Training Loss: 1.9472700357437134, Validation Loss: 1.9037830829620361\n",
      "Epoch 19, Training Loss: 1.9227460622787476, Validation Loss: 1.8763470649719238\n",
      "Epoch 20, Training Loss: 1.8982877731323242, Validation Loss: 1.8554937839508057\n",
      "Epoch 21, Training Loss: 1.8797359466552734, Validation Loss: 1.8375813961029053\n",
      "Epoch 22, Training Loss: 1.8641725778579712, Validation Loss: 1.823256015777588\n",
      "Epoch 23, Training Loss: 1.851804494857788, Validation Loss: 1.81160306930542\n",
      "Epoch 24, Training Loss: 1.8418926000595093, Validation Loss: 1.801145315170288\n",
      "Epoch 25, Training Loss: 1.8326810598373413, Validation Loss: 1.792762041091919\n",
      "Epoch 26, Training Loss: 1.8254709243774414, Validation Loss: 1.7824636697769165\n",
      "Epoch 27, Training Loss: 1.8170137405395508, Validation Loss: 1.7726211547851562\n",
      "Epoch 28, Training Loss: 1.808789610862732, Validation Loss: 1.7635753154754639\n",
      "Epoch 29, Training Loss: 1.8006404638290405, Validation Loss: 1.7542469501495361\n",
      "Epoch 30, Training Loss: 1.7914334535598755, Validation Loss: 1.7445945739746094\n",
      "Epoch 31, Training Loss: 1.7817020416259766, Validation Loss: 1.7322640419006348\n",
      "Epoch 32, Training Loss: 1.7688721418380737, Validation Loss: 1.7210835218429565\n",
      "Epoch 33, Training Loss: 1.7566804885864258, Validation Loss: 1.7110774517059326\n",
      "Epoch 34, Training Loss: 1.745384693145752, Validation Loss: 1.7019908428192139\n",
      "Epoch 35, Training Loss: 1.7352906465530396, Validation Loss: 1.6935797929763794\n",
      "Epoch 36, Training Loss: 1.726380467414856, Validation Loss: 1.686617374420166\n",
      "Epoch 37, Training Loss: 1.718520164489746, Validation Loss: 1.6815294027328491\n",
      "Epoch 38, Training Loss: 1.712639570236206, Validation Loss: 1.6774721145629883\n",
      "Epoch 39, Training Loss: 1.7082337141036987, Validation Loss: 1.6761997938156128\n",
      "Epoch 40, Training Loss: 1.706681251525879, Validation Loss: 1.6722187995910645\n",
      "Epoch 41, Training Loss: 1.7024201154708862, Validation Loss: 1.6683671474456787\n",
      "Epoch 42, Training Loss: 1.6981480121612549, Validation Loss: 1.6599045991897583\n",
      "Epoch 43, Training Loss: 1.6902008056640625, Validation Loss: 1.652762532234192\n",
      "Epoch 44, Training Loss: 1.6834661960601807, Validation Loss: 1.644773244857788\n",
      "Epoch 45, Training Loss: 1.675675392150879, Validation Loss: 1.6389756202697754\n",
      "Epoch 46, Training Loss: 1.6698403358459473, Validation Loss: 1.6326963901519775\n",
      "Epoch 47, Training Loss: 1.6637566089630127, Validation Loss: 1.6284267902374268\n",
      "Epoch 48, Training Loss: 1.6595412492752075, Validation Loss: 1.623837947845459\n",
      "Epoch 49, Training Loss: 1.654969334602356, Validation Loss: 1.6191890239715576\n",
      "Epoch 50, Training Loss: 1.6502257585525513, Validation Loss: 1.6152806282043457\n",
      "Epoch 51, Training Loss: 1.6458595991134644, Validation Loss: 1.6095174551010132\n",
      "Epoch 52, Training Loss: 1.6404953002929688, Validation Loss: 1.60462486743927\n",
      "Epoch 53, Training Loss: 1.6357660293579102, Validation Loss: 1.5998718738555908\n",
      "Epoch 54, Training Loss: 1.6313581466674805, Validation Loss: 1.5946565866470337\n",
      "Epoch 55, Training Loss: 1.6266844272613525, Validation Loss: 1.5898497104644775\n",
      "Epoch 56, Training Loss: 1.6226202249526978, Validation Loss: 1.5860891342163086\n",
      "Epoch 57, Training Loss: 1.6186422109603882, Validation Loss: 1.5809719562530518\n",
      "Epoch 58, Training Loss: 1.613879680633545, Validation Loss: 1.5762319564819336\n",
      "Epoch 59, Training Loss: 1.6088923215866089, Validation Loss: 1.5716321468353271\n",
      "Epoch 60, Training Loss: 1.6042382717132568, Validation Loss: 1.5669937133789062\n",
      "Epoch 61, Training Loss: 1.5998845100402832, Validation Loss: 1.5630552768707275\n",
      "Epoch 62, Training Loss: 1.5953993797302246, Validation Loss: 1.5580706596374512\n",
      "Epoch 63, Training Loss: 1.5909250974655151, Validation Loss: 1.5536733865737915\n",
      "Epoch 64, Training Loss: 1.5867441892623901, Validation Loss: 1.549787163734436\n",
      "Epoch 65, Training Loss: 1.5825865268707275, Validation Loss: 1.5445911884307861\n",
      "Epoch 66, Training Loss: 1.5781508684158325, Validation Loss: 1.5404143333435059\n",
      "Epoch 67, Training Loss: 1.5737740993499756, Validation Loss: 1.5366753339767456\n",
      "Epoch 68, Training Loss: 1.5696752071380615, Validation Loss: 1.5316981077194214\n",
      "Epoch 69, Training Loss: 1.5654101371765137, Validation Loss: 1.5279616117477417\n",
      "Epoch 70, Training Loss: 1.5607410669326782, Validation Loss: 1.5239681005477905\n",
      "Epoch 71, Training Loss: 1.556078553199768, Validation Loss: 1.5185461044311523\n",
      "Epoch 72, Training Loss: 1.5511168241500854, Validation Loss: 1.5146864652633667\n",
      "Epoch 73, Training Loss: 1.5455447435379028, Validation Loss: 1.5098960399627686\n",
      "Epoch 74, Training Loss: 1.5399818420410156, Validation Loss: 1.50558602809906\n",
      "Epoch 75, Training Loss: 1.5346364974975586, Validation Loss: 1.5029305219650269\n",
      "Epoch 76, Training Loss: 1.5296852588653564, Validation Loss: 1.4968786239624023\n",
      "Epoch 77, Training Loss: 1.526494026184082, Validation Loss: 1.5075987577438354\n",
      "Epoch 78, Training Loss: 1.528979778289795, Validation Loss: 1.4933314323425293\n",
      "Epoch 79, Training Loss: 1.5240098237991333, Validation Loss: 1.4868074655532837\n",
      "Epoch 80, Training Loss: 1.5094106197357178, Validation Loss: 1.49337899684906\n",
      "Epoch 81, Training Loss: 1.5108966827392578, Validation Loss: 1.4796897172927856\n",
      "Epoch 82, Training Loss: 1.504536747932434, Validation Loss: 1.475655436515808\n",
      "Epoch 83, Training Loss: 1.4969587326049805, Validation Loss: 1.484232783317566\n",
      "Epoch 84, Training Loss: 1.496414303779602, Validation Loss: 1.47055983543396\n",
      "Epoch 85, Training Loss: 1.4856462478637695, Validation Loss: 1.4699074029922485\n",
      "Epoch 86, Training Loss: 1.487136721611023, Validation Loss: 1.4745866060256958\n",
      "Epoch 87, Training Loss: 1.480603575706482, Validation Loss: 1.4637936353683472\n",
      "Epoch 88, Training Loss: 1.4704740047454834, Validation Loss: 1.4639836549758911\n",
      "Epoch 89, Training Loss: 1.4748796224594116, Validation Loss: 1.464345097541809\n",
      "Epoch 90, Training Loss: 1.464144229888916, Validation Loss: 1.4576151371002197\n",
      "Epoch 91, Training Loss: 1.456869125366211, Validation Loss: 1.4540386199951172\n",
      "Epoch 92, Training Loss: 1.459546685218811, Validation Loss: 1.4524774551391602\n",
      "Epoch 93, Training Loss: 1.4482752084732056, Validation Loss: 1.4538298845291138\n",
      "Epoch 94, Training Loss: 1.446184754371643, Validation Loss: 1.4440560340881348\n",
      "Epoch 95, Training Loss: 1.4435850381851196, Validation Loss: 1.441325068473816\n",
      "Epoch 96, Training Loss: 1.4336270093917847, Validation Loss: 1.4450727701187134\n",
      "Epoch 97, Training Loss: 1.4325703382492065, Validation Loss: 1.4329930543899536\n",
      "Epoch 98, Training Loss: 1.427123785018921, Validation Loss: 1.4291210174560547\n",
      "Epoch 99, Training Loss: 1.4206037521362305, Validation Loss: 1.4326560497283936\n",
      "Epoch 100, Training Loss: 1.4189151525497437, Validation Loss: 1.4213428497314453\n",
      "Epoch 101, Training Loss: 1.4126629829406738, Validation Loss: 1.4179692268371582\n",
      "Epoch 102, Training Loss: 1.4090648889541626, Validation Loss: 1.4219070672988892\n",
      "Epoch 103, Training Loss: 1.4068713188171387, Validation Loss: 1.4123921394348145\n",
      "Epoch 104, Training Loss: 1.4013828039169312, Validation Loss: 1.4091410636901855\n",
      "Epoch 105, Training Loss: 1.3971632719039917, Validation Loss: 1.4107856750488281\n",
      "Epoch 106, Training Loss: 1.394575834274292, Validation Loss: 1.4024505615234375\n",
      "Epoch 107, Training Loss: 1.3906733989715576, Validation Loss: 1.402107834815979\n",
      "Epoch 108, Training Loss: 1.385839819908142, Validation Loss: 1.3992292881011963\n",
      "Epoch 109, Training Loss: 1.3820072412490845, Validation Loss: 1.3924287557601929\n",
      "Epoch 110, Training Loss: 1.379486083984375, Validation Loss: 1.395148515701294\n",
      "Epoch 111, Training Loss: 1.3760511875152588, Validation Loss: 1.3853780031204224\n",
      "Epoch 112, Training Loss: 1.3728175163269043, Validation Loss: 1.3872698545455933\n",
      "Epoch 113, Training Loss: 1.3690475225448608, Validation Loss: 1.3854331970214844\n",
      "Epoch 114, Training Loss: 1.3658815622329712, Validation Loss: 1.3810546398162842\n",
      "Epoch 115, Training Loss: 1.3631913661956787, Validation Loss: 1.3856887817382812\n",
      "Epoch 116, Training Loss: 1.3612860441207886, Validation Loss: 1.3773257732391357\n",
      "Epoch 117, Training Loss: 1.358600378036499, Validation Loss: 1.3801586627960205\n",
      "Epoch 118, Training Loss: 1.3550899028778076, Validation Loss: 1.3738588094711304\n",
      "Epoch 119, Training Loss: 1.3523856401443481, Validation Loss: 1.3732285499572754\n",
      "Epoch 120, Training Loss: 1.3497998714447021, Validation Loss: 1.3725368976593018\n",
      "Epoch 121, Training Loss: 1.3471676111221313, Validation Loss: 1.3675918579101562\n",
      "Epoch 122, Training Loss: 1.3448001146316528, Validation Loss: 1.369123935699463\n",
      "Epoch 123, Training Loss: 1.3425794839859009, Validation Loss: 1.3628952503204346\n",
      "Epoch 124, Training Loss: 1.3404221534729004, Validation Loss: 1.365983247756958\n",
      "Epoch 125, Training Loss: 1.3378995656967163, Validation Loss: 1.3601804971694946\n",
      "Epoch 126, Training Loss: 1.3356616497039795, Validation Loss: 1.3629432916641235\n",
      "Epoch 127, Training Loss: 1.3333922624588013, Validation Loss: 1.3559266328811646\n",
      "Epoch 128, Training Loss: 1.331053376197815, Validation Loss: 1.3574378490447998\n",
      "Epoch 129, Training Loss: 1.328535795211792, Validation Loss: 1.3499099016189575\n",
      "Epoch 130, Training Loss: 1.3261466026306152, Validation Loss: 1.3520625829696655\n",
      "Epoch 131, Training Loss: 1.3236582279205322, Validation Loss: 1.3456106185913086\n",
      "Epoch 132, Training Loss: 1.3211901187896729, Validation Loss: 1.3462114334106445\n",
      "Epoch 133, Training Loss: 1.3186761140823364, Validation Loss: 1.340783715248108\n",
      "Epoch 134, Training Loss: 1.3162785768508911, Validation Loss: 1.3388593196868896\n",
      "Epoch 135, Training Loss: 1.3139322996139526, Validation Loss: 1.3352388143539429\n",
      "Epoch 136, Training Loss: 1.31199312210083, Validation Loss: 1.333484172821045\n",
      "Epoch 137, Training Loss: 1.3099807500839233, Validation Loss: 1.3309848308563232\n",
      "Epoch 138, Training Loss: 1.3079036474227905, Validation Loss: 1.329617977142334\n",
      "Epoch 139, Training Loss: 1.3058185577392578, Validation Loss: 1.3273980617523193\n",
      "Epoch 140, Training Loss: 1.3040494918823242, Validation Loss: 1.3270649909973145\n",
      "Epoch 141, Training Loss: 1.3023253679275513, Validation Loss: 1.3219451904296875\n",
      "Epoch 142, Training Loss: 1.3008791208267212, Validation Loss: 1.3293696641921997\n",
      "Epoch 143, Training Loss: 1.3009995222091675, Validation Loss: 1.3205955028533936\n",
      "Epoch 144, Training Loss: 1.3090592622756958, Validation Loss: 1.3778059482574463\n",
      "Epoch 145, Training Loss: 1.3343620300292969, Validation Loss: 1.3418198823928833\n",
      "Epoch 146, Training Loss: 1.3419486284255981, Validation Loss: 1.3230369091033936\n",
      "Epoch 147, Training Loss: 1.2948700189590454, Validation Loss: 1.3520208597183228\n",
      "Epoch 148, Training Loss: 1.313123106956482, Validation Loss: 1.3192827701568604\n",
      "Epoch 149, Training Loss: 1.3126261234283447, Validation Loss: 1.3084611892700195\n",
      "Epoch 150, Training Loss: 1.288964033126831, Validation Loss: 1.3529679775238037\n",
      "Epoch 151, Training Loss: 1.3119072914123535, Validation Loss: 1.3056821823120117\n",
      "Epoch 152, Training Loss: 1.2874865531921387, Validation Loss: 1.307133436203003\n",
      "Epoch 153, Training Loss: 1.2976232767105103, Validation Loss: 1.32392156124115\n",
      "Epoch 154, Training Loss: 1.2914420366287231, Validation Loss: 1.3176919221878052\n",
      "Epoch 155, Training Loss: 1.2870731353759766, Validation Loss: 1.303242564201355\n",
      "Epoch 156, Training Loss: 1.2924933433532715, Validation Loss: 1.29912531375885\n",
      "Epoch 157, Training Loss: 1.2801276445388794, Validation Loss: 1.3233829736709595\n",
      "Epoch 158, Training Loss: 1.2897377014160156, Validation Loss: 1.2991913557052612\n",
      "Epoch 159, Training Loss: 1.2771801948547363, Validation Loss: 1.2968268394470215\n",
      "Epoch 160, Training Loss: 1.2850573062896729, Validation Loss: 1.3000373840332031\n",
      "Epoch 161, Training Loss: 1.2753440141677856, Validation Loss: 1.3108489513397217\n",
      "Epoch 162, Training Loss: 1.2798092365264893, Validation Loss: 1.2917861938476562\n",
      "Epoch 163, Training Loss: 1.2743819952011108, Validation Loss: 1.2902461290359497\n",
      "Epoch 164, Training Loss: 1.2753040790557861, Validation Loss: 1.300624966621399\n",
      "Epoch 165, Training Loss: 1.2730604410171509, Validation Loss: 1.2974048852920532\n",
      "Epoch 166, Training Loss: 1.2711392641067505, Validation Loss: 1.2865220308303833\n",
      "Epoch 167, Training Loss: 1.2717406749725342, Validation Loss: 1.2865318059921265\n",
      "Epoch 168, Training Loss: 1.2676388025283813, Validation Loss: 1.2983663082122803\n",
      "Epoch 169, Training Loss: 1.2697412967681885, Validation Loss: 1.2878615856170654\n",
      "Epoch 170, Training Loss: 1.2651153802871704, Validation Loss: 1.2836899757385254\n",
      "Epoch 171, Training Loss: 1.2676653861999512, Validation Loss: 1.2879465818405151\n",
      "Epoch 172, Training Loss: 1.2633366584777832, Validation Loss: 1.2922072410583496\n",
      "Epoch 173, Training Loss: 1.264488697052002, Validation Loss: 1.2811470031738281\n",
      "Epoch 174, Training Loss: 1.2625473737716675, Validation Loss: 1.2803016901016235\n",
      "Epoch 175, Training Loss: 1.2612576484680176, Validation Loss: 1.2884145975112915\n",
      "Epoch 176, Training Loss: 1.2616784572601318, Validation Loss: 1.282313585281372\n",
      "Epoch 177, Training Loss: 1.2588505744934082, Validation Loss: 1.278298258781433\n",
      "Epoch 178, Training Loss: 1.2599029541015625, Validation Loss: 1.2818500995635986\n",
      "Epoch 179, Training Loss: 1.257215976715088, Validation Loss: 1.284786343574524\n",
      "Epoch 180, Training Loss: 1.2576169967651367, Validation Loss: 1.2762501239776611\n",
      "Epoch 181, Training Loss: 1.2561907768249512, Validation Loss: 1.2758886814117432\n",
      "Epoch 182, Training Loss: 1.2551274299621582, Validation Loss: 1.2815109491348267\n",
      "Epoch 183, Training Loss: 1.255010962486267, Validation Loss: 1.2767454385757446\n",
      "Epoch 184, Training Loss: 1.2531377077102661, Validation Loss: 1.272937536239624\n",
      "Epoch 185, Training Loss: 1.2535115480422974, Validation Loss: 1.2760642766952515\n",
      "Epoch 186, Training Loss: 1.251911997795105, Validation Loss: 1.2757362127304077\n",
      "Epoch 187, Training Loss: 1.251471996307373, Validation Loss: 1.2703882455825806\n",
      "Epoch 188, Training Loss: 1.2508771419525146, Validation Loss: 1.271605372428894\n",
      "Epoch 189, Training Loss: 1.249650239944458, Validation Loss: 1.2748764753341675\n",
      "Epoch 190, Training Loss: 1.249660849571228, Validation Loss: 1.2696921825408936\n",
      "Epoch 191, Training Loss: 1.2485218048095703, Validation Loss: 1.2688896656036377\n",
      "Epoch 192, Training Loss: 1.2478481531143188, Validation Loss: 1.2719279527664185\n",
      "Epoch 193, Training Loss: 1.247480034828186, Validation Loss: 1.2683789730072021\n",
      "Epoch 194, Training Loss: 1.2464182376861572, Validation Loss: 1.2664475440979004\n",
      "Epoch 195, Training Loss: 1.2461416721343994, Validation Loss: 1.2699159383773804\n",
      "Epoch 196, Training Loss: 1.2455943822860718, Validation Loss: 1.266035795211792\n",
      "Epoch 197, Training Loss: 1.2445881366729736, Validation Loss: 1.263889193534851\n",
      "Epoch 198, Training Loss: 1.244249939918518, Validation Loss: 1.2665749788284302\n",
      "Epoch 199, Training Loss: 1.2437111139297485, Validation Loss: 1.2643632888793945\n",
      "Epoch 200, Training Loss: 1.2428845167160034, Validation Loss: 1.2619656324386597\n",
      "Epoch 201, Training Loss: 1.2425729036331177, Validation Loss: 1.2638640403747559\n",
      "Epoch 202, Training Loss: 1.2419753074645996, Validation Loss: 1.2610232830047607\n",
      "Epoch 203, Training Loss: 1.241191029548645, Validation Loss: 1.2589858770370483\n",
      "Epoch 204, Training Loss: 1.2408192157745361, Validation Loss: 1.2612581253051758\n",
      "Epoch 205, Training Loss: 1.2402058839797974, Validation Loss: 1.259336233139038\n",
      "Epoch 206, Training Loss: 1.2395002841949463, Validation Loss: 1.2572399377822876\n",
      "Epoch 207, Training Loss: 1.2390544414520264, Validation Loss: 1.2589291334152222\n",
      "Epoch 208, Training Loss: 1.2385902404785156, Validation Loss: 1.2568708658218384\n",
      "Epoch 209, Training Loss: 1.2379070520401, Validation Loss: 1.2561460733413696\n",
      "Epoch 210, Training Loss: 1.2374932765960693, Validation Loss: 1.2574735879898071\n",
      "Epoch 211, Training Loss: 1.2370460033416748, Validation Loss: 1.2545751333236694\n",
      "Epoch 212, Training Loss: 1.2364590167999268, Validation Loss: 1.254081130027771\n",
      "Epoch 213, Training Loss: 1.2359559535980225, Validation Loss: 1.2553460597991943\n",
      "Epoch 214, Training Loss: 1.235568642616272, Validation Loss: 1.252785086631775\n",
      "Epoch 215, Training Loss: 1.235138177871704, Validation Loss: 1.2530438899993896\n",
      "Epoch 216, Training Loss: 1.234543800354004, Validation Loss: 1.2520747184753418\n",
      "Epoch 217, Training Loss: 1.2340503931045532, Validation Loss: 1.2510193586349487\n",
      "Epoch 218, Training Loss: 1.2336294651031494, Validation Loss: 1.2520416975021362\n",
      "Epoch 219, Training Loss: 1.2331693172454834, Validation Loss: 1.2499290704727173\n",
      "Epoch 220, Training Loss: 1.232606291770935, Validation Loss: 1.2493871450424194\n",
      "Epoch 221, Training Loss: 1.2321219444274902, Validation Loss: 1.2497478723526\n",
      "Epoch 222, Training Loss: 1.2316536903381348, Validation Loss: 1.248016357421875\n",
      "Epoch 223, Training Loss: 1.2311904430389404, Validation Loss: 1.2480708360671997\n",
      "Epoch 224, Training Loss: 1.2306625843048096, Validation Loss: 1.2466788291931152\n",
      "Epoch 225, Training Loss: 1.230154037475586, Validation Loss: 1.245320439338684\n",
      "Epoch 226, Training Loss: 1.2297059297561646, Validation Loss: 1.2457791566848755\n",
      "Epoch 227, Training Loss: 1.2292157411575317, Validation Loss: 1.2442723512649536\n",
      "Epoch 228, Training Loss: 1.22868812084198, Validation Loss: 1.243296504020691\n",
      "Epoch 229, Training Loss: 1.2281817197799683, Validation Loss: 1.2441133260726929\n",
      "Epoch 230, Training Loss: 1.2278043031692505, Validation Loss: 1.2419397830963135\n",
      "Epoch 231, Training Loss: 1.2273319959640503, Validation Loss: 1.2430986166000366\n",
      "Epoch 232, Training Loss: 1.2268770933151245, Validation Loss: 1.240384578704834\n",
      "Epoch 233, Training Loss: 1.226346492767334, Validation Loss: 1.2405529022216797\n",
      "Epoch 234, Training Loss: 1.2258609533309937, Validation Loss: 1.2401145696640015\n",
      "Epoch 235, Training Loss: 1.2253755331039429, Validation Loss: 1.239693522453308\n",
      "Epoch 236, Training Loss: 1.224916934967041, Validation Loss: 1.2387834787368774\n",
      "Epoch 237, Training Loss: 1.2244566679000854, Validation Loss: 1.2377866506576538\n",
      "Epoch 238, Training Loss: 1.2239840030670166, Validation Loss: 1.2381880283355713\n",
      "Epoch 239, Training Loss: 1.2235455513000488, Validation Loss: 1.2365525960922241\n",
      "Epoch 240, Training Loss: 1.2231276035308838, Validation Loss: 1.2374703884124756\n",
      "Epoch 241, Training Loss: 1.2227561473846436, Validation Loss: 1.2344812154769897\n",
      "Epoch 242, Training Loss: 1.2224303483963013, Validation Loss: 1.2374753952026367\n",
      "Epoch 243, Training Loss: 1.2220220565795898, Validation Loss: 1.2331418991088867\n",
      "Epoch 244, Training Loss: 1.221614956855774, Validation Loss: 1.2361267805099487\n",
      "Epoch 245, Training Loss: 1.2212200164794922, Validation Loss: 1.2323349714279175\n",
      "Epoch 246, Training Loss: 1.2208631038665771, Validation Loss: 1.2370500564575195\n",
      "Epoch 247, Training Loss: 1.2206354141235352, Validation Loss: 1.2310619354248047\n",
      "Epoch 248, Training Loss: 1.2204105854034424, Validation Loss: 1.2364896535873413\n",
      "Epoch 249, Training Loss: 1.2198882102966309, Validation Loss: 1.2313075065612793\n",
      "Epoch 250, Training Loss: 1.2192081212997437, Validation Loss: 1.234260082244873\n",
      "Epoch 251, Training Loss: 1.218551516532898, Validation Loss: 1.2321345806121826\n",
      "Epoch 252, Training Loss: 1.2180616855621338, Validation Loss: 1.2319871187210083\n",
      "Epoch 253, Training Loss: 1.2175872325897217, Validation Loss: 1.231245517730713\n",
      "Epoch 254, Training Loss: 1.217193603515625, Validation Loss: 1.2302383184432983\n",
      "Epoch 255, Training Loss: 1.2168384790420532, Validation Loss: 1.232023000717163\n",
      "Epoch 256, Training Loss: 1.2165119647979736, Validation Loss: 1.228715181350708\n",
      "Epoch 257, Training Loss: 1.216317057609558, Validation Loss: 1.2329736948013306\n",
      "Epoch 258, Training Loss: 1.2161014080047607, Validation Loss: 1.2276233434677124\n",
      "Epoch 259, Training Loss: 1.216153860092163, Validation Loss: 1.234769582748413\n",
      "Epoch 260, Training Loss: 1.2163519859313965, Validation Loss: 1.2258244752883911\n",
      "Epoch 261, Training Loss: 1.2164406776428223, Validation Loss: 1.2361443042755127\n",
      "Epoch 262, Training Loss: 1.2163372039794922, Validation Loss: 1.2251019477844238\n",
      "Epoch 263, Training Loss: 1.215527057647705, Validation Loss: 1.2327320575714111\n",
      "Epoch 264, Training Loss: 1.2146692276000977, Validation Loss: 1.2247676849365234\n",
      "Epoch 265, Training Loss: 1.2136512994766235, Validation Loss: 1.2289522886276245\n",
      "Epoch 266, Training Loss: 1.2129051685333252, Validation Loss: 1.2245877981185913\n",
      "Epoch 267, Training Loss: 1.2123218774795532, Validation Loss: 1.2264810800552368\n",
      "Epoch 268, Training Loss: 1.211745262145996, Validation Loss: 1.2240298986434937\n",
      "Epoch 269, Training Loss: 1.2113186120986938, Validation Loss: 1.2242316007614136\n",
      "Epoch 270, Training Loss: 1.2109346389770508, Validation Loss: 1.2246013879776\n",
      "Epoch 271, Training Loss: 1.2106120586395264, Validation Loss: 1.2219775915145874\n",
      "Epoch 272, Training Loss: 1.2103426456451416, Validation Loss: 1.2254923582077026\n",
      "Epoch 273, Training Loss: 1.2100764513015747, Validation Loss: 1.222131609916687\n",
      "Epoch 274, Training Loss: 1.2100352048873901, Validation Loss: 1.226886510848999\n",
      "Epoch 275, Training Loss: 1.210540771484375, Validation Loss: 1.2193074226379395\n",
      "Epoch 276, Training Loss: 1.2112605571746826, Validation Loss: 1.233888030052185\n",
      "Epoch 277, Training Loss: 1.2126034498214722, Validation Loss: 1.2195993661880493\n",
      "Epoch 278, Training Loss: 1.2140110731124878, Validation Loss: 1.2406644821166992\n",
      "Epoch 279, Training Loss: 1.2176579236984253, Validation Loss: 1.2205157279968262\n",
      "Epoch 280, Training Loss: 1.2179752588272095, Validation Loss: 1.238122582435608\n",
      "Epoch 281, Training Loss: 1.2152873277664185, Validation Loss: 1.2186782360076904\n",
      "Epoch 282, Training Loss: 1.2102775573730469, Validation Loss: 1.22209632396698\n",
      "Epoch 283, Training Loss: 1.2071963548660278, Validation Loss: 1.2223927974700928\n",
      "Epoch 284, Training Loss: 1.2070972919464111, Validation Loss: 1.217347264289856\n",
      "Epoch 285, Training Loss: 1.2088048458099365, Validation Loss: 1.2304086685180664\n",
      "Epoch 286, Training Loss: 1.2098934650421143, Validation Loss: 1.2153477668762207\n",
      "Epoch 287, Training Loss: 1.2084805965423584, Validation Loss: 1.2214058637619019\n",
      "Epoch 288, Training Loss: 1.2058533430099487, Validation Loss: 1.2192070484161377\n",
      "Epoch 289, Training Loss: 1.20504891872406, Validation Loss: 1.216392159461975\n",
      "Epoch 290, Training Loss: 1.2055468559265137, Validation Loss: 1.2268990278244019\n",
      "Epoch 291, Training Loss: 1.206838607788086, Validation Loss: 1.2154194116592407\n",
      "Epoch 292, Training Loss: 1.2058229446411133, Validation Loss: 1.2220442295074463\n",
      "Epoch 293, Training Loss: 1.2047802209854126, Validation Loss: 1.2173293828964233\n",
      "Epoch 294, Training Loss: 1.2034242153167725, Validation Loss: 1.2159043550491333\n",
      "Epoch 295, Training Loss: 1.2032870054244995, Validation Loss: 1.220699429512024\n",
      "Epoch 296, Training Loss: 1.2037131786346436, Validation Loss: 1.2137562036514282\n",
      "Epoch 297, Training Loss: 1.203942060470581, Validation Loss: 1.2209967374801636\n",
      "Epoch 298, Training Loss: 1.203665852546692, Validation Loss: 1.2139363288879395\n",
      "Epoch 299, Training Loss: 1.2025028467178345, Validation Loss: 1.2184960842132568\n",
      "Epoch 300, Training Loss: 1.2016814947128296, Validation Loss: 1.2174710035324097\n",
      "Epoch 301, Training Loss: 1.201171636581421, Validation Loss: 1.2159178256988525\n",
      "Epoch 302, Training Loss: 1.2010940313339233, Validation Loss: 1.2201721668243408\n",
      "Epoch 303, Training Loss: 1.2011851072311401, Validation Loss: 1.2136708498001099\n",
      "Epoch 304, Training Loss: 1.20116126537323, Validation Loss: 1.219569444656372\n",
      "Epoch 305, Training Loss: 1.2009611129760742, Validation Loss: 1.2131743431091309\n",
      "Epoch 306, Training Loss: 1.2004867792129517, Validation Loss: 1.2184351682662964\n",
      "Epoch 307, Training Loss: 1.199890375137329, Validation Loss: 1.2136633396148682\n",
      "Epoch 308, Training Loss: 1.1993266344070435, Validation Loss: 1.2163078784942627\n",
      "Epoch 309, Training Loss: 1.198742151260376, Validation Loss: 1.215500831604004\n",
      "Epoch 310, Training Loss: 1.1983486413955688, Validation Loss: 1.2152698040008545\n",
      "Epoch 311, Training Loss: 1.1980164051055908, Validation Loss: 1.216178297996521\n",
      "Epoch 312, Training Loss: 1.1978049278259277, Validation Loss: 1.2131421566009521\n",
      "Epoch 313, Training Loss: 1.1976999044418335, Validation Loss: 1.2185107469558716\n",
      "Epoch 314, Training Loss: 1.197750210762024, Validation Loss: 1.212030291557312\n",
      "Epoch 315, Training Loss: 1.197508454322815, Validation Loss: 1.2179217338562012\n",
      "Epoch 316, Training Loss: 1.1973285675048828, Validation Loss: 1.2119277715682983\n",
      "Epoch 317, Training Loss: 1.19703209400177, Validation Loss: 1.2184675931930542\n",
      "Epoch 318, Training Loss: 1.1967661380767822, Validation Loss: 1.2117080688476562\n",
      "Epoch 319, Training Loss: 1.1959952116012573, Validation Loss: 1.215894103050232\n",
      "Epoch 320, Training Loss: 1.19540536403656, Validation Loss: 1.2116235494613647\n",
      "Epoch 321, Training Loss: 1.1949490308761597, Validation Loss: 1.2135703563690186\n",
      "Epoch 322, Training Loss: 1.1944023370742798, Validation Loss: 1.2118250131607056\n",
      "Epoch 323, Training Loss: 1.1939666271209717, Validation Loss: 1.2119494676589966\n",
      "Epoch 324, Training Loss: 1.193691372871399, Validation Loss: 1.2123855352401733\n",
      "Epoch 325, Training Loss: 1.1934453248977661, Validation Loss: 1.2115496397018433\n",
      "Epoch 326, Training Loss: 1.1930861473083496, Validation Loss: 1.2103625535964966\n",
      "Epoch 327, Training Loss: 1.1927891969680786, Validation Loss: 1.2118955850601196\n",
      "Epoch 328, Training Loss: 1.1925199031829834, Validation Loss: 1.2094213962554932\n",
      "Epoch 329, Training Loss: 1.1925065517425537, Validation Loss: 1.2156884670257568\n",
      "Epoch 330, Training Loss: 1.1933456659317017, Validation Loss: 1.2072227001190186\n",
      "Epoch 331, Training Loss: 1.1951709985733032, Validation Loss: 1.2300609350204468\n",
      "Epoch 332, Training Loss: 1.2011336088180542, Validation Loss: 1.213792324066162\n",
      "Epoch 333, Training Loss: 1.2094372510910034, Validation Loss: 1.2503399848937988\n",
      "Epoch 334, Training Loss: 1.2154878377914429, Validation Loss: 1.2150061130523682\n",
      "Epoch 335, Training Loss: 1.209290862083435, Validation Loss: 1.2221360206604004\n",
      "Epoch 336, Training Loss: 1.1954233646392822, Validation Loss: 1.2138559818267822\n",
      "Epoch 337, Training Loss: 1.190940499305725, Validation Loss: 1.2103941440582275\n",
      "Epoch 338, Training Loss: 1.2006148099899292, Validation Loss: 1.2412647008895874\n",
      "Epoch 339, Training Loss: 1.2073462009429932, Validation Loss: 1.2079319953918457\n",
      "Epoch 340, Training Loss: 1.196372628211975, Validation Loss: 1.2100884914398193\n",
      "Epoch 341, Training Loss: 1.1890983581542969, Validation Loss: 1.2235496044158936\n",
      "Epoch 342, Training Loss: 1.1948051452636719, Validation Loss: 1.2089107036590576\n",
      "Epoch 343, Training Loss: 1.1978776454925537, Validation Loss: 1.2184749841690063\n",
      "Epoch 344, Training Loss: 1.1927658319473267, Validation Loss: 1.2116734981536865\n",
      "Epoch 345, Training Loss: 1.188633680343628, Validation Loss: 1.2087854146957397\n",
      "Epoch 346, Training Loss: 1.1950302124023438, Validation Loss: 1.226148009300232\n",
      "Epoch 347, Training Loss: 1.196071743965149, Validation Loss: 1.2052295207977295\n",
      "Epoch 348, Training Loss: 1.1890748739242554, Validation Loss: 1.2055141925811768\n",
      "Epoch 349, Training Loss: 1.188935399055481, Validation Loss: 1.2218924760818481\n",
      "Epoch 350, Training Loss: 1.1929782629013062, Validation Loss: 1.2060091495513916\n",
      "Epoch 351, Training Loss: 1.1891453266143799, Validation Loss: 1.206430435180664\n",
      "Epoch 352, Training Loss: 1.1866415739059448, Validation Loss: 1.213233232498169\n",
      "Epoch 353, Training Loss: 1.1884572505950928, Validation Loss: 1.204694390296936\n",
      "Epoch 354, Training Loss: 1.1880435943603516, Validation Loss: 1.208543062210083\n",
      "Epoch 355, Training Loss: 1.1861566305160522, Validation Loss: 1.2071760892868042\n",
      "Epoch 356, Training Loss: 1.1854850053787231, Validation Loss: 1.2042913436889648\n",
      "Epoch 357, Training Loss: 1.1867259740829468, Validation Loss: 1.2098360061645508\n",
      "Epoch 358, Training Loss: 1.185854196548462, Validation Loss: 1.2052803039550781\n",
      "Epoch 359, Training Loss: 1.1847119331359863, Validation Loss: 1.204036831855774\n",
      "Epoch 360, Training Loss: 1.1845561265945435, Validation Loss: 1.2082080841064453\n",
      "Epoch 361, Training Loss: 1.1850618124008179, Validation Loss: 1.203345537185669\n",
      "Epoch 362, Training Loss: 1.1842477321624756, Validation Loss: 1.2045143842697144\n",
      "Epoch 363, Training Loss: 1.18366277217865, Validation Loss: 1.2067468166351318\n",
      "Epoch 364, Training Loss: 1.1835838556289673, Validation Loss: 1.2013057470321655\n",
      "Epoch 365, Training Loss: 1.183667778968811, Validation Loss: 1.204947829246521\n",
      "Epoch 366, Training Loss: 1.1829605102539062, Validation Loss: 1.2033110857009888\n",
      "Epoch 367, Training Loss: 1.1822712421417236, Validation Loss: 1.2017511129379272\n",
      "Epoch 368, Training Loss: 1.1821553707122803, Validation Loss: 1.2051610946655273\n",
      "Epoch 369, Training Loss: 1.1823807954788208, Validation Loss: 1.200799822807312\n",
      "Epoch 370, Training Loss: 1.1818339824676514, Validation Loss: 1.2034050226211548\n",
      "Epoch 371, Training Loss: 1.181203842163086, Validation Loss: 1.2026540040969849\n",
      "Epoch 372, Training Loss: 1.1809743642807007, Validation Loss: 1.1995720863342285\n",
      "Epoch 373, Training Loss: 1.1810643672943115, Validation Loss: 1.2043230533599854\n",
      "Epoch 374, Training Loss: 1.1810197830200195, Validation Loss: 1.1995128393173218\n",
      "Epoch 375, Training Loss: 1.1804617643356323, Validation Loss: 1.2017935514450073\n",
      "Epoch 376, Training Loss: 1.1799565553665161, Validation Loss: 1.2006863355636597\n",
      "Epoch 377, Training Loss: 1.1795896291732788, Validation Loss: 1.1993982791900635\n",
      "Epoch 378, Training Loss: 1.1793570518493652, Validation Loss: 1.2026560306549072\n",
      "Epoch 379, Training Loss: 1.1794174909591675, Validation Loss: 1.1988121271133423\n",
      "Epoch 380, Training Loss: 1.179391622543335, Validation Loss: 1.2033300399780273\n",
      "Epoch 381, Training Loss: 1.1789149045944214, Validation Loss: 1.1997202634811401\n",
      "Epoch 382, Training Loss: 1.178444504737854, Validation Loss: 1.2008613348007202\n",
      "Epoch 383, Training Loss: 1.1781858205795288, Validation Loss: 1.2009060382843018\n",
      "Epoch 384, Training Loss: 1.1778764724731445, Validation Loss: 1.199225902557373\n",
      "Epoch 385, Training Loss: 1.1775925159454346, Validation Loss: 1.2000083923339844\n",
      "Epoch 386, Training Loss: 1.177456021308899, Validation Loss: 1.1997002363204956\n",
      "Epoch 387, Training Loss: 1.1772606372833252, Validation Loss: 1.199793815612793\n",
      "Epoch 388, Training Loss: 1.1770179271697998, Validation Loss: 1.1991385221481323\n",
      "Epoch 389, Training Loss: 1.1767477989196777, Validation Loss: 1.198777198791504\n",
      "Epoch 390, Training Loss: 1.1764631271362305, Validation Loss: 1.198752522468567\n",
      "Epoch 391, Training Loss: 1.1762465238571167, Validation Loss: 1.198990821838379\n",
      "Epoch 392, Training Loss: 1.1761362552642822, Validation Loss: 1.197732925415039\n",
      "Epoch 393, Training Loss: 1.1761248111724854, Validation Loss: 1.1988428831100464\n",
      "Epoch 394, Training Loss: 1.1758559942245483, Validation Loss: 1.197082281112671\n",
      "Epoch 395, Training Loss: 1.1756693124771118, Validation Loss: 1.1983778476715088\n",
      "Epoch 396, Training Loss: 1.1753106117248535, Validation Loss: 1.1960281133651733\n",
      "Epoch 397, Training Loss: 1.1750366687774658, Validation Loss: 1.1987162828445435\n",
      "Epoch 398, Training Loss: 1.1750465631484985, Validation Loss: 1.1956274509429932\n",
      "Epoch 399, Training Loss: 1.1750894784927368, Validation Loss: 1.1997461318969727\n",
      "Epoch 400, Training Loss: 1.1752033233642578, Validation Loss: 1.1936289072036743\n",
      "Epoch 401, Training Loss: 1.1756536960601807, Validation Loss: 1.2025551795959473\n",
      "Epoch 402, Training Loss: 1.1762291193008423, Validation Loss: 1.1927568912506104\n",
      "Epoch 403, Training Loss: 1.1764705181121826, Validation Loss: 1.2041733264923096\n",
      "Epoch 404, Training Loss: 1.1764600276947021, Validation Loss: 1.1922740936279297\n",
      "Epoch 405, Training Loss: 1.1755659580230713, Validation Loss: 1.2000415325164795\n",
      "Epoch 406, Training Loss: 1.175146222114563, Validation Loss: 1.1933262348175049\n",
      "Epoch 407, Training Loss: 1.1739944219589233, Validation Loss: 1.1959072351455688\n",
      "Epoch 408, Training Loss: 1.1733514070510864, Validation Loss: 1.1931312084197998\n",
      "Epoch 409, Training Loss: 1.1724954843521118, Validation Loss: 1.1916388273239136\n",
      "Epoch 410, Training Loss: 1.1717402935028076, Validation Loss: 1.1935875415802002\n",
      "Epoch 411, Training Loss: 1.171568751335144, Validation Loss: 1.1910476684570312\n",
      "Epoch 412, Training Loss: 1.171541452407837, Validation Loss: 1.1943467855453491\n",
      "Epoch 413, Training Loss: 1.1722359657287598, Validation Loss: 1.1899646520614624\n",
      "Epoch 414, Training Loss: 1.1724928617477417, Validation Loss: 1.1967376470565796\n",
      "Epoch 415, Training Loss: 1.1732596158981323, Validation Loss: 1.1871150732040405\n",
      "Epoch 416, Training Loss: 1.173742651939392, Validation Loss: 1.2005608081817627\n",
      "Epoch 417, Training Loss: 1.1742278337478638, Validation Loss: 1.1847516298294067\n",
      "Epoch 418, Training Loss: 1.1737011671066284, Validation Loss: 1.1987718343734741\n",
      "Epoch 419, Training Loss: 1.172946572303772, Validation Loss: 1.1824620962142944\n",
      "Epoch 420, Training Loss: 1.1709330081939697, Validation Loss: 1.189518928527832\n",
      "Epoch 421, Training Loss: 1.1688085794448853, Validation Loss: 1.1860634088516235\n",
      "Epoch 422, Training Loss: 1.1676703691482544, Validation Loss: 1.1826492547988892\n",
      "Epoch 423, Training Loss: 1.1678309440612793, Validation Loss: 1.1911016702651978\n",
      "Epoch 424, Training Loss: 1.168447732925415, Validation Loss: 1.1806193590164185\n",
      "Epoch 425, Training Loss: 1.1688882112503052, Validation Loss: 1.192156434059143\n",
      "Epoch 426, Training Loss: 1.169182538986206, Validation Loss: 1.1825408935546875\n",
      "Epoch 427, Training Loss: 1.16835618019104, Validation Loss: 1.1884126663208008\n",
      "Epoch 428, Training Loss: 1.1671249866485596, Validation Loss: 1.1823009252548218\n",
      "Epoch 429, Training Loss: 1.1653677225112915, Validation Loss: 1.1831849813461304\n",
      "Epoch 430, Training Loss: 1.1641238927841187, Validation Loss: 1.183142900466919\n",
      "Epoch 431, Training Loss: 1.16404390335083, Validation Loss: 1.1808184385299683\n",
      "Epoch 432, Training Loss: 1.1640726327896118, Validation Loss: 1.1852797269821167\n",
      "Epoch 433, Training Loss: 1.1640567779541016, Validation Loss: 1.1761888265609741\n",
      "Epoch 434, Training Loss: 1.1636040210723877, Validation Loss: 1.18384850025177\n",
      "Epoch 435, Training Loss: 1.1630902290344238, Validation Loss: 1.1745110750198364\n",
      "Epoch 436, Training Loss: 1.1616817712783813, Validation Loss: 1.1776812076568604\n",
      "Epoch 437, Training Loss: 1.160514235496521, Validation Loss: 1.1752095222473145\n",
      "Epoch 438, Training Loss: 1.1600308418273926, Validation Loss: 1.1732797622680664\n",
      "Epoch 439, Training Loss: 1.1595933437347412, Validation Loss: 1.1767778396606445\n",
      "Epoch 440, Training Loss: 1.159438133239746, Validation Loss: 1.1718924045562744\n",
      "Epoch 441, Training Loss: 1.1594018936157227, Validation Loss: 1.1778995990753174\n",
      "Epoch 442, Training Loss: 1.1590948104858398, Validation Loss: 1.1711233854293823\n",
      "Epoch 443, Training Loss: 1.159701943397522, Validation Loss: 1.178748369216919\n",
      "Epoch 444, Training Loss: 1.1609854698181152, Validation Loss: 1.170824408531189\n",
      "Epoch 445, Training Loss: 1.16111159324646, Validation Loss: 1.179263710975647\n",
      "Epoch 446, Training Loss: 1.160524845123291, Validation Loss: 1.1689188480377197\n",
      "Epoch 447, Training Loss: 1.1592087745666504, Validation Loss: 1.176876425743103\n",
      "Epoch 448, Training Loss: 1.1573773622512817, Validation Loss: 1.1655296087265015\n",
      "Epoch 449, Training Loss: 1.15548574924469, Validation Loss: 1.1693230867385864\n",
      "Epoch 450, Training Loss: 1.1533018350601196, Validation Loss: 1.1672136783599854\n",
      "Epoch 451, Training Loss: 1.1520882844924927, Validation Loss: 1.1637568473815918\n",
      "Epoch 452, Training Loss: 1.1521775722503662, Validation Loss: 1.1707649230957031\n",
      "Epoch 453, Training Loss: 1.1531356573104858, Validation Loss: 1.1603714227676392\n",
      "Epoch 454, Training Loss: 1.1546154022216797, Validation Loss: 1.1702229976654053\n",
      "Epoch 455, Training Loss: 1.1547455787658691, Validation Loss: 1.163038969039917\n",
      "Epoch 456, Training Loss: 1.1551012992858887, Validation Loss: 1.1685583591461182\n",
      "Epoch 457, Training Loss: 1.1560828685760498, Validation Loss: 1.159531831741333\n",
      "Epoch 458, Training Loss: 1.1514997482299805, Validation Loss: 1.1607896089553833\n",
      "Epoch 459, Training Loss: 1.1491096019744873, Validation Loss: 1.1588352918624878\n",
      "Epoch 460, Training Loss: 1.1493096351623535, Validation Loss: 1.1605640649795532\n",
      "Epoch 461, Training Loss: 1.1502124071121216, Validation Loss: 1.1648095846176147\n",
      "Epoch 462, Training Loss: 1.1493555307388306, Validation Loss: 1.1565483808517456\n",
      "Epoch 463, Training Loss: 1.1505153179168701, Validation Loss: 1.1607297658920288\n",
      "Epoch 464, Training Loss: 1.1482013463974, Validation Loss: 1.1543738842010498\n",
      "Epoch 465, Training Loss: 1.1451425552368164, Validation Loss: 1.1533793210983276\n",
      "Epoch 466, Training Loss: 1.146822214126587, Validation Loss: 1.1606128215789795\n",
      "Epoch 467, Training Loss: 1.1467857360839844, Validation Loss: 1.152320384979248\n",
      "Epoch 468, Training Loss: 1.1449594497680664, Validation Loss: 1.1548130512237549\n",
      "Epoch 469, Training Loss: 1.1441388130187988, Validation Loss: 1.1547528505325317\n",
      "Epoch 470, Training Loss: 1.1431326866149902, Validation Loss: 1.151167631149292\n",
      "Epoch 471, Training Loss: 1.1423193216323853, Validation Loss: 1.1529935598373413\n",
      "Epoch 472, Training Loss: 1.141369342803955, Validation Loss: 1.1519086360931396\n",
      "Epoch 473, Training Loss: 1.1418989896774292, Validation Loss: 1.1509407758712769\n",
      "Epoch 474, Training Loss: 1.1418870687484741, Validation Loss: 1.1475642919540405\n",
      "Epoch 475, Training Loss: 1.13956618309021, Validation Loss: 1.1487295627593994\n",
      "Epoch 476, Training Loss: 1.1395745277404785, Validation Loss: 1.1485381126403809\n",
      "Epoch 477, Training Loss: 1.1393166780471802, Validation Loss: 1.1479833126068115\n",
      "Epoch 478, Training Loss: 1.1391147375106812, Validation Loss: 1.1485711336135864\n",
      "Epoch 479, Training Loss: 1.1373989582061768, Validation Loss: 1.146077036857605\n",
      "Epoch 480, Training Loss: 1.1372218132019043, Validation Loss: 1.1481695175170898\n",
      "Epoch 481, Training Loss: 1.1372474431991577, Validation Loss: 1.1463571786880493\n",
      "Epoch 482, Training Loss: 1.1356998682022095, Validation Loss: 1.1431983709335327\n",
      "Epoch 483, Training Loss: 1.134999394416809, Validation Loss: 1.1449161767959595\n",
      "Epoch 484, Training Loss: 1.1348307132720947, Validation Loss: 1.1426873207092285\n",
      "Epoch 485, Training Loss: 1.1344064474105835, Validation Loss: 1.141814947128296\n",
      "Epoch 486, Training Loss: 1.1334669589996338, Validation Loss: 1.1412839889526367\n",
      "Epoch 487, Training Loss: 1.132473111152649, Validation Loss: 1.1391721963882446\n",
      "Epoch 488, Training Loss: 1.1318161487579346, Validation Loss: 1.1397380828857422\n",
      "Epoch 489, Training Loss: 1.1318739652633667, Validation Loss: 1.137270212173462\n",
      "Epoch 490, Training Loss: 1.1309990882873535, Validation Loss: 1.1357024908065796\n",
      "Epoch 491, Training Loss: 1.1302276849746704, Validation Loss: 1.1353251934051514\n",
      "Epoch 492, Training Loss: 1.1298199892044067, Validation Loss: 1.135190725326538\n",
      "Epoch 493, Training Loss: 1.1292048692703247, Validation Loss: 1.1337676048278809\n",
      "Epoch 494, Training Loss: 1.1286039352416992, Validation Loss: 1.1337565183639526\n",
      "Epoch 495, Training Loss: 1.1282209157943726, Validation Loss: 1.1323410272598267\n",
      "Epoch 496, Training Loss: 1.127586007118225, Validation Loss: 1.1338223218917847\n",
      "Epoch 497, Training Loss: 1.1273428201675415, Validation Loss: 1.1310161352157593\n",
      "Epoch 498, Training Loss: 1.1272748708724976, Validation Loss: 1.1335930824279785\n",
      "Epoch 499, Training Loss: 1.126772165298462, Validation Loss: 1.1296916007995605\n",
      "Epoch 500, Training Loss: 1.1265437602996826, Validation Loss: 1.1305503845214844\n",
      "Epoch 501, Training Loss: 1.12543523311615, Validation Loss: 1.1314373016357422\n",
      "Epoch 502, Training Loss: 1.125170350074768, Validation Loss: 1.129062294960022\n",
      "Epoch 503, Training Loss: 1.125458836555481, Validation Loss: 1.1320083141326904\n",
      "Epoch 504, Training Loss: 1.1249529123306274, Validation Loss: 1.1277451515197754\n",
      "Epoch 505, Training Loss: 1.123976469039917, Validation Loss: 1.128369927406311\n",
      "Epoch 506, Training Loss: 1.1232802867889404, Validation Loss: 1.1275931596755981\n",
      "Epoch 507, Training Loss: 1.1227667331695557, Validation Loss: 1.127774953842163\n",
      "Epoch 508, Training Loss: 1.1226677894592285, Validation Loss: 1.1275229454040527\n",
      "Epoch 509, Training Loss: 1.1223326921463013, Validation Loss: 1.1252715587615967\n",
      "Epoch 510, Training Loss: 1.122795581817627, Validation Loss: 1.1283812522888184\n",
      "Epoch 511, Training Loss: 1.1229263544082642, Validation Loss: 1.1248708963394165\n",
      "Epoch 512, Training Loss: 1.1226328611373901, Validation Loss: 1.1282367706298828\n",
      "Epoch 513, Training Loss: 1.1217396259307861, Validation Loss: 1.1272906064987183\n",
      "Epoch 514, Training Loss: 1.1211687326431274, Validation Loss: 1.1254395246505737\n",
      "Epoch 515, Training Loss: 1.1206140518188477, Validation Loss: 1.1293472051620483\n",
      "Epoch 516, Training Loss: 1.1221356391906738, Validation Loss: 1.1255784034729004\n",
      "Epoch 517, Training Loss: 1.122967004776001, Validation Loss: 1.130765676498413\n",
      "Epoch 518, Training Loss: 1.12271249294281, Validation Loss: 1.1233402490615845\n",
      "Epoch 519, Training Loss: 1.121052861213684, Validation Loss: 1.1253267526626587\n",
      "Epoch 520, Training Loss: 1.1197590827941895, Validation Loss: 1.1263593435287476\n",
      "Epoch 521, Training Loss: 1.120512843132019, Validation Loss: 1.1272201538085938\n",
      "Epoch 522, Training Loss: 1.1235772371292114, Validation Loss: 1.127305507659912\n",
      "Epoch 523, Training Loss: 1.1208220720291138, Validation Loss: 1.1219831705093384\n",
      "Epoch 524, Training Loss: 1.119130253791809, Validation Loss: 1.1223586797714233\n",
      "Epoch 525, Training Loss: 1.1185716390609741, Validation Loss: 1.1250263452529907\n",
      "Epoch 526, Training Loss: 1.1190799474716187, Validation Loss: 1.1269460916519165\n",
      "Epoch 527, Training Loss: 1.1210947036743164, Validation Loss: 1.1264655590057373\n",
      "Epoch 528, Training Loss: 1.1196987628936768, Validation Loss: 1.1215479373931885\n",
      "Epoch 529, Training Loss: 1.1176618337631226, Validation Loss: 1.12337064743042\n",
      "Epoch 530, Training Loss: 1.117159128189087, Validation Loss: 1.12057626247406\n",
      "Epoch 531, Training Loss: 1.1169984340667725, Validation Loss: 1.124772071838379\n",
      "Epoch 532, Training Loss: 1.1172289848327637, Validation Loss: 1.1223535537719727\n",
      "Epoch 533, Training Loss: 1.1172605752944946, Validation Loss: 1.1245520114898682\n",
      "Epoch 534, Training Loss: 1.1176061630249023, Validation Loss: 1.1238577365875244\n",
      "Epoch 535, Training Loss: 1.1173129081726074, Validation Loss: 1.1224267482757568\n",
      "Epoch 536, Training Loss: 1.118522047996521, Validation Loss: 1.122881293296814\n",
      "Epoch 537, Training Loss: 1.1177887916564941, Validation Loss: 1.1188349723815918\n",
      "Epoch 538, Training Loss: 1.1150627136230469, Validation Loss: 1.1227144002914429\n",
      "Epoch 539, Training Loss: 1.1150941848754883, Validation Loss: 1.1248797178268433\n",
      "Epoch 540, Training Loss: 1.117577075958252, Validation Loss: 1.1285427808761597\n",
      "Epoch 541, Training Loss: 1.120735764503479, Validation Loss: 1.1224868297576904\n",
      "Epoch 542, Training Loss: 1.1177592277526855, Validation Loss: 1.1199573278427124\n",
      "Epoch 543, Training Loss: 1.1147723197937012, Validation Loss: 1.1188361644744873\n",
      "Epoch 544, Training Loss: 1.1135358810424805, Validation Loss: 1.1212953329086304\n",
      "Epoch 545, Training Loss: 1.1135529279708862, Validation Loss: 1.1209951639175415\n",
      "Epoch 546, Training Loss: 1.1145074367523193, Validation Loss: 1.124969482421875\n",
      "Epoch 547, Training Loss: 1.1158860921859741, Validation Loss: 1.1209214925765991\n",
      "Epoch 548, Training Loss: 1.1160078048706055, Validation Loss: 1.1197234392166138\n",
      "Epoch 549, Training Loss: 1.1136833429336548, Validation Loss: 1.1182156801223755\n",
      "Epoch 550, Training Loss: 1.1128443479537964, Validation Loss: 1.117490291595459\n",
      "Epoch 551, Training Loss: 1.1126618385314941, Validation Loss: 1.1195032596588135\n",
      "Epoch 552, Training Loss: 1.112803339958191, Validation Loss: 1.120574712753296\n",
      "Epoch 553, Training Loss: 1.1137185096740723, Validation Loss: 1.1200703382492065\n",
      "Epoch 554, Training Loss: 1.114200234413147, Validation Loss: 1.1236215829849243\n",
      "Epoch 555, Training Loss: 1.1148854494094849, Validation Loss: 1.120347499847412\n",
      "Epoch 556, Training Loss: 1.1145572662353516, Validation Loss: 1.1221153736114502\n",
      "Epoch 557, Training Loss: 1.1132912635803223, Validation Loss: 1.1171172857284546\n",
      "Epoch 558, Training Loss: 1.1118913888931274, Validation Loss: 1.1228851079940796\n",
      "Epoch 559, Training Loss: 1.1118093729019165, Validation Loss: 1.1180670261383057\n",
      "Epoch 560, Training Loss: 1.1123872995376587, Validation Loss: 1.1216260194778442\n",
      "Epoch 561, Training Loss: 1.1117827892303467, Validation Loss: 1.1157666444778442\n",
      "Epoch 562, Training Loss: 1.1105703115463257, Validation Loss: 1.1180955171585083\n",
      "Epoch 563, Training Loss: 1.1108213663101196, Validation Loss: 1.1201509237289429\n",
      "Epoch 564, Training Loss: 1.110743761062622, Validation Loss: 1.1187175512313843\n",
      "Epoch 565, Training Loss: 1.1117877960205078, Validation Loss: 1.1263457536697388\n",
      "Epoch 566, Training Loss: 1.1159123182296753, Validation Loss: 1.1228584051132202\n",
      "Epoch 567, Training Loss: 1.1167857646942139, Validation Loss: 1.119517207145691\n",
      "Epoch 568, Training Loss: 1.1140859127044678, Validation Loss: 1.126114010810852\n",
      "Epoch 569, Training Loss: 1.1152814626693726, Validation Loss: 1.118945837020874\n",
      "Epoch 570, Training Loss: 1.117921233177185, Validation Loss: 1.1301769018173218\n",
      "Epoch 571, Training Loss: 1.1175220012664795, Validation Loss: 1.117701768875122\n",
      "Epoch 572, Training Loss: 1.1135821342468262, Validation Loss: 1.1216940879821777\n",
      "Epoch 573, Training Loss: 1.1128244400024414, Validation Loss: 1.1237009763717651\n",
      "Epoch 574, Training Loss: 1.1132720708847046, Validation Loss: 1.115988850593567\n",
      "Epoch 575, Training Loss: 1.1122608184814453, Validation Loss: 1.118399977684021\n",
      "Epoch 576, Training Loss: 1.1099328994750977, Validation Loss: 1.113349437713623\n",
      "Epoch 577, Training Loss: 1.1084957122802734, Validation Loss: 1.1162000894546509\n",
      "Epoch 578, Training Loss: 1.1104809045791626, Validation Loss: 1.1149494647979736\n",
      "Epoch 579, Training Loss: 1.1108592748641968, Validation Loss: 1.1176117658615112\n",
      "Epoch 580, Training Loss: 1.1103805303573608, Validation Loss: 1.1140594482421875\n",
      "Epoch 581, Training Loss: 1.1114544868469238, Validation Loss: 1.1202921867370605\n",
      "Epoch 582, Training Loss: 1.1111315488815308, Validation Loss: 1.1118603944778442\n",
      "Epoch 583, Training Loss: 1.110858678817749, Validation Loss: 1.1179786920547485\n",
      "Epoch 584, Training Loss: 1.1095678806304932, Validation Loss: 1.113556981086731\n",
      "Epoch 585, Training Loss: 1.1083159446716309, Validation Loss: 1.1127862930297852\n",
      "Epoch 586, Training Loss: 1.1080535650253296, Validation Loss: 1.1122711896896362\n",
      "Epoch 587, Training Loss: 1.1071033477783203, Validation Loss: 1.1128449440002441\n",
      "Epoch 588, Training Loss: 1.106116771697998, Validation Loss: 1.1110976934432983\n",
      "Epoch 589, Training Loss: 1.106943130493164, Validation Loss: 1.114925742149353\n",
      "Epoch 590, Training Loss: 1.1066581010818481, Validation Loss: 1.1088764667510986\n",
      "Epoch 591, Training Loss: 1.1059577465057373, Validation Loss: 1.1109223365783691\n",
      "Epoch 592, Training Loss: 1.105374813079834, Validation Loss: 1.1101123094558716\n",
      "Epoch 593, Training Loss: 1.1051619052886963, Validation Loss: 1.107852578163147\n",
      "Epoch 594, Training Loss: 1.1053515672683716, Validation Loss: 1.1116607189178467\n",
      "Epoch 595, Training Loss: 1.1052920818328857, Validation Loss: 1.1088076829910278\n",
      "Epoch 596, Training Loss: 1.1047543287277222, Validation Loss: 1.1089483499526978\n",
      "Epoch 597, Training Loss: 1.1045422554016113, Validation Loss: 1.1107691526412964\n",
      "Epoch 598, Training Loss: 1.104780673980713, Validation Loss: 1.1084262132644653\n",
      "Epoch 599, Training Loss: 1.1051502227783203, Validation Loss: 1.1103841066360474\n",
      "Epoch 600, Training Loss: 1.10520339012146, Validation Loss: 1.1086647510528564\n",
      "Epoch 601, Training Loss: 1.1049799919128418, Validation Loss: 1.1067482233047485\n",
      "Epoch 602, Training Loss: 1.106601357460022, Validation Loss: 1.119667649269104\n",
      "Epoch 603, Training Loss: 1.1103578805923462, Validation Loss: 1.1129732131958008\n",
      "Epoch 604, Training Loss: 1.1154804229736328, Validation Loss: 1.1327801942825317\n",
      "Epoch 605, Training Loss: 1.121323823928833, Validation Loss: 1.1145665645599365\n",
      "Epoch 606, Training Loss: 1.1175318956375122, Validation Loss: 1.120605707168579\n",
      "Epoch 607, Training Loss: 1.1116960048675537, Validation Loss: 1.1058882474899292\n",
      "Epoch 608, Training Loss: 1.1061161756515503, Validation Loss: 1.1082396507263184\n",
      "Epoch 609, Training Loss: 1.1046948432922363, Validation Loss: 1.1102402210235596\n",
      "Epoch 610, Training Loss: 1.1057183742523193, Validation Loss: 1.108848214149475\n",
      "Epoch 611, Training Loss: 1.108536958694458, Validation Loss: 1.1166073083877563\n",
      "Epoch 612, Training Loss: 1.109853982925415, Validation Loss: 1.1054563522338867\n",
      "Epoch 613, Training Loss: 1.1068170070648193, Validation Loss: 1.1073791980743408\n",
      "Epoch 614, Training Loss: 1.1040276288986206, Validation Loss: 1.1067965030670166\n",
      "Epoch 615, Training Loss: 1.103605031967163, Validation Loss: 1.102758765220642\n",
      "Epoch 616, Training Loss: 1.1054202318191528, Validation Loss: 1.111619234085083\n",
      "Epoch 617, Training Loss: 1.1071887016296387, Validation Loss: 1.1032475233078003\n",
      "Epoch 618, Training Loss: 1.1069622039794922, Validation Loss: 1.1076608896255493\n",
      "Epoch 619, Training Loss: 1.104705572128296, Validation Loss: 1.1045786142349243\n",
      "Epoch 620, Training Loss: 1.1033374071121216, Validation Loss: 1.1016839742660522\n",
      "Epoch 621, Training Loss: 1.1029155254364014, Validation Loss: 1.1058199405670166\n",
      "Epoch 622, Training Loss: 1.103195309638977, Validation Loss: 1.1024113893508911\n",
      "Epoch 623, Training Loss: 1.1049368381500244, Validation Loss: 1.1088470220565796\n",
      "Epoch 624, Training Loss: 1.1070194244384766, Validation Loss: 1.1061946153640747\n",
      "Epoch 625, Training Loss: 1.105216383934021, Validation Loss: 1.1042619943618774\n",
      "Epoch 626, Training Loss: 1.102839708328247, Validation Loss: 1.1040860414505005\n",
      "Epoch 627, Training Loss: 1.1013818979263306, Validation Loss: 1.1030364036560059\n",
      "Epoch 628, Training Loss: 1.1018999814987183, Validation Loss: 1.1064174175262451\n",
      "Epoch 629, Training Loss: 1.1040416955947876, Validation Loss: 1.1032354831695557\n",
      "Epoch 630, Training Loss: 1.1038330793380737, Validation Loss: 1.1065635681152344\n",
      "Epoch 631, Training Loss: 1.102943778038025, Validation Loss: 1.1010667085647583\n",
      "Epoch 632, Training Loss: 1.1020948886871338, Validation Loss: 1.1095346212387085\n",
      "Epoch 633, Training Loss: 1.1026575565338135, Validation Loss: 1.1019489765167236\n",
      "Epoch 634, Training Loss: 1.1028856039047241, Validation Loss: 1.1045124530792236\n",
      "Epoch 635, Training Loss: 1.1011910438537598, Validation Loss: 1.1025452613830566\n",
      "Epoch 636, Training Loss: 1.1005423069000244, Validation Loss: 1.1007047891616821\n",
      "Epoch 637, Training Loss: 1.1016638278961182, Validation Loss: 1.1084648370742798\n",
      "Epoch 638, Training Loss: 1.102005958557129, Validation Loss: 1.1004964113235474\n",
      "Epoch 639, Training Loss: 1.1010273694992065, Validation Loss: 1.1036325693130493\n",
      "Epoch 640, Training Loss: 1.1010123491287231, Validation Loss: 1.1053035259246826\n",
      "Epoch 641, Training Loss: 1.1020158529281616, Validation Loss: 1.1039001941680908\n",
      "Epoch 642, Training Loss: 1.103409767150879, Validation Loss: 1.1045794486999512\n",
      "Epoch 643, Training Loss: 1.1018145084381104, Validation Loss: 1.1020958423614502\n",
      "Epoch 644, Training Loss: 1.100426197052002, Validation Loss: 1.0992162227630615\n",
      "Epoch 645, Training Loss: 1.1004507541656494, Validation Loss: 1.1076267957687378\n",
      "Epoch 646, Training Loss: 1.1013277769088745, Validation Loss: 1.1015568971633911\n",
      "Epoch 647, Training Loss: 1.0998380184173584, Validation Loss: 1.1006933450698853\n",
      "Epoch 648, Training Loss: 1.099266767501831, Validation Loss: 1.1091713905334473\n",
      "Epoch 649, Training Loss: 1.1012729406356812, Validation Loss: 1.0992627143859863\n",
      "Epoch 650, Training Loss: 1.1036008596420288, Validation Loss: 1.1139429807662964\n",
      "Epoch 651, Training Loss: 1.10439932346344, Validation Loss: 1.1008670330047607\n",
      "Epoch 652, Training Loss: 1.1030696630477905, Validation Loss: 1.1096330881118774\n",
      "Epoch 653, Training Loss: 1.1051552295684814, Validation Loss: 1.1030422449111938\n",
      "Epoch 654, Training Loss: 1.1050899028778076, Validation Loss: 1.105067491531372\n",
      "Epoch 655, Training Loss: 1.1016569137573242, Validation Loss: 1.0979257822036743\n",
      "Epoch 656, Training Loss: 1.1003552675247192, Validation Loss: 1.1063206195831299\n",
      "Epoch 657, Training Loss: 1.099861741065979, Validation Loss: 1.100009560585022\n",
      "Epoch 658, Training Loss: 1.0979681015014648, Validation Loss: 1.101476788520813\n",
      "Epoch 659, Training Loss: 1.0974751710891724, Validation Loss: 1.104286789894104\n",
      "Epoch 660, Training Loss: 1.0983326435089111, Validation Loss: 1.0996979475021362\n",
      "Epoch 661, Training Loss: 1.0987565517425537, Validation Loss: 1.1056687831878662\n",
      "Epoch 662, Training Loss: 1.0984010696411133, Validation Loss: 1.097633719444275\n",
      "Epoch 663, Training Loss: 1.0980154275894165, Validation Loss: 1.099256157875061\n",
      "Epoch 664, Training Loss: 1.0980745553970337, Validation Loss: 1.101287841796875\n",
      "Epoch 665, Training Loss: 1.0985066890716553, Validation Loss: 1.1023672819137573\n",
      "Epoch 666, Training Loss: 1.0995702743530273, Validation Loss: 1.1008626222610474\n",
      "Epoch 667, Training Loss: 1.0998997688293457, Validation Loss: 1.1099120378494263\n",
      "Epoch 668, Training Loss: 1.1031320095062256, Validation Loss: 1.1034231185913086\n",
      "Epoch 669, Training Loss: 1.1072282791137695, Validation Loss: 1.1218204498291016\n",
      "Epoch 670, Training Loss: 1.109929084777832, Validation Loss: 1.1036043167114258\n",
      "Epoch 671, Training Loss: 1.1060361862182617, Validation Loss: 1.1061944961547852\n",
      "Epoch 672, Training Loss: 1.101876974105835, Validation Loss: 1.1027324199676514\n",
      "Epoch 673, Training Loss: 1.098765254020691, Validation Loss: 1.100251317024231\n",
      "Epoch 674, Training Loss: 1.097807765007019, Validation Loss: 1.099635362625122\n",
      "Epoch 675, Training Loss: 1.0966540575027466, Validation Loss: 1.0981858968734741\n",
      "Epoch 676, Training Loss: 1.098167061805725, Validation Loss: 1.1021851301193237\n",
      "Epoch 677, Training Loss: 1.0987762212753296, Validation Loss: 1.1029434204101562\n",
      "Epoch 678, Training Loss: 1.0992628335952759, Validation Loss: 1.1042540073394775\n",
      "Epoch 679, Training Loss: 1.097838282585144, Validation Loss: 1.0988067388534546\n",
      "Epoch 680, Training Loss: 1.0970358848571777, Validation Loss: 1.1015946865081787\n",
      "Epoch 681, Training Loss: 1.0963459014892578, Validation Loss: 1.0983655452728271\n",
      "Epoch 682, Training Loss: 1.096047282218933, Validation Loss: 1.100691318511963\n",
      "Epoch 683, Training Loss: 1.0954740047454834, Validation Loss: 1.0965391397476196\n",
      "Epoch 684, Training Loss: 1.0955829620361328, Validation Loss: 1.1015784740447998\n",
      "Epoch 685, Training Loss: 1.096017837524414, Validation Loss: 1.0962532758712769\n",
      "Epoch 686, Training Loss: 1.0968140363693237, Validation Loss: 1.1039891242980957\n",
      "Epoch 687, Training Loss: 1.0968867540359497, Validation Loss: 1.0985440015792847\n",
      "Epoch 688, Training Loss: 1.0970494747161865, Validation Loss: 1.1049715280532837\n",
      "Epoch 689, Training Loss: 1.097486138343811, Validation Loss: 1.099520206451416\n",
      "Epoch 690, Training Loss: 1.0967921018600464, Validation Loss: 1.0996235609054565\n",
      "Epoch 691, Training Loss: 1.0963033437728882, Validation Loss: 1.0989145040512085\n",
      "Epoch 692, Training Loss: 1.0968493223190308, Validation Loss: 1.099765419960022\n",
      "Epoch 693, Training Loss: 1.0965147018432617, Validation Loss: 1.1009587049484253\n",
      "Epoch 694, Training Loss: 1.0958964824676514, Validation Loss: 1.0997793674468994\n",
      "Epoch 695, Training Loss: 1.0961257219314575, Validation Loss: 1.096951961517334\n",
      "Epoch 696, Training Loss: 1.0964068174362183, Validation Loss: 1.1031705141067505\n",
      "Epoch 697, Training Loss: 1.0960911512374878, Validation Loss: 1.0968859195709229\n",
      "Epoch 698, Training Loss: 1.0960569381713867, Validation Loss: 1.1026664972305298\n",
      "Epoch 699, Training Loss: 1.0965063571929932, Validation Loss: 1.1015894412994385\n",
      "Epoch 700, Training Loss: 1.0973190069198608, Validation Loss: 1.1009163856506348\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(dml_model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(5000):\n",
    "    dml_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = dml_model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1+1 ])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    dml_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dml_model(X_user_val,  product_features, all_x_other_products,prices)[0]\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        torch.save(dml_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b27e55e",
   "metadata": {
    "id": "1b27e55e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5efd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3387.49\n",
      "Expected Revenue all treated: $663.02\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "QGABODM51OV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGABODM51OV4",
    "outputId": "3769a33f-2282-4787-e276-3f7d3b56c540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2724.4629516601562"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121cd47",
   "metadata": {
    "id": "1121cd47"
   },
   "source": [
    "# debias the GTE estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "SMTzkngzuUls",
   "metadata": {
    "id": "SMTzkngzuUls"
   },
   "outputs": [],
   "source": [
    "test_prepared_data = prepare_data(X_user_test, X_product,  price*(discount*prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = test_prepared_data\n",
    "\n",
    "# Compute Theta0 and Theta1\n",
    "_,theta0_output,theta1_output = dml_model(user_features, product_features, all_x_other_products,prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de3fecfc-2f03-48e9-a089-5cebf4ac0f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 10])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rag3u55FWjiu",
   "metadata": {
    "id": "Rag3u55FWjiu"
   },
   "source": [
    "# use formulation debias for H_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "SSXrdFP4WnZL",
   "metadata": {
    "id": "SSXrdFP4WnZL"
   },
   "outputs": [],
   "source": [
    "def H_theta(theta0_output,theta1_output,all_treated_price,price):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = all_treated_price.unsqueeze(0).expand(N, M)\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_treated_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    all_treated_probabilities = F.softmax(all_treated_uti, dim=1)\n",
    "    all_control_probabilities = F.softmax(all_control_uti, dim=1)\n",
    "\n",
    "    price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "    treated_price_with_outside =  torch.cat((torch.zeros(1, device=all_treated_price.device),all_treated_price), dim=0)\n",
    "\n",
    "    H = torch.sum(all_treated_probabilities*treated_price_with_outside - all_control_probabilities*price_with_outside,dim=1)\n",
    "    expsum_treated = torch.sum(torch.exp(all_treated_uti),dim=1)\n",
    "    expsum_control = torch.sum(torch.exp(all_control_uti),dim=1)\n",
    "\n",
    "    expsum_treated_expanded = expsum_treated.unsqueeze(1).expand(-1, all_treated_uti.shape[1])  # Shape [N, M+1]\n",
    "    expsum_control_expanded = expsum_control.unsqueeze(1).expand(-1, all_control_uti.shape[1])  # Shape [N, M+1]\n",
    "\n",
    "    H_theta0 = torch.sum((torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded-\\\n",
    "                          torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded)\\\n",
    "                         *price_with_outside,dim=1)\n",
    "    H_theta1 = torch.sum(price_with_outside*(torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded*treated_price_with_outside-\\\n",
    "                                             torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded*price_with_outside),dim=1)\n",
    "\n",
    "\n",
    "    return H,H_theta0,H_theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "BFHZQM3VfBUI",
   "metadata": {
    "id": "BFHZQM3VfBUI"
   },
   "outputs": [],
   "source": [
    "H,H_theta0,H_theta1 = H_theta(theta0_output,theta1_output,all_treated_price,price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "yivIC_MKad5x",
   "metadata": {
    "id": "yivIC_MKad5x"
   },
   "outputs": [],
   "source": [
    "def l_theta(theta0_output,theta1_output,adjusted_price,decision_test):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_adjusted_price = adjusted_price.unsqueeze(0).expand(N, M)\n",
    "    uti = theta0_output + theta1_output * expand_adjusted_price\n",
    "    adjusted_price_with_outside =  torch.cat([torch.zeros(1, device=adjusted_price.device),adjusted_price])\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=uti.device)\n",
    "    uti = torch.cat((zero_utilities,uti), dim=1)\n",
    "\n",
    "    probabilities = F.softmax(uti, dim=1)\n",
    "    prod_indices = torch.ones(NUM_Product, device=device)\n",
    "    prod_indices = torch.cat([torch.zeros(1,device=device),prod_indices])\n",
    "    ltheta0 = probabilities[torch.arange(decision_test.size(0)), decision_test+1] -prod_indices[decision_test+1]\n",
    "    ltheta1 = (probabilities[torch.arange(decision_test.size(0)), decision_test+1] * adjusted_price_with_outside[decision_test+1]) - adjusted_price_with_outside[decision_test+1]\n",
    "\n",
    "\n",
    "    return ltheta0,ltheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77bcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "O8c-tupIgHVu",
   "metadata": {
    "id": "O8c-tupIgHVu"
   },
   "outputs": [],
   "source": [
    "adjusted_price = price*(discount*prod_randomization).to(device)\n",
    "decision_test = decision_test.to(device)\n",
    "ltheta0,ltheta1= l_theta(theta0_output,theta1_output,adjusted_price,decision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UVRht78QaxSG",
   "metadata": {
    "id": "UVRht78QaxSG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lambdainv(theta0_output, theta1_output, price, decision_test,epsilon =10):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = discount*price.unsqueeze(0).expand(N, M)\n",
    "\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_control_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities_control = F.softmax(all_control_uti, dim=1)\n",
    "    probabilities_treated = F.softmax(all_treated_uti, dim=1)\n",
    "\n",
    "    # Extract probabilities of chosen products\n",
    "    chosen_prob_control = probabilities_control[torch.arange(N), decision_test]\n",
    "    chosen_prob_treated = probabilities_treated[torch.arange(N), decision_test]\n",
    "\n",
    "    # Calculate second derivatives\n",
    "    ltheta00 = chosen_prob_control * (1 - chosen_prob_control) + chosen_prob_treated * (1 - chosen_prob_treated)\n",
    "    ltheta01 = chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test] + \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])\n",
    "    ltheta11 = chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test]**2 + \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])**2\n",
    "    ltheta00=ltheta00/2\n",
    "    ltheta01=ltheta01/2\n",
    "    ltheta11=ltheta11/2\n",
    "\n",
    "    # Form the 2x2 Hessian matrices for each instance\n",
    "    ltheta00 = ltheta00.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta01 = ltheta01.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta11 = ltheta11.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    top_row = torch.cat((ltheta00, ltheta01), dim=2)\n",
    "    bottom_row = torch.cat((ltheta01, ltheta11), dim=2)\n",
    "\n",
    "    L_matrix = torch.cat((top_row, bottom_row), dim=1)\n",
    "\n",
    "    # Regularization and inversion\n",
    "    \n",
    "    identity_matrix = torch.eye(2, dtype=L_matrix.dtype, device=L_matrix.device) * epsilon\n",
    "    L_matrix_reg = L_matrix + identity_matrix.unsqueeze(0).unsqueeze(0)\n",
    "    L_inv = torch.linalg.inv(L_matrix_reg)\n",
    "\n",
    "    return L_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6f7441c-eea7-4f98-8631-71cb75d14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.001,0.01,0.1,0.5,1,5,10]\n",
    "min_mape = float('inf')\n",
    "best_epsilon = None\n",
    "best_final_result = None\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    # Update L_inv for the current epsilon\n",
    "    try:\n",
    "        L_inv = lambdainv(theta0_output, theta1_output, price, decision_test, epsilon).float()\n",
    "    \n",
    "        # Calculate final_result with the given epsilon\n",
    "        H_theta_array = torch.stack((H_theta0, H_theta1), dim=-1).unsqueeze(1).float()  \n",
    "        l_theta_array = torch.stack((ltheta0, ltheta1), dim=-1).unsqueeze(-1).float()  \n",
    "    \n",
    "        # Perform matrix multiplications\n",
    "        result_intermediate = torch.matmul(H_theta_array, L_inv.squeeze(0)) \n",
    "        final_result = torch.matmul(result_intermediate, l_theta_array).squeeze(-1)  \n",
    "        final_result[torch.isnan(final_result) | torch.isinf(final_result)] = 0\n",
    "    \n",
    "        # Calculate sdl and dedl\n",
    "        sdl = H.sum().cpu().detach().numpy() * 2\n",
    "        dedl = (H.sum().cpu().detach().numpy() - final_result.sum().cpu().detach().numpy()) * 2\n",
    "    \n",
    "        # Calculate MAPE of dedl with respect to true\n",
    "        mape_dedl = np.abs((dedl - true) / true)\n",
    "    \n",
    "        # Update best_epsilon if the current epsilon yields a lower MAPE\n",
    "        if mape_dedl < min_mape:\n",
    "            min_mape = mape_dedl\n",
    "            best_epsilon = epsilon\n",
    "            best_final_result = final_result\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "q11HQu-goWM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11HQu-goWM0",
    "outputId": "ecf71629-8dbc-4d80-8d4a-de1f2534e3eb"
   },
   "outputs": [],
   "source": [
    "sdl = H.sum().cpu().detach().numpy()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3a445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedl = (H.sum().cpu().detach().numpy()-best_final_result.sum().cpu().detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c6d8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5448.92626953125, -4466.58251953125, 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl,dedl,best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d44fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of SDL:  -21.33%\n",
      "Absolute Percentage Estimation Error of SP MNL:  -0.55%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of SDL:  {100*np.abs(sdl-revenue_difference)/revenue_difference:.2f}%\")\n",
    "print(f\"Absolute Percentage Estimation Error of SP MNL:  {100*np.abs(dedl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eeea3d7d-d20e-48c8-b037-107fce171386",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pe = (naive - true) / true\n",
    "linear_pe = (linear - true) / true\n",
    "pdl_pe = (pdl - true) / true\n",
    "sdl_pe = (sdl - true) / true\n",
    "dedl_pe = (dedl - true) / true\n",
    "naive_mse = (naive - true)**2\n",
    "linear_mse =(linear - true)**2\n",
    "pdl_mse = (pdl - true)**2\n",
    "sdl_mse = (sdl - true)**2\n",
    "dedl_mse = (dedl - true)**2\n",
    "naive_e = (naive - true)\n",
    "linear_e =(linear - true)\n",
    "pdl_e = (pdl - true)\n",
    "sdl_e = (sdl - true)\n",
    "dedl_e = (dedl - true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1094fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1776921083296106 0.23181642538978905 0.1924034075705658 0.21328039807193516 -0.005452313124395358 -5289.09679633379 -1041.103616297245 -864.09702450037 -957.857034265995 24.486715734004974 27974544.92098836 1083896.7398672013 746663.667750393 917490.0980928476 599.5992474379668\n"
     ]
    }
   ],
   "source": [
    "print(naive_pe,linear_pe,pdl_pe,sdl_pe,dedl_pe,naive_e,linear_e,pdl_e,sdl_e,dedl_e,naive_mse,linear_mse,pdl_mse,sdl_mse,dedl_mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "c2a5b9ca",
    "7e135e9e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
