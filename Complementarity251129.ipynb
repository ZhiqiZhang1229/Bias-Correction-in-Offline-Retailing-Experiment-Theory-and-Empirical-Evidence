{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27192f",
   "metadata": {
    "id": "6c27192f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14efbb6",
   "metadata": {
    "id": "f14efbb6"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56819b49-b3bb-4d32-ad88-a458e7ee142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ff42b4-0dd7-4555-bfbc-c1d6a7c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Product = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42efd20-8c8f-4675-aa1a-9a2534ee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_percentage = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce9a174-68e6-47f3-ad2f-966cc2698e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc24047e-fa23-40ae-b761-5f0d1fbe595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338e9f4b-a5a7-47a9-b06d-5216d077594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083308b",
   "metadata": {
    "id": "9083308b"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "USER_Cont_FEATURES = 2*user_continuous_feature_multiplier\n",
    "USER_Dicr_FEATURES = 3\n",
    "\n",
    "Product_Cont_FEATURES = 3*prod_continuous_feature_multiplier\n",
    "Product_Dicr_FEATURES = 2\n",
    "OUTSIDE_OPTION_UTILITY = 0\n",
    "utilities = torch.zeros(NUM_USER, NUM_Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442d9dd4",
   "metadata": {
    "id": "442d9dd4"
   },
   "outputs": [],
   "source": [
    "def generate_features(N, C, D):\n",
    "    continuous_features = np.zeros((N, C))\n",
    "    for i in range(C):\n",
    "        continuous_features[:, i] = np.random.uniform(0,1,size=N)\n",
    "    binary_features = np.random.randint(0, 2, (N, D))\n",
    "    return np.hstack((continuous_features, binary_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb84b940",
   "metadata": {
    "id": "cb84b940"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class UtilityDNN(nn.Module):\n",
    "    def __init__(self, user_features, product_features):\n",
    "        super(UtilityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features + product_features, 1)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.uniform_(self.fc1.weight, a=-0.0, b=0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class PriceSensitivityDNN(nn.Module):\n",
    "    def __init__(self, user_features):\n",
    "        super(PriceSensitivityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features,1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "\n",
    "        nn.init.constant_(self.fc1.weight, 0)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.weight, 0)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.abs(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be800ea9",
   "metadata": {
    "id": "be800ea9"
   },
   "outputs": [],
   "source": [
    "X_user = generate_features(NUM_USER,USER_Cont_FEATURES, USER_Dicr_FEATURES)\n",
    "X_product = generate_features(NUM_Product, Product_Cont_FEATURES, Product_Dicr_FEATURES)\n",
    "price = np.random.uniform(0.5 ,1, NUM_Product)\n",
    "\n",
    "X_user = torch.from_numpy(X_user).float()\n",
    "X_product = torch.from_numpy(X_product).float()\n",
    "price = torch.from_numpy(price).float()\n",
    "gumbel_dist = torch.distributions.Gumbel(0, 1)\n",
    "gumbel_noise = gumbel_dist.sample((NUM_USER, NUM_Product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b15fbba-7cea-4bbe-a6b4-5decb6f40ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8851, 0.5474, 0.5374, 0.7576])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b1ab05",
   "metadata": {
    "id": "13b1ab05"
   },
   "outputs": [],
   "source": [
    "pair_utility_model = UtilityDNN(X_user.shape[1], X_product.shape[1])\n",
    "price_sensitivity_model = PriceSensitivityDNN(X_user.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590d5d95-5c75-43e6-ae03-eddddbfd3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.3816, 0.1550, 0.2653, 0.4765, 0.1089, 0.4730, 0.1282, 0.3273, 0.4812,\n",
      "         0.4673]])\n",
      "Biases: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = pair_utility_model.fc1.weight.data\n",
    "layer_biases = pair_utility_model.fc1.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb32ec0-62e7-4760-8bda-c816621ef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_biases = torch.from_numpy(np.array(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084c7cb6-2ebc-4cf1-92ea-d229b229bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new weights and biases\n",
    "with torch.no_grad():  # Avoid tracking this operation in the computation graph\n",
    "    price_sensitivity_model.fc2.bias.copy_(new_biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e280f46-89ba-422d-9bd1-6dc50dab0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.]])\n",
      "Biases: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = price_sensitivity_model.fc2.weight.data\n",
    "layer_biases = price_sensitivity_model.fc2.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d850d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bundles = 2 ** X_product.shape[0]\n",
    "bundle_utilities = torch.abs(torch.randn(num_bundles))\n",
    "def utility_model_batched_with_bundles(x_user, X_product, price, user_randomization, prod_randomization, pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    num_bundles = 2 ** num_products  # Total number of possible bundles\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  # Initialize decision array\n",
    "\n",
    "    # Generate all possible bundles (binary representations)\n",
    "    bundle_choices = torch.tensor(\n",
    "    [[int(bit) for bit in np.binary_repr(i, width=num_products)] for i in range(num_bundles)],\n",
    "    dtype=torch.float32)\n",
    "\n",
    "    # Convert numpy arrays to tensors if necessary\n",
    "    if isinstance(user_randomization, np.ndarray):\n",
    "        user_randomization = torch.from_numpy(user_randomization).to(torch.bool)\n",
    "    if isinstance(prod_randomization, np.ndarray):\n",
    "        prod_randomization = torch.from_numpy(prod_randomization).to(torch.bool)\n",
    "    if isinstance(price, np.ndarray):\n",
    "        price = torch.from_numpy(price)\n",
    "\n",
    "    # Compute price sensitivities outside the batch loop\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    # Iterate over users in batches\n",
    "    for i in range(0, num_users, batch_size):\n",
    "        batch_end = min(i + batch_size, num_users)  # Define the end of the batch\n",
    "        batch_indices = slice(i, batch_end)  # Slice for batch indexing\n",
    "\n",
    "        # Repeat the product features and price for each user in the batch\n",
    "        batch_user_features = x_user[batch_indices].unsqueeze(1).expand(-1, num_products, -1)\n",
    "        batch_prod_features = X_product.unsqueeze(0).expand(batch_end - i, -1, -1)\n",
    "        batch_price = price.unsqueeze(0).expand(batch_end - i, -1)\n",
    "\n",
    "        # Handle treatment adjustments in batch\n",
    "        batch_user_treatment = user_randomization[batch_indices].unsqueeze(1).expand(-1, num_products) == 1\n",
    "        batch_prod_treatment = prod_randomization.unsqueeze(0).expand(batch_end - i, -1) == 1\n",
    "        batch_adjusted_price = torch.where(batch_user_treatment | batch_prod_treatment, batch_price * discount, batch_price)\n",
    "\n",
    "        # Combine user and product features\n",
    "        combined_features = torch.cat((batch_user_features, batch_prod_features), dim=2)\n",
    "\n",
    "        # Compute utilities for each user-product pair using the neural network in a batch\n",
    "        utility_from_dnn = pair_utility_model(combined_features.view(-1, combined_features.shape[-1])).view(batch_end - i, num_products)\n",
    "\n",
    "        # Compute price effect\n",
    "        price_effect = price_sensitivities[batch_indices] * batch_adjusted_price\n",
    "        product_utilities = utility_from_dnn - price_effect + gumbel_noise[batch_indices]\n",
    "\n",
    "        # Calculate total bundle utilities by summing product utilities for each bundle and adding bundle-specific utilities\n",
    "        total_bundle_utilities = torch.zeros(batch_end - i, num_bundles)\n",
    "        for b in range(num_bundles):\n",
    "            bundle_mask = bundle_choices[b]  # Binary mask for the current bundle\n",
    "            bundle_utilities_sum = (product_utilities * bundle_mask).sum(dim=1)  # Sum of product utilities in the bundle\n",
    "            total_bundle_utilities[:, b] = bundle_utilities_sum + bundle_utilities[b]  # Add bundle-specific utility\n",
    "\n",
    "        # Find the bundle with the highest utility for each user\n",
    "        max_utilities, chosen_bundles = torch.max(total_bundle_utilities, dim=1)\n",
    "\n",
    "        # The empty bundle (index 0) represents the outside option (no products chosen)\n",
    "        decisions[batch_indices] = torch.where(max_utilities > 0, chosen_bundles, torch.zeros_like(chosen_bundles))\n",
    "\n",
    "    return decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d035359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue_bundle(decisions, prices):\n",
    "    total_revenue = 0.0\n",
    "    num_products = prices.shape[0]\n",
    "\n",
    "    # Iterate over each decision (bundle index) and calculate the total price of the chosen products\n",
    "    for decision in decisions:\n",
    "        if decision != 0:  # Check if the decision is not the outside option (empty bundle)\n",
    "            # Convert the bundle index to a binary mask representing the products in the bundle\n",
    "            bundle_mask = torch.tensor([int(x) for x in np.binary_repr(decision.item(), width=num_products)], dtype=torch.bool)\n",
    "            # Sum the prices of the products included in the bundle\n",
    "            total_revenue += prices[bundle_mask].sum().item()\n",
    "\n",
    "    return total_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97381ec",
   "metadata": {
    "id": "c97381ec"
   },
   "source": [
    "# GTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c9652",
   "metadata": {
    "id": "c74c9652"
   },
   "source": [
    "## All treated scenario: all products are discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8620e2f7",
   "metadata": {
    "id": "8620e2f7"
   },
   "outputs": [],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a69429d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a69429d2",
    "outputId": "da77f021-0faf-424d-91be-088437608bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([15, 15, 15,  ..., 15, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "decisions_all_treat=utility_model_batched_with_bundles(X_user, X_product, price, user_randomization, prod_randomization,\n",
    "                                                         pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10)\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3b7df52-fdc0-46a9-ad64-9119957d6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "all_num_unique = torch.unique(decisions_all_treat).numel()\n",
    "print(all_num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8748518-f99d-47ae-a031-7b89fbbfa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(173)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(9827)\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1,16):\n",
    "    print(torch.sum(decisions_all_treat==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f59995c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "f59995c9",
    "outputId": "b6b193fc-3ab4-4e76-8096-49dfeae0a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue from sales when all products are discounted: $5424.42\n"
     ]
    }
   ],
   "source": [
    "total_revenue_all_treated = calculate_revenue_bundle(decisions_all_treat, price*discount)\n",
    "print(f\"Total revenue from sales when all products are discounted: ${total_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86abbc5",
   "metadata": {},
   "source": [
    "## All control scenario: all products remain the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e8b6df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "34e8b6df",
    "outputId": "846dbf7a-920f-4c54-d79f-1b527ec86ff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([15, 15, 15,  ...,  7, 15, 15])\n",
      "Total Revenue from Sales: $26212.38\n"
     ]
    }
   ],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "\n",
    "decisions_all_control =utility_model_batched_with_bundles(X_user, X_product, price, user_randomization, prod_randomization,\n",
    "                                                         pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_control)\n",
    "total_revenue_all_control = calculate_revenue_bundle(decisions_all_control, price)\n",
    "print(f\"Total Revenue from Sales: ${total_revenue_all_control:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e3c0837",
   "metadata": {
    "id": "2e3c0837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (ALLTreated - ALLControl): $-20787.96\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = total_revenue_all_treated - total_revenue_all_control\n",
    "print(f\"Revenue Difference (ALLTreated - ALLControl): ${revenue_difference:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/total_revenue_all_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "Sp6U7JOV1vEi",
   "metadata": {
    "id": "Sp6U7JOV1vEi"
   },
   "outputs": [],
   "source": [
    "true = revenue_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec3bca",
   "metadata": {
    "id": "d0ec3bca"
   },
   "source": [
    "## product randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cbbd718",
   "metadata": {
    "id": "4cbbd718"
   },
   "outputs": [],
   "source": [
    "def calculate_bundle_revenue(decisions, prices, prod_randomization):\n",
    "    revenue_treated = 0.0\n",
    "    revenue_control = 0.0\n",
    "    num_products = prices.shape[0]\n",
    "\n",
    "    # Iterate over each user's decision (bundle index)\n",
    "    for decision in decisions:\n",
    "        if decision != 0:  # If the user did not choose the outside option (empty bundle)\n",
    "            # Convert the bundle index to a binary mask representing the products in the bundle\n",
    "            bundle_mask = torch.tensor([int(x) for x in np.binary_repr(decision.item(), width=num_products)], dtype=torch.bool)\n",
    "\n",
    "            # Iterate over products in the bundle\n",
    "            for product_index in range(num_products):\n",
    "                if bundle_mask[product_index]:  # If the product is included in the bundle\n",
    "                    product_price = prices[product_index].item()\n",
    "                    if prod_randomization[product_index]:  # Check if the product is in the treatment group\n",
    "                        revenue_treated += product_price\n",
    "                    else:  # The product is in the control group\n",
    "                        revenue_control += product_price\n",
    "\n",
    "    return revenue_treated, revenue_control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "402ee3f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402ee3f8",
    "outputId": "17e270ca-c5e5-4e92-fd2f-2c675eb2fbf8"
   },
   "outputs": [],
   "source": [
    "utilities = torch.zeros(NUM_USER, NUM_Product)\n",
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1-treatment_percentage, treatment_percentage])\n",
    "# prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, ])\n",
    "decisions_product_randomization =utility_model_batched_with_bundles(X_user, X_product, price, user_randomization, prod_randomization,\n",
    "                                                         pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022d9af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022d9af1",
    "outputId": "9f338893-e1f9-4c6a-945c-408a2e5b6f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue from Treated Products: $0.00\n",
      "Revenue from Control Products: $26212.37\n",
      "Revenue Difference (Treated - Control) by naive DIM: $-52424.75\n"
     ]
    }
   ],
   "source": [
    "revenue_treated, revenue_control = calculate_bundle_revenue(decisions_product_randomization, price-price*(1-discount)*prod_randomization, prod_randomization)\n",
    "naive = revenue_treated/treatment_percentage - revenue_control/(1-treatment_percentage)\n",
    "print(f\"Revenue from Treated Products: ${revenue_treated:.2f}\")\n",
    "print(f\"Revenue from Control Products: ${revenue_control:.2f}\")\n",
    "print(f\"Revenue Difference (Treated - Control) by naive DIM: ${naive:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/revenue_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b9eb",
   "metadata": {},
   "source": [
    "## Prepare training and testing data given experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wUgBFRaYHK7-",
   "metadata": {
    "id": "wUgBFRaYHK7-"
   },
   "outputs": [],
   "source": [
    "X_user_1, X_user_2, decision_1, decision_2 = train_test_split(\n",
    "X_user, decisions_product_randomization, test_size=1/2, random_state=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bd5975d-53cb-45c6-90e2-c36e313515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\n",
    "    'features': X_user_1,\n",
    "    'labels': decision_1\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    'features': X_user_2,\n",
    "    'labels': decision_2\n",
    "}\n",
    "\n",
    "# Flag to switch between training and test set\n",
    "use_train_set = False  # Set to False for the test set\n",
    "\n",
    "# Function to get the current active dataset\n",
    "def get_active_dataset(use_train):\n",
    "    return train_set if use_train else test_set\n",
    "def get_test_dataset(use_train):\n",
    "    return test_set if use_train else train_set\n",
    "# Retrieve the current dataset based on the flag\n",
    "current_dataset = get_active_dataset(use_train_set)\n",
    "X_user_train = current_dataset['features']\n",
    "decision_train = current_dataset['labels']\n",
    "X_user_test = get_test_dataset(use_train_set)['features']\n",
    "decision_test =  get_test_dataset(use_train_set)['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16813234",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_choices = torch.tensor(\n",
    "    [[int(bit) for bit in np.binary_repr(i, width=X_product.shape[0])] for i in range(2**X_product.shape[0])],\n",
    "    dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320887a",
   "metadata": {
    "id": "d320887a"
   },
   "source": [
    "# use simple MNL structural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b14cd096",
   "metadata": {
    "id": "b14cd096"
   },
   "outputs": [],
   "source": [
    "class BundleMNLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim, num_bundles):\n",
    "        super(BundleMNLModel, self).__init__()\n",
    "        self.beta_user = nn.Parameter(torch.randn(user_feature_dim))\n",
    "        self.beta_product = nn.Parameter(torch.randn(product_feature_dim))\n",
    "        self.beta_price = nn.Parameter(torch.tensor(-1.0))\n",
    "        self.bundle_utilities = nn.Parameter(torch.randn(num_bundles))  # Bundle-specific utilities\n",
    "\n",
    "    def forward(self, x_user, X_product, price, prod_randomization, bundle_choices):\n",
    "        N, M = x_user.shape[0], X_product.shape[0]\n",
    "        num_bundles = bundle_choices.shape[0]\n",
    "\n",
    "        # Expand user and product features to create a [N, M, F] shaped tensor\n",
    "        x_user_expanded = x_user.unsqueeze(1).expand(-1, M, -1)\n",
    "        X_product_expanded = X_product.unsqueeze(0).expand(N, -1, -1)\n",
    "\n",
    "        # Calculate linear utility from features\n",
    "        utility_user = torch.sum(x_user_expanded * self.beta_user, dim=2)\n",
    "        utility_product = torch.sum(X_product_expanded * self.beta_product, dim=2)\n",
    "\n",
    "        # Adjust prices based on product randomization (apply discount if treated)\n",
    "        adjusted_price = torch.where(prod_randomization.unsqueeze(0), price * discount, price)\n",
    "        utility_price = adjusted_price * self.beta_price\n",
    "\n",
    "        # Total product utilities (user + product + price)\n",
    "        product_utilities = utility_user + utility_product + utility_price\n",
    "\n",
    "        # Calculate total bundle utilities for each user and bundle\n",
    "        total_bundle_utilities = torch.zeros(N, num_bundles, device=x_user.device)\n",
    "        for b in range(num_bundles):\n",
    "            bundle_mask = bundle_choices[b]\n",
    "            bundle_mask = bundle_mask.to(device)\n",
    "            bundle_product_utilities = (product_utilities * bundle_mask).sum(dim=1)\n",
    "            total_bundle_utilities[:, b] = bundle_product_utilities + self.bundle_utilities[b]\n",
    "\n",
    "\n",
    "        return total_bundle_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d027617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89f614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = BundleMNLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES,\n",
    "                       num_bundles = 2 ** NUM_Product).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74e297a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74e297a",
    "outputId": "d9b4de3b-2c2d-452e-b491-1b217ba2aaf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.1449\n",
      "Epoch 500, Loss: 0.3682\n",
      "Epoch 1000, Loss: 0.3542\n",
      "Epoch 1500, Loss: 0.3526\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import torch.nn.functional as F\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    utilities = model(X_user_train, X_product, price, prod_randomization, bundle_choices)\n",
    "    choice_probabilities = F.log_softmax(utilities, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da3dbeb5-c326-49f2-95d1-948ed3fbb443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-23.1409, -13.2797, -18.4777,  ...,  -9.4791,  -9.3488,  -0.1244],\n",
       "        [-26.2084, -15.5639, -20.7619,  ..., -10.1967, -10.0664,  -0.0588],\n",
       "        [-24.8968, -14.5857, -19.7837,  ...,  -9.8855,  -9.7552,  -0.0811],\n",
       "        ...,\n",
       "        [-20.5237, -11.3438, -16.5419,  ...,  -8.9060,  -8.7757,  -0.2328],\n",
       "        [-22.3858, -12.7195, -17.9175,  ...,  -9.3089,  -9.1786,  -0.1494],\n",
       "        [-26.4584, -15.7504, -20.9485,  ..., -10.2565, -10.1262,  -0.0552]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f78d4b21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78d4b21",
    "outputId": "6078e9c0-ce87-4f8e-dd3d-88c3cfb2a616"
   },
   "outputs": [],
   "source": [
    "beta_price_est = model.beta_price.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ac97d98-d381-4a84-82c7-aa6821c3aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.402046\n"
     ]
    }
   ],
   "source": [
    "print(beta_price_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98a2386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_prices = torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    bundle_prices[b] = torch.sum(price[bundle_mask.bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17eb38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17eb38d3",
    "outputId": "df4ab9ed-52f4-4538-87a5-66932ad5f9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue (Control Group): $13089.64\n",
      "Expected Revenue (Treated Group): $13206.61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Control group (all products are control)\n",
    "all_product_control = torch.zeros(NUM_Product, dtype=torch.bool).to(device)\n",
    "# Treated group (all products are treated)\n",
    "all_product_treated = torch.ones(NUM_Product, dtype=torch.bool).to(device)\n",
    "\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "\n",
    "# Calculate expected revenue for control group\n",
    "utilities_control = model(X_user_test, X_product, price, all_product_control, bundle_choices)\n",
    "probabilities_control = F.softmax(utilities_control, dim=1)\n",
    "expected_revenue_control = torch.sum(probabilities_control * bundle_prices.expand_as(probabilities_control), dim=0).sum()\n",
    "\n",
    "print(f\"Expected Revenue (Control Group): ${expected_revenue_control.item():.2f}\")\n",
    "\n",
    "# Calculate expected revenue for treated group\n",
    "utilities_treated = model(X_user_test, X_product, price, all_product_treated, bundle_choices)\n",
    "probabilities_treated = F.softmax(utilities_treated, dim=1)\n",
    "expected_revenue_treated = torch.sum(probabilities_treated * bundle_prices.expand_as(probabilities_treated), dim=0).sum()\n",
    "\n",
    "print(f\"Expected Revenue (Treated Group): ${expected_revenue_treated.item():.2f}\")\n",
    "\n",
    "linear = (expected_revenue_treated-expected_revenue_control).cpu().detach().numpy()\n",
    "linear = linear*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "P7Z_BF2C1kdj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7Z_BF2C1kdj",
    "outputId": "6998bb85-85b3-4cf8-da34-4ecded076114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (Treated - Control) by Linear MNL: $233.96\n",
      "Absolute Percentage Estimation Error of Linear MNL:  -101.13%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Revenue Difference (Treated - Control) by Linear MNL: ${linear:.2f}\")\n",
    "print(f\"Absolute Percentage Estimation Error of Linear MNL:  {100*np.abs(linear-revenue_difference)/revenue_difference:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dada1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2899bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_bundle(user_features, product_features, prices):\n",
    "    # distinct device reference\n",
    "    device = product_features.device\n",
    "    \n",
    "    num_products = product_features.shape[0]\n",
    "    num_bundles = 2 ** num_products\n",
    "    bundle_choices = torch.tensor(\n",
    "        [[int(bit) for bit in np.binary_repr(i, width=num_products)] for i in range(num_bundles)],\n",
    "        dtype=torch.bool,\n",
    "        device=device \n",
    "    )\n",
    "    \n",
    "    # Calculate bundle prices\n",
    "    bundle_prices = torch.tensor([prices[bundle_mask].sum() for bundle_mask in bundle_choices], device=device)\n",
    "\n",
    "    # Initialize lists\n",
    "    all_x_included_products = []\n",
    "    all_x_other_products = []\n",
    "    all_bundle_prices = []\n",
    "    \n",
    "    # Iterate through each bundle\n",
    "    for i, bundle_mask in enumerate(bundle_choices):\n",
    "        # Get included product indices\n",
    "        included_indices = torch.where(bundle_mask)[0]\n",
    "        excluded_indices = torch.where(~bundle_mask)[0]\n",
    "\n",
    "        if included_indices.nelement() > 0:\n",
    "            included_products = product_features[included_indices].reshape(-1)\n",
    "        else:\n",
    "            included_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Features of excluded products\n",
    "        if excluded_indices.nelement() > 0:\n",
    "            other_products = product_features[excluded_indices].reshape(-1)\n",
    "        else:\n",
    "            other_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Price of the current bundle\n",
    "        current_bundle_price = bundle_prices[i]\n",
    "\n",
    "        # Append to lists\n",
    "        all_x_included_products.append(included_products)\n",
    "        all_x_other_products.append(other_products)\n",
    "        all_bundle_prices.append(current_bundle_price)\n",
    "        \n",
    "\n",
    "    max_included_len = max([x.numel() for x in all_x_included_products])\n",
    "    max_other_len = max([x.numel() for x in all_x_other_products])\n",
    "\n",
    "  \n",
    "    all_x_included_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_included_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_included_products\n",
    "    ])\n",
    "\n",
    "    all_x_other_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_other_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_other_products\n",
    "    ])\n",
    "    \n",
    "    all_bundle_prices = torch.stack(all_bundle_prices)\n",
    "\n",
    "    return user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "134acfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)\n",
    "X_user_train1 = X_user_train1.to(device)\n",
    "#for complementarity model\n",
    "prepared_data = prepare_data_bundle(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices= prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39864359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(PDLModel, self).__init__()\n",
    "        # Combined feature dimension includes product features, price, and user features, as well as other products' features and prices\n",
    "        total_feature_dim = user_feature_dim +product_feature_dim*(NUM_Product)+2  # +1 for price\n",
    "\n",
    "        # Single neural network to process the combined features\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1) \n",
    "        )\n",
    "            # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_user, x_product, x_other_products,prices):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                        x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        aggregated_other_features.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        prices.view(1, -1, 1).expand(N, -1, -1)),\n",
    "                                        dim=2)\n",
    "   \n",
    "\n",
    "        # Compute utility for each combined feature set\n",
    "        utilities = self.network(combined_features).squeeze(-1)\n",
    "\n",
    "\n",
    "        return utilities\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a160035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdlmodel = PDLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1ceb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.771914482116699, Validation Loss: 2.769660472869873\n",
      "Epoch 2, Training Loss: 2.7696404457092285, Validation Loss: 2.767451047897339\n",
      "Epoch 3, Training Loss: 2.767428398132324, Validation Loss: 2.764925479888916\n",
      "Epoch 4, Training Loss: 2.764913558959961, Validation Loss: 2.7621521949768066\n",
      "Epoch 5, Training Loss: 2.762153148651123, Validation Loss: 2.7584805488586426\n",
      "Epoch 6, Training Loss: 2.758495330810547, Validation Loss: 2.7542171478271484\n",
      "Epoch 7, Training Loss: 2.754244804382324, Validation Loss: 2.7490170001983643\n",
      "Epoch 8, Training Loss: 2.749060869216919, Validation Loss: 2.74277400970459\n",
      "Epoch 9, Training Loss: 2.7428219318389893, Validation Loss: 2.7352075576782227\n",
      "Epoch 10, Training Loss: 2.7352547645568848, Validation Loss: 2.725489854812622\n",
      "Epoch 11, Training Loss: 2.7254979610443115, Validation Loss: 2.7124996185302734\n",
      "Epoch 12, Training Loss: 2.71250581741333, Validation Loss: 2.6952247619628906\n",
      "Epoch 13, Training Loss: 2.695263385772705, Validation Loss: 2.6733288764953613\n",
      "Epoch 14, Training Loss: 2.673365831375122, Validation Loss: 2.646820306777954\n",
      "Epoch 15, Training Loss: 2.6468191146850586, Validation Loss: 2.615234613418579\n",
      "Epoch 16, Training Loss: 2.615182638168335, Validation Loss: 2.577949285507202\n",
      "Epoch 17, Training Loss: 2.5778534412384033, Validation Loss: 2.534304141998291\n",
      "Epoch 18, Training Loss: 2.5341548919677734, Validation Loss: 2.4836978912353516\n",
      "Epoch 19, Training Loss: 2.4835102558135986, Validation Loss: 2.4255006313323975\n",
      "Epoch 20, Training Loss: 2.4252798557281494, Validation Loss: 2.358569860458374\n",
      "Epoch 21, Training Loss: 2.358414888381958, Validation Loss: 2.2819325923919678\n",
      "Epoch 22, Training Loss: 2.2818894386291504, Validation Loss: 2.1946280002593994\n",
      "Epoch 23, Training Loss: 2.194675922393799, Validation Loss: 2.095658540725708\n",
      "Epoch 24, Training Loss: 2.095808267593384, Validation Loss: 1.98427152633667\n",
      "Epoch 25, Training Loss: 1.9846333265304565, Validation Loss: 1.861459732055664\n",
      "Epoch 26, Training Loss: 1.8619192838668823, Validation Loss: 1.727036714553833\n",
      "Epoch 27, Training Loss: 1.7274929285049438, Validation Loss: 1.580531120300293\n",
      "Epoch 28, Training Loss: 1.5809261798858643, Validation Loss: 1.4238767623901367\n",
      "Epoch 29, Training Loss: 1.424239158630371, Validation Loss: 1.2612571716308594\n",
      "Epoch 30, Training Loss: 1.2616827487945557, Validation Loss: 1.099449872970581\n",
      "Epoch 31, Training Loss: 1.100061297416687, Validation Loss: 0.9483861327171326\n",
      "Epoch 32, Training Loss: 0.9490810632705688, Validation Loss: 0.8204069137573242\n",
      "Epoch 33, Training Loss: 0.821167528629303, Validation Loss: 0.7261711359024048\n",
      "Epoch 34, Training Loss: 0.7273102402687073, Validation Loss: 0.6671015620231628\n",
      "Epoch 35, Training Loss: 0.6687236428260803, Validation Loss: 0.638218343257904\n",
      "Epoch 36, Training Loss: 0.6402629613876343, Validation Loss: 0.6303613185882568\n",
      "Epoch 37, Training Loss: 0.6329325437545776, Validation Loss: 0.6334863901138306\n",
      "Epoch 38, Training Loss: 0.6364902257919312, Validation Loss: 0.6387290358543396\n",
      "Epoch 39, Training Loss: 0.6420537829399109, Validation Loss: 0.6404675841331482\n",
      "Epoch 40, Training Loss: 0.644096314907074, Validation Loss: 0.6360365748405457\n",
      "Epoch 41, Training Loss: 0.6399467587471008, Validation Loss: 0.6243706941604614\n",
      "Epoch 42, Training Loss: 0.628474235534668, Validation Loss: 0.6052812933921814\n",
      "Epoch 43, Training Loss: 0.6095659136772156, Validation Loss: 0.5792679786682129\n",
      "Epoch 44, Training Loss: 0.5836626291275024, Validation Loss: 0.5480403304100037\n",
      "Epoch 45, Training Loss: 0.5520724654197693, Validation Loss: 0.5135219097137451\n",
      "Epoch 46, Training Loss: 0.5171180367469788, Validation Loss: 0.4780386686325073\n",
      "Epoch 47, Training Loss: 0.48120930790901184, Validation Loss: 0.44500336050987244\n",
      "Epoch 48, Training Loss: 0.4477792978286743, Validation Loss: 0.42022454738616943\n",
      "Epoch 49, Training Loss: 0.42258599400520325, Validation Loss: 0.40688857436180115\n",
      "Epoch 50, Training Loss: 0.4088859260082245, Validation Loss: 0.4060717225074768\n",
      "Epoch 51, Training Loss: 0.4077153503894806, Validation Loss: 0.4144514501094818\n",
      "Epoch 52, Training Loss: 0.4157031774520874, Validation Loss: 0.42174747586250305\n",
      "Epoch 53, Training Loss: 0.42274048924446106, Validation Loss: 0.4188607633113861\n",
      "Epoch 54, Training Loss: 0.41986459493637085, Validation Loss: 0.40552008152008057\n",
      "Epoch 55, Training Loss: 0.40674278140068054, Validation Loss: 0.38825514912605286\n",
      "Epoch 56, Training Loss: 0.3898582458496094, Validation Loss: 0.37434256076812744\n",
      "Epoch 57, Training Loss: 0.37638890743255615, Validation Loss: 0.3673974275588989\n",
      "Epoch 58, Training Loss: 0.3699566125869751, Validation Loss: 0.3669017255306244\n",
      "Epoch 59, Training Loss: 0.3699159324169159, Validation Loss: 0.36981189250946045\n",
      "Epoch 60, Training Loss: 0.37312009930610657, Validation Loss: 0.3727116286754608\n",
      "Epoch 61, Training Loss: 0.37626969814300537, Validation Loss: 0.37335896492004395\n",
      "Epoch 62, Training Loss: 0.3771522343158722, Validation Loss: 0.37115320563316345\n",
      "Epoch 63, Training Loss: 0.3750763535499573, Validation Loss: 0.3667876124382019\n",
      "Epoch 64, Training Loss: 0.3706601560115814, Validation Loss: 0.36172914505004883\n",
      "Epoch 65, Training Loss: 0.36540716886520386, Validation Loss: 0.3576677441596985\n",
      "Epoch 66, Training Loss: 0.3611777722835541, Validation Loss: 0.3561166822910309\n",
      "Epoch 67, Training Loss: 0.3593999743461609, Validation Loss: 0.3572889566421509\n",
      "Epoch 68, Training Loss: 0.3603384494781494, Validation Loss: 0.35984671115875244\n",
      "Epoch 69, Training Loss: 0.3627355098724365, Validation Loss: 0.3616254925727844\n",
      "Epoch 70, Training Loss: 0.36444297432899475, Validation Loss: 0.3611206114292145\n",
      "Epoch 71, Training Loss: 0.36399322748184204, Validation Loss: 0.3586142063140869\n",
      "Epoch 72, Training Loss: 0.36166098713874817, Validation Loss: 0.35567083954811096\n",
      "Epoch 73, Training Loss: 0.35896793007850647, Validation Loss: 0.3537561893463135\n",
      "Epoch 74, Training Loss: 0.3573516309261322, Validation Loss: 0.3533810079097748\n",
      "Epoch 75, Training Loss: 0.35726314783096313, Validation Loss: 0.3540651798248291\n",
      "Epoch 76, Training Loss: 0.3581814169883728, Validation Loss: 0.3548979163169861\n",
      "Epoch 77, Training Loss: 0.359175443649292, Validation Loss: 0.3551461100578308\n",
      "Epoch 78, Training Loss: 0.3595122694969177, Validation Loss: 0.3545866906642914\n",
      "Epoch 79, Training Loss: 0.3589648902416229, Validation Loss: 0.35350316762924194\n",
      "Epoch 80, Training Loss: 0.35782095789909363, Validation Loss: 0.35244685411453247\n",
      "Epoch 81, Training Loss: 0.356662392616272, Validation Loss: 0.35193970799446106\n",
      "Epoch 82, Training Loss: 0.35603103041648865, Validation Loss: 0.3521629571914673\n",
      "Epoch 83, Training Loss: 0.35610467195510864, Validation Loss: 0.35280218720436096\n",
      "Epoch 84, Training Loss: 0.35661616921424866, Validation Loss: 0.353288471698761\n",
      "Epoch 85, Training Loss: 0.357035368680954, Validation Loss: 0.35319921374320984\n",
      "Epoch 86, Training Loss: 0.3569641709327698, Validation Loss: 0.35254690051078796\n",
      "Epoch 87, Training Loss: 0.3564198315143585, Validation Loss: 0.35173299908638\n",
      "Epoch 88, Training Loss: 0.3557620048522949, Validation Loss: 0.3511601686477661\n",
      "Epoch 89, Training Loss: 0.3553656041622162, Validation Loss: 0.3509805500507355\n",
      "Epoch 90, Training Loss: 0.3553556799888611, Validation Loss: 0.35106587409973145\n",
      "Epoch 91, Training Loss: 0.3555823564529419, Validation Loss: 0.3511665463447571\n",
      "Epoch 92, Training Loss: 0.35578128695487976, Validation Loss: 0.3510989546775818\n",
      "Epoch 93, Training Loss: 0.3557639420032501, Validation Loss: 0.35085102915763855\n",
      "Epoch 94, Training Loss: 0.35551947355270386, Validation Loss: 0.3505588173866272\n",
      "Epoch 95, Training Loss: 0.3551879823207855, Validation Loss: 0.3503858149051666\n",
      "Epoch 96, Training Loss: 0.3549506366252899, Validation Loss: 0.3504074215888977\n",
      "Epoch 97, Training Loss: 0.3549066483974457, Validation Loss: 0.3505573272705078\n",
      "Epoch 98, Training Loss: 0.3550054430961609, Validation Loss: 0.350679874420166\n",
      "Epoch 99, Training Loss: 0.3551066219806671, Validation Loss: 0.35064777731895447\n",
      "Epoch 100, Training Loss: 0.3550941050052643, Validation Loss: 0.3504505157470703\n",
      "Epoch 101, Training Loss: 0.35495561361312866, Validation Loss: 0.35018494725227356\n",
      "Epoch 102, Training Loss: 0.3547779321670532, Validation Loss: 0.34996894001960754\n",
      "Epoch 103, Training Loss: 0.35466283559799194, Validation Loss: 0.3498581349849701\n",
      "Epoch 104, Training Loss: 0.35464927554130554, Validation Loss: 0.3498270511627197\n",
      "Epoch 105, Training Loss: 0.35469770431518555, Validation Loss: 0.34981098771095276\n",
      "Epoch 106, Training Loss: 0.35473453998565674, Validation Loss: 0.34976378083229065\n",
      "Epoch 107, Training Loss: 0.3547106087207794, Validation Loss: 0.34968867897987366\n",
      "Epoch 108, Training Loss: 0.35463064908981323, Validation Loss: 0.3496243357658386\n",
      "Epoch 109, Training Loss: 0.35454005002975464, Validation Loss: 0.3496064841747284\n",
      "Epoch 110, Training Loss: 0.354486346244812, Validation Loss: 0.34963756799697876\n",
      "Epoch 111, Training Loss: 0.35448357462882996, Validation Loss: 0.3496808111667633\n",
      "Epoch 112, Training Loss: 0.354505330324173, Validation Loss: 0.3496911823749542\n",
      "Epoch 113, Training Loss: 0.354513019323349, Validation Loss: 0.3496444523334503\n",
      "Epoch 114, Training Loss: 0.35448628664016724, Validation Loss: 0.34955406188964844\n",
      "Epoch 115, Training Loss: 0.3544353246688843, Validation Loss: 0.34945589303970337\n",
      "Epoch 116, Training Loss: 0.35438835620880127, Validation Loss: 0.3493801951408386\n",
      "Epoch 117, Training Loss: 0.3543660640716553, Validation Loss: 0.34933462738990784\n",
      "Epoch 118, Training Loss: 0.3543669879436493, Validation Loss: 0.3493076264858246\n",
      "Epoch 119, Training Loss: 0.3543728291988373, Validation Loss: 0.34928447008132935\n",
      "Epoch 120, Training Loss: 0.35436567664146423, Validation Loss: 0.34926071763038635\n",
      "Epoch 121, Training Loss: 0.35434141755104065, Validation Loss: 0.34924307465553284\n",
      "Epoch 122, Training Loss: 0.35431045293807983, Validation Loss: 0.34924009442329407\n",
      "Epoch 123, Training Loss: 0.35428717732429504, Validation Loss: 0.3492511212825775\n",
      "Epoch 124, Training Loss: 0.3542778789997101, Validation Loss: 0.3492644131183624\n",
      "Epoch 125, Training Loss: 0.35427719354629517, Validation Loss: 0.34926486015319824\n",
      "Epoch 126, Training Loss: 0.35427403450012207, Validation Loss: 0.3492441475391388\n",
      "Epoch 127, Training Loss: 0.3542617857456207, Validation Loss: 0.3492063581943512\n",
      "Epoch 128, Training Loss: 0.3542425036430359, Validation Loss: 0.34916311502456665\n",
      "Epoch 129, Training Loss: 0.3542240858078003, Validation Loss: 0.34912556409835815\n",
      "Epoch 130, Training Loss: 0.35421252250671387, Validation Loss: 0.34909841418266296\n",
      "Epoch 131, Training Loss: 0.354207307100296, Validation Loss: 0.349079966545105\n",
      "Epoch 132, Training Loss: 0.3542031943798065, Validation Loss: 0.34906715154647827\n",
      "Epoch 133, Training Loss: 0.3541952073574066, Validation Loss: 0.34905877709388733\n",
      "Epoch 134, Training Loss: 0.3541826009750366, Validation Loss: 0.3490559160709381\n",
      "Epoch 135, Training Loss: 0.35416877269744873, Validation Loss: 0.3490590453147888\n",
      "Epoch 136, Training Loss: 0.3541576564311981, Validation Loss: 0.3490654528141022\n",
      "Epoch 137, Training Loss: 0.3541504740715027, Validation Loss: 0.3490694761276245\n",
      "Epoch 138, Training Loss: 0.35414502024650574, Validation Loss: 0.34906622767448425\n",
      "Epoch 139, Training Loss: 0.35413825511932373, Validation Loss: 0.3490540683269501\n",
      "Epoch 140, Training Loss: 0.35412880778312683, Validation Loss: 0.3490355312824249\n",
      "Epoch 141, Training Loss: 0.3541180193424225, Validation Loss: 0.3490155339241028\n",
      "Epoch 142, Training Loss: 0.3541082441806793, Validation Loss: 0.34899812936782837\n",
      "Epoch 143, Training Loss: 0.35410064458847046, Validation Loss: 0.3489851951599121\n",
      "Epoch 144, Training Loss: 0.35409435629844666, Validation Loss: 0.3489767014980316\n",
      "Epoch 145, Training Loss: 0.3540877401828766, Validation Loss: 0.3489721417427063\n",
      "Epoch 146, Training Loss: 0.3540797531604767, Validation Loss: 0.3489712178707123\n",
      "Epoch 147, Training Loss: 0.35407087206840515, Validation Loss: 0.3489733934402466\n",
      "Epoch 148, Training Loss: 0.3540622889995575, Validation Loss: 0.34897738695144653\n",
      "Epoch 149, Training Loss: 0.35405486822128296, Validation Loss: 0.3489808142185211\n",
      "Epoch 150, Training Loss: 0.3540482819080353, Validation Loss: 0.3489813208580017\n",
      "Epoch 151, Training Loss: 0.3540416657924652, Validation Loss: 0.34897756576538086\n",
      "Epoch 152, Training Loss: 0.3540343940258026, Validation Loss: 0.3489701747894287\n",
      "Epoch 153, Training Loss: 0.35402655601501465, Validation Loss: 0.3489609658718109\n",
      "Epoch 154, Training Loss: 0.35401883721351624, Validation Loss: 0.348952054977417\n",
      "Epoch 155, Training Loss: 0.3540117144584656, Validation Loss: 0.3489450216293335\n",
      "Epoch 156, Training Loss: 0.3540051281452179, Validation Loss: 0.34894055128097534\n",
      "Epoch 157, Training Loss: 0.353998601436615, Validation Loss: 0.34893864393234253\n",
      "Epoch 158, Training Loss: 0.35399171710014343, Validation Loss: 0.34893903136253357\n",
      "Epoch 159, Training Loss: 0.3539844751358032, Validation Loss: 0.3489413559436798\n",
      "Epoch 160, Training Loss: 0.35397741198539734, Validation Loss: 0.3489444851875305\n",
      "Epoch 161, Training Loss: 0.35397058725357056, Validation Loss: 0.34894728660583496\n",
      "Epoch 162, Training Loss: 0.3539641201496124, Validation Loss: 0.34894853830337524\n",
      "Epoch 163, Training Loss: 0.3539576828479767, Validation Loss: 0.348947674036026\n",
      "Epoch 164, Training Loss: 0.3539510667324066, Validation Loss: 0.34894484281539917\n",
      "Epoch 165, Training Loss: 0.353944331407547, Validation Loss: 0.3489409387111664\n",
      "Epoch 166, Training Loss: 0.35393762588500977, Validation Loss: 0.34893709421157837\n",
      "Epoch 167, Training Loss: 0.35393115878105164, Validation Loss: 0.3489340543746948\n",
      "Epoch 168, Training Loss: 0.35392481088638306, Validation Loss: 0.3489324450492859\n",
      "Epoch 169, Training Loss: 0.35391852259635925, Validation Loss: 0.3489323556423187\n",
      "Epoch 170, Training Loss: 0.3539121747016907, Validation Loss: 0.3489336669445038\n",
      "Epoch 171, Training Loss: 0.3539057672023773, Validation Loss: 0.3489358723163605\n",
      "Epoch 172, Training Loss: 0.35389935970306396, Validation Loss: 0.3489384949207306\n",
      "Epoch 173, Training Loss: 0.35389313101768494, Validation Loss: 0.3489407002925873\n",
      "Epoch 174, Training Loss: 0.3538869619369507, Validation Loss: 0.34894198179244995\n",
      "Epoch 175, Training Loss: 0.3538808822631836, Validation Loss: 0.3489420711994171\n",
      "Epoch 176, Training Loss: 0.35387474298477173, Validation Loss: 0.34894120693206787\n",
      "Epoch 177, Training Loss: 0.3538685739040375, Validation Loss: 0.34893983602523804\n",
      "Epoch 178, Training Loss: 0.353862464427948, Validation Loss: 0.34893861413002014\n",
      "Epoch 179, Training Loss: 0.3538564443588257, Validation Loss: 0.3489379584789276\n",
      "Epoch 180, Training Loss: 0.35385051369667053, Validation Loss: 0.34893807768821716\n",
      "Epoch 181, Training Loss: 0.353844553232193, Validation Loss: 0.34893909096717834\n",
      "Epoch 182, Training Loss: 0.35383862257003784, Validation Loss: 0.348940908908844\n",
      "Epoch 183, Training Loss: 0.3538327217102051, Validation Loss: 0.3489430844783783\n",
      "Epoch 184, Training Loss: 0.3538268208503723, Validation Loss: 0.3489452004432678\n",
      "Epoch 185, Training Loss: 0.3538210093975067, Validation Loss: 0.3489469885826111\n",
      "Epoch 186, Training Loss: 0.3538152277469635, Validation Loss: 0.348948210477829\n",
      "Epoch 187, Training Loss: 0.3538094758987427, Validation Loss: 0.3489486873149872\n",
      "Epoch 188, Training Loss: 0.35380372405052185, Validation Loss: 0.34894874691963196\n",
      "Epoch 189, Training Loss: 0.3537980020046234, Validation Loss: 0.3489486873149872\n",
      "Epoch 190, Training Loss: 0.35379230976104736, Validation Loss: 0.34894874691963196\n",
      "Epoch 191, Training Loss: 0.3537866771221161, Validation Loss: 0.34894922375679016\n",
      "Epoch 192, Training Loss: 0.3537811040878296, Validation Loss: 0.34895020723342896\n",
      "Epoch 193, Training Loss: 0.3537754416465759, Validation Loss: 0.3489516079425812\n",
      "Epoch 194, Training Loss: 0.3537699282169342, Validation Loss: 0.34895339608192444\n",
      "Epoch 195, Training Loss: 0.3537643849849701, Validation Loss: 0.3489551842212677\n",
      "Epoch 196, Training Loss: 0.353758841753006, Validation Loss: 0.3489568531513214\n",
      "Epoch 197, Training Loss: 0.35375338792800903, Validation Loss: 0.34895825386047363\n",
      "Epoch 198, Training Loss: 0.3537479341030121, Validation Loss: 0.3489592373371124\n",
      "Epoch 199, Training Loss: 0.3537425100803375, Validation Loss: 0.34895995259284973\n",
      "Epoch 200, Training Loss: 0.35373711585998535, Validation Loss: 0.3489604592323303\n",
      "Epoch 201, Training Loss: 0.3537317216396332, Validation Loss: 0.3489610254764557\n",
      "Epoch 202, Training Loss: 0.35372641682624817, Validation Loss: 0.3489616811275482\n",
      "Epoch 203, Training Loss: 0.3537210524082184, Validation Loss: 0.348962664604187\n",
      "Epoch 204, Training Loss: 0.35371577739715576, Validation Loss: 0.3489639163017273\n",
      "Epoch 205, Training Loss: 0.35371050238609314, Validation Loss: 0.3489653766155243\n",
      "Epoch 206, Training Loss: 0.3537052869796753, Validation Loss: 0.34896689653396606\n",
      "Epoch 207, Training Loss: 0.35370007157325745, Validation Loss: 0.3489683270454407\n",
      "Epoch 208, Training Loss: 0.3536948561668396, Validation Loss: 0.34896963834762573\n",
      "Epoch 209, Training Loss: 0.35368967056274414, Validation Loss: 0.3489707112312317\n",
      "Epoch 210, Training Loss: 0.35368457436561584, Validation Loss: 0.34897157549858093\n",
      "Epoch 211, Training Loss: 0.3536794185638428, Validation Loss: 0.348972350358963\n",
      "Epoch 212, Training Loss: 0.3536743223667145, Validation Loss: 0.3489730656147003\n",
      "Epoch 213, Training Loss: 0.35366925597190857, Validation Loss: 0.3489739000797272\n",
      "Epoch 214, Training Loss: 0.35366418957710266, Validation Loss: 0.3489748239517212\n",
      "Epoch 215, Training Loss: 0.35365918278694153, Validation Loss: 0.34897592663764954\n",
      "Epoch 216, Training Loss: 0.3536542057991028, Validation Loss: 0.34897714853286743\n",
      "Epoch 217, Training Loss: 0.35364919900894165, Validation Loss: 0.3489784002304077\n",
      "Epoch 218, Training Loss: 0.3536442220211029, Validation Loss: 0.3489796221256256\n",
      "Epoch 219, Training Loss: 0.3536393344402313, Validation Loss: 0.34898072481155396\n",
      "Epoch 220, Training Loss: 0.35363438725471497, Validation Loss: 0.34898173809051514\n",
      "Epoch 221, Training Loss: 0.3536294996738434, Validation Loss: 0.34898263216018677\n",
      "Epoch 222, Training Loss: 0.3536246120929718, Validation Loss: 0.34898337721824646\n",
      "Epoch 223, Training Loss: 0.353619784116745, Validation Loss: 0.34898415207862854\n",
      "Epoch 224, Training Loss: 0.3536149561405182, Validation Loss: 0.3489849269390106\n",
      "Epoch 225, Training Loss: 0.35361015796661377, Validation Loss: 0.34898579120635986\n",
      "Epoch 226, Training Loss: 0.35360538959503174, Validation Loss: 0.34898677468299866\n",
      "Epoch 227, Training Loss: 0.3536006212234497, Validation Loss: 0.3489878177642822\n",
      "Epoch 228, Training Loss: 0.35359588265419006, Validation Loss: 0.34898877143859863\n",
      "Epoch 229, Training Loss: 0.3535911738872528, Validation Loss: 0.3489898145198822\n",
      "Epoch 230, Training Loss: 0.35358649492263794, Validation Loss: 0.348990797996521\n",
      "Epoch 231, Training Loss: 0.35358181595802307, Validation Loss: 0.348991721868515\n",
      "Epoch 232, Training Loss: 0.3535771667957306, Validation Loss: 0.34899255633354187\n",
      "Epoch 233, Training Loss: 0.3535725176334381, Validation Loss: 0.34899330139160156\n",
      "Epoch 234, Training Loss: 0.3535679280757904, Validation Loss: 0.34899410605430603\n",
      "Epoch 235, Training Loss: 0.3535633087158203, Validation Loss: 0.3489949107170105\n",
      "Epoch 236, Training Loss: 0.353558748960495, Validation Loss: 0.34899574518203735\n",
      "Epoch 237, Training Loss: 0.3535541892051697, Validation Loss: 0.3489966094493866\n",
      "Epoch 238, Training Loss: 0.35354968905448914, Validation Loss: 0.34899744391441345\n",
      "Epoch 239, Training Loss: 0.3535451889038086, Validation Loss: 0.34899839758872986\n",
      "Epoch 240, Training Loss: 0.35354068875312805, Validation Loss: 0.3489992916584015\n",
      "Epoch 241, Training Loss: 0.3535362184047699, Validation Loss: 0.34900009632110596\n",
      "Epoch 242, Training Loss: 0.35353174805641174, Validation Loss: 0.3490009009838104\n",
      "Epoch 243, Training Loss: 0.35352733731269836, Validation Loss: 0.3490016758441925\n",
      "Epoch 244, Training Loss: 0.3535229563713074, Validation Loss: 0.3490023910999298\n",
      "Epoch 245, Training Loss: 0.353518545627594, Validation Loss: 0.3490031063556671\n",
      "Epoch 246, Training Loss: 0.3535141944885254, Validation Loss: 0.34900379180908203\n",
      "Epoch 247, Training Loss: 0.3535098433494568, Validation Loss: 0.3490045666694641\n",
      "Epoch 248, Training Loss: 0.35350555181503296, Validation Loss: 0.3490053713321686\n",
      "Epoch 249, Training Loss: 0.35350120067596436, Validation Loss: 0.34900611639022827\n",
      "Epoch 250, Training Loss: 0.3534969091415405, Validation Loss: 0.3490069508552551\n",
      "Epoch 251, Training Loss: 0.3534926474094391, Validation Loss: 0.3490076959133148\n",
      "Epoch 252, Training Loss: 0.35348841547966003, Validation Loss: 0.3490085303783417\n",
      "Epoch 253, Training Loss: 0.3534841537475586, Validation Loss: 0.34900930523872375\n",
      "Epoch 254, Training Loss: 0.3534799814224243, Validation Loss: 0.34901002049446106\n",
      "Epoch 255, Training Loss: 0.35347574949264526, Validation Loss: 0.34901076555252075\n",
      "Epoch 256, Training Loss: 0.3534716069698334, Validation Loss: 0.34901154041290283\n",
      "Epoch 257, Training Loss: 0.3534674644470215, Validation Loss: 0.3490123152732849\n",
      "Epoch 258, Training Loss: 0.3534633219242096, Validation Loss: 0.349013090133667\n",
      "Epoch 259, Training Loss: 0.3534592092037201, Validation Loss: 0.3490138649940491\n",
      "Epoch 260, Training Loss: 0.3534550964832306, Validation Loss: 0.34901461005210876\n",
      "Epoch 261, Training Loss: 0.35345104336738586, Validation Loss: 0.34901535511016846\n",
      "Epoch 262, Training Loss: 0.35344696044921875, Validation Loss: 0.34901610016822815\n",
      "Epoch 263, Training Loss: 0.3534429371356964, Validation Loss: 0.34901684522628784\n",
      "Epoch 264, Training Loss: 0.3534389138221741, Validation Loss: 0.34901753067970276\n",
      "Epoch 265, Training Loss: 0.35343489050865173, Validation Loss: 0.34901824593544006\n",
      "Epoch 266, Training Loss: 0.35343092679977417, Validation Loss: 0.349018931388855\n",
      "Epoch 267, Training Loss: 0.3534269630908966, Validation Loss: 0.3490196764469147\n",
      "Epoch 268, Training Loss: 0.35342302918434143, Validation Loss: 0.3490203022956848\n",
      "Epoch 269, Training Loss: 0.35341906547546387, Validation Loss: 0.3490210175514221\n",
      "Epoch 270, Training Loss: 0.35341519117355347, Validation Loss: 0.34902170300483704\n",
      "Epoch 271, Training Loss: 0.3534112870693207, Validation Loss: 0.34902238845825195\n",
      "Epoch 272, Training Loss: 0.3534073829650879, Validation Loss: 0.3490230143070221\n",
      "Epoch 273, Training Loss: 0.3534035384654999, Validation Loss: 0.349023699760437\n",
      "Epoch 274, Training Loss: 0.35339969396591187, Validation Loss: 0.34902435541152954\n",
      "Epoch 275, Training Loss: 0.35339587926864624, Validation Loss: 0.34902504086494446\n",
      "Epoch 276, Training Loss: 0.3533920645713806, Validation Loss: 0.3490256369113922\n",
      "Epoch 277, Training Loss: 0.35338830947875977, Validation Loss: 0.34902626276016235\n",
      "Epoch 278, Training Loss: 0.35338449478149414, Validation Loss: 0.3490268588066101\n",
      "Epoch 279, Training Loss: 0.3533807694911957, Validation Loss: 0.3490274250507355\n",
      "Epoch 280, Training Loss: 0.3533770442008972, Validation Loss: 0.34902799129486084\n",
      "Epoch 281, Training Loss: 0.35337331891059875, Validation Loss: 0.3490285873413086\n",
      "Epoch 282, Training Loss: 0.3533695936203003, Validation Loss: 0.34902915358543396\n",
      "Epoch 283, Training Loss: 0.3533659279346466, Validation Loss: 0.3490298092365265\n",
      "Epoch 284, Training Loss: 0.3533622920513153, Validation Loss: 0.34903040528297424\n",
      "Epoch 285, Training Loss: 0.3533586263656616, Validation Loss: 0.3490310311317444\n",
      "Epoch 286, Training Loss: 0.3533550202846527, Validation Loss: 0.34903162717819214\n",
      "Epoch 287, Training Loss: 0.3533514142036438, Validation Loss: 0.3490321934223175\n",
      "Epoch 288, Training Loss: 0.3533477783203125, Validation Loss: 0.34903275966644287\n",
      "Epoch 289, Training Loss: 0.353344202041626, Validation Loss: 0.34903332591056824\n",
      "Epoch 290, Training Loss: 0.35334065556526184, Validation Loss: 0.3490338921546936\n",
      "Epoch 291, Training Loss: 0.3533371090888977, Validation Loss: 0.3490343689918518\n",
      "Epoch 292, Training Loss: 0.35333362221717834, Validation Loss: 0.34903496503829956\n",
      "Epoch 293, Training Loss: 0.3533300757408142, Validation Loss: 0.34903550148010254\n",
      "Epoch 294, Training Loss: 0.35332655906677246, Validation Loss: 0.3490360677242279\n",
      "Epoch 295, Training Loss: 0.3533230721950531, Validation Loss: 0.3490366041660309\n",
      "Epoch 296, Training Loss: 0.3533196449279785, Validation Loss: 0.34903717041015625\n",
      "Epoch 297, Training Loss: 0.35331618785858154, Validation Loss: 0.34903767704963684\n",
      "Epoch 298, Training Loss: 0.35331276059150696, Validation Loss: 0.34903815388679504\n",
      "Epoch 299, Training Loss: 0.35330936312675476, Validation Loss: 0.34903866052627563\n",
      "Epoch 300, Training Loss: 0.3533059358596802, Validation Loss: 0.34903910756111145\n",
      "Epoch 301, Training Loss: 0.35330256819725037, Validation Loss: 0.34903955459594727\n",
      "Epoch 302, Training Loss: 0.35329920053482056, Validation Loss: 0.34904006123542786\n",
      "Epoch 303, Training Loss: 0.35329586267471313, Validation Loss: 0.34904053807258606\n",
      "Epoch 304, Training Loss: 0.3532925248146057, Validation Loss: 0.3490409851074219\n",
      "Epoch 305, Training Loss: 0.3532892167568207, Validation Loss: 0.3490414321422577\n",
      "Epoch 306, Training Loss: 0.35328590869903564, Validation Loss: 0.3490419089794159\n",
      "Epoch 307, Training Loss: 0.3532826006412506, Validation Loss: 0.3490423262119293\n",
      "Epoch 308, Training Loss: 0.35327935218811035, Validation Loss: 0.34904274344444275\n",
      "Epoch 309, Training Loss: 0.3532761037349701, Validation Loss: 0.3490431010723114\n",
      "Epoch 310, Training Loss: 0.35327285528182983, Validation Loss: 0.34904348850250244\n",
      "Epoch 311, Training Loss: 0.35326963663101196, Validation Loss: 0.3490438759326935\n",
      "Epoch 312, Training Loss: 0.3532664179801941, Validation Loss: 0.34904420375823975\n",
      "Epoch 313, Training Loss: 0.3532632291316986, Validation Loss: 0.3490445017814636\n",
      "Epoch 314, Training Loss: 0.3532600402832031, Validation Loss: 0.34904488921165466\n",
      "Epoch 315, Training Loss: 0.35325688123703003, Validation Loss: 0.34904518723487854\n",
      "Epoch 316, Training Loss: 0.3532537519931793, Validation Loss: 0.3490455448627472\n",
      "Epoch 317, Training Loss: 0.3532505929470062, Validation Loss: 0.34904584288597107\n",
      "Epoch 318, Training Loss: 0.3532474935054779, Validation Loss: 0.3490462303161621\n",
      "Epoch 319, Training Loss: 0.3532443940639496, Validation Loss: 0.3490465581417084\n",
      "Epoch 320, Training Loss: 0.3532412648200989, Validation Loss: 0.34904682636260986\n",
      "Epoch 321, Training Loss: 0.35323819518089294, Validation Loss: 0.34904712438583374\n",
      "Epoch 322, Training Loss: 0.353235125541687, Validation Loss: 0.34904739260673523\n",
      "Epoch 323, Training Loss: 0.35323208570480347, Validation Loss: 0.3490476906299591\n",
      "Epoch 324, Training Loss: 0.3532290756702423, Validation Loss: 0.349047988653183\n",
      "Epoch 325, Training Loss: 0.35322603583335876, Validation Loss: 0.3490482270717621\n",
      "Epoch 326, Training Loss: 0.3532230257987976, Validation Loss: 0.34904852509498596\n",
      "Epoch 327, Training Loss: 0.35322001576423645, Validation Loss: 0.34904882311820984\n",
      "Epoch 328, Training Loss: 0.3532170355319977, Validation Loss: 0.34904909133911133\n",
      "Epoch 329, Training Loss: 0.3532140851020813, Validation Loss: 0.34904927015304565\n",
      "Epoch 330, Training Loss: 0.3532111346721649, Validation Loss: 0.34904953837394714\n",
      "Epoch 331, Training Loss: 0.35320818424224854, Validation Loss: 0.34904971718788147\n",
      "Epoch 332, Training Loss: 0.35320523381233215, Validation Loss: 0.34904995560646057\n",
      "Epoch 333, Training Loss: 0.35320231318473816, Validation Loss: 0.3490501344203949\n",
      "Epoch 334, Training Loss: 0.35319945216178894, Validation Loss: 0.3490503430366516\n",
      "Epoch 335, Training Loss: 0.35319650173187256, Validation Loss: 0.34905052185058594\n",
      "Epoch 336, Training Loss: 0.3531936705112457, Validation Loss: 0.34905070066452026\n",
      "Epoch 337, Training Loss: 0.3531907796859741, Validation Loss: 0.3490508794784546\n",
      "Epoch 338, Training Loss: 0.3531879484653473, Validation Loss: 0.34905102849006653\n",
      "Epoch 339, Training Loss: 0.3531850576400757, Validation Loss: 0.34905120730400085\n",
      "Epoch 340, Training Loss: 0.35318225622177124, Validation Loss: 0.349051296710968\n",
      "Epoch 341, Training Loss: 0.3531794250011444, Validation Loss: 0.34905141592025757\n",
      "Epoch 342, Training Loss: 0.35317662358283997, Validation Loss: 0.3490515351295471\n",
      "Epoch 343, Training Loss: 0.3531738221645355, Validation Loss: 0.3490516245365143\n",
      "Epoch 344, Training Loss: 0.35317105054855347, Validation Loss: 0.34905174374580383\n",
      "Epoch 345, Training Loss: 0.3531682789325714, Validation Loss: 0.349051833152771\n",
      "Epoch 346, Training Loss: 0.35316550731658936, Validation Loss: 0.34905189275741577\n",
      "Epoch 347, Training Loss: 0.3531627655029297, Validation Loss: 0.34905198216438293\n",
      "Epoch 348, Training Loss: 0.35316002368927, Validation Loss: 0.3490520417690277\n",
      "Epoch 349, Training Loss: 0.35315728187561035, Validation Loss: 0.3490521311759949\n",
      "Epoch 350, Training Loss: 0.35315459966659546, Validation Loss: 0.34905216097831726\n",
      "Epoch 351, Training Loss: 0.3531518876552582, Validation Loss: 0.34905222058296204\n",
      "Epoch 352, Training Loss: 0.3531492054462433, Validation Loss: 0.3490522503852844\n",
      "Epoch 353, Training Loss: 0.353146493434906, Validation Loss: 0.3490522801876068\n",
      "Epoch 354, Training Loss: 0.3531438410282135, Validation Loss: 0.3490523099899292\n",
      "Epoch 355, Training Loss: 0.3531411588191986, Validation Loss: 0.3490523397922516\n",
      "Epoch 356, Training Loss: 0.3531385064125061, Validation Loss: 0.34905239939689636\n",
      "Epoch 357, Training Loss: 0.3531358540058136, Validation Loss: 0.34905239939689636\n",
      "Epoch 358, Training Loss: 0.35313326120376587, Validation Loss: 0.34905245900154114\n",
      "Epoch 359, Training Loss: 0.35313063859939575, Validation Loss: 0.34905242919921875\n",
      "Epoch 360, Training Loss: 0.35312801599502563, Validation Loss: 0.34905245900154114\n",
      "Epoch 361, Training Loss: 0.3531254231929779, Validation Loss: 0.3490525186061859\n",
      "Epoch 362, Training Loss: 0.3531228303909302, Validation Loss: 0.3490525484085083\n",
      "Epoch 363, Training Loss: 0.35312023758888245, Validation Loss: 0.3490525782108307\n",
      "Epoch 364, Training Loss: 0.3531176745891571, Validation Loss: 0.3490526080131531\n",
      "Epoch 365, Training Loss: 0.35311514139175415, Validation Loss: 0.3490526080131531\n",
      "Epoch 366, Training Loss: 0.3531125783920288, Validation Loss: 0.3490526080131531\n",
      "Epoch 367, Training Loss: 0.35311004519462585, Validation Loss: 0.34905263781547546\n",
      "Epoch 368, Training Loss: 0.3531075417995453, Validation Loss: 0.3490526080131531\n",
      "Epoch 369, Training Loss: 0.35310497879981995, Validation Loss: 0.3490525782108307\n",
      "Epoch 370, Training Loss: 0.3531024754047394, Validation Loss: 0.3490525484085083\n",
      "Epoch 371, Training Loss: 0.3531000018119812, Validation Loss: 0.3490525186061859\n",
      "Epoch 372, Training Loss: 0.35309749841690063, Validation Loss: 0.3490524888038635\n",
      "Epoch 373, Training Loss: 0.35309499502182007, Validation Loss: 0.3490524888038635\n",
      "Epoch 374, Training Loss: 0.3530925214290619, Validation Loss: 0.34905242919921875\n",
      "Epoch 375, Training Loss: 0.3530900776386261, Validation Loss: 0.3490523397922516\n",
      "Epoch 376, Training Loss: 0.3530876338481903, Validation Loss: 0.3490522503852844\n",
      "Epoch 377, Training Loss: 0.3530851900577545, Validation Loss: 0.34905219078063965\n",
      "Epoch 378, Training Loss: 0.3530827462673187, Validation Loss: 0.3490521311759949\n",
      "Epoch 379, Training Loss: 0.35308030247688293, Validation Loss: 0.3490520119667053\n",
      "Epoch 380, Training Loss: 0.35307785868644714, Validation Loss: 0.34905195236206055\n",
      "Epoch 381, Training Loss: 0.35307547450065613, Validation Loss: 0.34905189275741577\n",
      "Epoch 382, Training Loss: 0.3530730605125427, Validation Loss: 0.3490518033504486\n",
      "Epoch 383, Training Loss: 0.3530706763267517, Validation Loss: 0.34905171394348145\n",
      "Epoch 384, Training Loss: 0.3530682921409607, Validation Loss: 0.3490515947341919\n",
      "Epoch 385, Training Loss: 0.3530659079551697, Validation Loss: 0.34905147552490234\n",
      "Epoch 386, Training Loss: 0.35306352376937866, Validation Loss: 0.3490513563156128\n",
      "Epoch 387, Training Loss: 0.35306113958358765, Validation Loss: 0.34905126690864563\n",
      "Epoch 388, Training Loss: 0.353058785200119, Validation Loss: 0.3490511178970337\n",
      "Epoch 389, Training Loss: 0.3530564606189728, Validation Loss: 0.34905096888542175\n",
      "Epoch 390, Training Loss: 0.35305413603782654, Validation Loss: 0.3490507900714874\n",
      "Epoch 391, Training Loss: 0.3530517518520355, Validation Loss: 0.3490506410598755\n",
      "Epoch 392, Training Loss: 0.3530494272708893, Validation Loss: 0.34905049204826355\n",
      "Epoch 393, Training Loss: 0.3530471622943878, Validation Loss: 0.3490503132343292\n",
      "Epoch 394, Training Loss: 0.3530448079109192, Validation Loss: 0.3490501344203949\n",
      "Epoch 395, Training Loss: 0.35304248332977295, Validation Loss: 0.34904995560646057\n",
      "Epoch 396, Training Loss: 0.3530402183532715, Validation Loss: 0.34904974699020386\n",
      "Epoch 397, Training Loss: 0.35303792357444763, Validation Loss: 0.34904953837394714\n",
      "Epoch 398, Training Loss: 0.35303565859794617, Validation Loss: 0.34904932975769043\n",
      "Epoch 399, Training Loss: 0.3530333638191223, Validation Loss: 0.3490491509437561\n",
      "Epoch 400, Training Loss: 0.35303109884262085, Validation Loss: 0.3490489721298218\n",
      "Epoch 401, Training Loss: 0.353028804063797, Validation Loss: 0.34904879331588745\n",
      "Epoch 402, Training Loss: 0.3530265986919403, Validation Loss: 0.34904858469963074\n",
      "Epoch 403, Training Loss: 0.35302436351776123, Validation Loss: 0.34904834628105164\n",
      "Epoch 404, Training Loss: 0.35302209854125977, Validation Loss: 0.34904810786247253\n",
      "Epoch 405, Training Loss: 0.3530198633670807, Validation Loss: 0.34904786944389343\n",
      "Epoch 406, Training Loss: 0.3530176281929016, Validation Loss: 0.3490476608276367\n",
      "Epoch 407, Training Loss: 0.3530154228210449, Validation Loss: 0.34904739260673523\n",
      "Epoch 408, Training Loss: 0.35301321744918823, Validation Loss: 0.34904712438583374\n",
      "Epoch 409, Training Loss: 0.35301101207733154, Validation Loss: 0.3490469455718994\n",
      "Epoch 410, Training Loss: 0.35300883650779724, Validation Loss: 0.3490467071533203\n",
      "Epoch 411, Training Loss: 0.35300666093826294, Validation Loss: 0.349046528339386\n",
      "Epoch 412, Training Loss: 0.35300445556640625, Validation Loss: 0.34904634952545166\n",
      "Epoch 413, Training Loss: 0.35300227999687195, Validation Loss: 0.34904611110687256\n",
      "Epoch 414, Training Loss: 0.35300007462501526, Validation Loss: 0.34904584288597107\n",
      "Epoch 415, Training Loss: 0.35299792885780334, Validation Loss: 0.34904569387435913\n",
      "Epoch 416, Training Loss: 0.35299578309059143, Validation Loss: 0.34904545545578003\n",
      "Epoch 417, Training Loss: 0.3529936671257019, Validation Loss: 0.3490452766418457\n",
      "Epoch 418, Training Loss: 0.35299152135849, Validation Loss: 0.3490450978279114\n",
      "Epoch 419, Training Loss: 0.3529893755912781, Validation Loss: 0.34904488921165466\n",
      "Epoch 420, Training Loss: 0.35298725962638855, Validation Loss: 0.34904465079307556\n",
      "Epoch 421, Training Loss: 0.35298511385917664, Validation Loss: 0.3490445017814636\n",
      "Epoch 422, Training Loss: 0.3529829978942871, Validation Loss: 0.3490443229675293\n",
      "Epoch 423, Training Loss: 0.35298091173171997, Validation Loss: 0.34904414415359497\n",
      "Epoch 424, Training Loss: 0.35297879576683044, Validation Loss: 0.34904396533966064\n",
      "Epoch 425, Training Loss: 0.3529767394065857, Validation Loss: 0.3490437865257263\n",
      "Epoch 426, Training Loss: 0.35297465324401855, Validation Loss: 0.34904366731643677\n",
      "Epoch 427, Training Loss: 0.35297253727912903, Validation Loss: 0.34904348850250244\n",
      "Epoch 428, Training Loss: 0.3529704809188843, Validation Loss: 0.3490433394908905\n",
      "Epoch 429, Training Loss: 0.35296839475631714, Validation Loss: 0.3490431606769562\n",
      "Epoch 430, Training Loss: 0.3529663383960724, Validation Loss: 0.34904301166534424\n",
      "Epoch 431, Training Loss: 0.35296428203582764, Validation Loss: 0.3490428030490875\n",
      "Epoch 432, Training Loss: 0.3529622554779053, Validation Loss: 0.3490426540374756\n",
      "Epoch 433, Training Loss: 0.35296016931533813, Validation Loss: 0.34904244542121887\n",
      "Epoch 434, Training Loss: 0.35295814275741577, Validation Loss: 0.3490423262119293\n",
      "Epoch 435, Training Loss: 0.3529561161994934, Validation Loss: 0.3490421175956726\n",
      "Epoch 436, Training Loss: 0.35295411944389343, Validation Loss: 0.3490419387817383\n",
      "Epoch 437, Training Loss: 0.35295209288597107, Validation Loss: 0.34904173016548157\n",
      "Epoch 438, Training Loss: 0.3529500961303711, Validation Loss: 0.34904152154922485\n",
      "Epoch 439, Training Loss: 0.35294806957244873, Validation Loss: 0.3490413427352905\n",
      "Epoch 440, Training Loss: 0.35294607281684875, Validation Loss: 0.3490411341190338\n",
      "Epoch 441, Training Loss: 0.3529440760612488, Validation Loss: 0.3490408957004547\n",
      "Epoch 442, Training Loss: 0.3529421091079712, Validation Loss: 0.34904059767723083\n",
      "Epoch 443, Training Loss: 0.3529401123523712, Validation Loss: 0.3490404188632965\n",
      "Epoch 444, Training Loss: 0.35293811559677124, Validation Loss: 0.3490401804447174\n",
      "Epoch 445, Training Loss: 0.35293614864349365, Validation Loss: 0.3490399420261383\n",
      "Epoch 446, Training Loss: 0.35293418169021606, Validation Loss: 0.3490397036075592\n",
      "Epoch 447, Training Loss: 0.35293224453926086, Validation Loss: 0.3490394353866577\n",
      "Epoch 448, Training Loss: 0.3529302477836609, Validation Loss: 0.3490391969680786\n",
      "Epoch 449, Training Loss: 0.3529283404350281, Validation Loss: 0.3490389287471771\n",
      "Epoch 450, Training Loss: 0.3529263734817505, Validation Loss: 0.34903866052627563\n",
      "Epoch 451, Training Loss: 0.3529244661331177, Validation Loss: 0.34903836250305176\n",
      "Epoch 452, Training Loss: 0.3529225289821625, Validation Loss: 0.34903809428215027\n",
      "Epoch 453, Training Loss: 0.3529205918312073, Validation Loss: 0.3490377962589264\n",
      "Epoch 454, Training Loss: 0.35291868448257446, Validation Loss: 0.3490375578403473\n",
      "Epoch 455, Training Loss: 0.35291677713394165, Validation Loss: 0.3490373194217682\n",
      "Epoch 456, Training Loss: 0.35291483998298645, Validation Loss: 0.34903693199157715\n",
      "Epoch 457, Training Loss: 0.352912962436676, Validation Loss: 0.34903669357299805\n",
      "Epoch 458, Training Loss: 0.3529110848903656, Validation Loss: 0.3490363657474518\n",
      "Epoch 459, Training Loss: 0.3529091775417328, Validation Loss: 0.3490360975265503\n",
      "Epoch 460, Training Loss: 0.35290729999542236, Validation Loss: 0.3490357995033264\n",
      "Epoch 461, Training Loss: 0.3529054522514343, Validation Loss: 0.3490355610847473\n",
      "Epoch 462, Training Loss: 0.3529035747051239, Validation Loss: 0.3490353524684906\n",
      "Epoch 463, Training Loss: 0.3529016971588135, Validation Loss: 0.3490351140499115\n",
      "Epoch 464, Training Loss: 0.35289984941482544, Validation Loss: 0.3490348160266876\n",
      "Epoch 465, Training Loss: 0.3528980016708374, Validation Loss: 0.3490346074104309\n",
      "Epoch 466, Training Loss: 0.35289615392684937, Validation Loss: 0.34903430938720703\n",
      "Epoch 467, Training Loss: 0.3528943359851837, Validation Loss: 0.34903407096862793\n",
      "Epoch 468, Training Loss: 0.3528924882411957, Validation Loss: 0.3490338921546936\n",
      "Epoch 469, Training Loss: 0.35289067029953003, Validation Loss: 0.3490336537361145\n",
      "Epoch 470, Training Loss: 0.3528888523578644, Validation Loss: 0.3490334749221802\n",
      "Epoch 471, Training Loss: 0.3528870642185211, Validation Loss: 0.3490332067012787\n",
      "Epoch 472, Training Loss: 0.35288524627685547, Validation Loss: 0.34903302788734436\n",
      "Epoch 473, Training Loss: 0.3528834581375122, Validation Loss: 0.34903278946876526\n",
      "Epoch 474, Training Loss: 0.35288166999816895, Validation Loss: 0.34903261065483093\n",
      "Epoch 475, Training Loss: 0.3528798818588257, Validation Loss: 0.3490324020385742\n",
      "Epoch 476, Training Loss: 0.3528780937194824, Validation Loss: 0.3490321934223175\n",
      "Epoch 477, Training Loss: 0.35287633538246155, Validation Loss: 0.3490319848060608\n",
      "Epoch 478, Training Loss: 0.3528745770454407, Validation Loss: 0.3490317761898041\n",
      "Epoch 479, Training Loss: 0.3528728187084198, Validation Loss: 0.34903159737586975\n",
      "Epoch 480, Training Loss: 0.35287103056907654, Validation Loss: 0.34903138875961304\n",
      "Epoch 481, Training Loss: 0.35286930203437805, Validation Loss: 0.3490311801433563\n",
      "Epoch 482, Training Loss: 0.35286757349967957, Validation Loss: 0.349031001329422\n",
      "Epoch 483, Training Loss: 0.3528658449649811, Validation Loss: 0.3490307331085205\n",
      "Epoch 484, Training Loss: 0.3528640866279602, Validation Loss: 0.3490305542945862\n",
      "Epoch 485, Training Loss: 0.3528623580932617, Validation Loss: 0.34903034567832947\n",
      "Epoch 486, Training Loss: 0.352860689163208, Validation Loss: 0.34903013706207275\n",
      "Epoch 487, Training Loss: 0.35285893082618713, Validation Loss: 0.34902989864349365\n",
      "Epoch 488, Training Loss: 0.35285723209381104, Validation Loss: 0.34902969002723694\n",
      "Epoch 489, Training Loss: 0.35285553336143494, Validation Loss: 0.34902942180633545\n",
      "Epoch 490, Training Loss: 0.35285383462905884, Validation Loss: 0.34902921319007874\n",
      "Epoch 491, Training Loss: 0.3528521656990051, Validation Loss: 0.34902894496917725\n",
      "Epoch 492, Training Loss: 0.35285046696662903, Validation Loss: 0.34902870655059814\n",
      "Epoch 493, Training Loss: 0.3528487980365753, Validation Loss: 0.34902849793434143\n",
      "Epoch 494, Training Loss: 0.3528471291065216, Validation Loss: 0.3490282893180847\n",
      "Epoch 495, Training Loss: 0.3528454899787903, Validation Loss: 0.349028080701828\n",
      "Epoch 496, Training Loss: 0.3528438210487366, Validation Loss: 0.3490278720855713\n",
      "Epoch 497, Training Loss: 0.35284218192100525, Validation Loss: 0.3490276634693146\n",
      "Epoch 498, Training Loss: 0.35284051299095154, Validation Loss: 0.3490274250507355\n",
      "Epoch 499, Training Loss: 0.3528388738632202, Validation Loss: 0.34902718663215637\n",
      "Epoch 500, Training Loss: 0.3528372347354889, Validation Loss: 0.3490268886089325\n",
      "Epoch 501, Training Loss: 0.35283562541007996, Validation Loss: 0.3490266799926758\n",
      "Epoch 502, Training Loss: 0.35283398628234863, Validation Loss: 0.3490264117717743\n",
      "Epoch 503, Training Loss: 0.3528323769569397, Validation Loss: 0.3490262031555176\n",
      "Epoch 504, Training Loss: 0.35283079743385315, Validation Loss: 0.34902602434158325\n",
      "Epoch 505, Training Loss: 0.3528291881084442, Validation Loss: 0.3490258455276489\n",
      "Epoch 506, Training Loss: 0.3528275787830353, Validation Loss: 0.34902557730674744\n",
      "Epoch 507, Training Loss: 0.35282599925994873, Validation Loss: 0.34902533888816833\n",
      "Epoch 508, Training Loss: 0.35282444953918457, Validation Loss: 0.34902510046958923\n",
      "Epoch 509, Training Loss: 0.352822870016098, Validation Loss: 0.3490249216556549\n",
      "Epoch 510, Training Loss: 0.3528212904930115, Validation Loss: 0.3490247428417206\n",
      "Epoch 511, Training Loss: 0.3528197109699249, Validation Loss: 0.34902456402778625\n",
      "Epoch 512, Training Loss: 0.35281816124916077, Validation Loss: 0.34902435541152954\n",
      "Epoch 513, Training Loss: 0.352816641330719, Validation Loss: 0.3490241765975952\n",
      "Epoch 514, Training Loss: 0.35281509160995483, Validation Loss: 0.3490239381790161\n",
      "Epoch 515, Training Loss: 0.3528135418891907, Validation Loss: 0.3490237295627594\n",
      "Epoch 516, Training Loss: 0.3528120219707489, Validation Loss: 0.3490234911441803\n",
      "Epoch 517, Training Loss: 0.35281050205230713, Validation Loss: 0.34902334213256836\n",
      "Epoch 518, Training Loss: 0.35280898213386536, Validation Loss: 0.34902310371398926\n",
      "Epoch 519, Training Loss: 0.3528074622154236, Validation Loss: 0.34902292490005493\n",
      "Epoch 520, Training Loss: 0.3528060019016266, Validation Loss: 0.3490227162837982\n",
      "Epoch 521, Training Loss: 0.3528044819831848, Validation Loss: 0.3490225374698639\n",
      "Epoch 522, Training Loss: 0.35280299186706543, Validation Loss: 0.3490222990512848\n",
      "Epoch 523, Training Loss: 0.35280150175094604, Validation Loss: 0.3490220904350281\n",
      "Epoch 524, Training Loss: 0.35280001163482666, Validation Loss: 0.34902188181877136\n",
      "Epoch 525, Training Loss: 0.3527985215187073, Validation Loss: 0.34902167320251465\n",
      "Epoch 526, Training Loss: 0.35279709100723267, Validation Loss: 0.34902143478393555\n",
      "Epoch 527, Training Loss: 0.3527956008911133, Validation Loss: 0.3490212559700012\n",
      "Epoch 528, Training Loss: 0.3527941405773163, Validation Loss: 0.3490210771560669\n",
      "Epoch 529, Training Loss: 0.3527927100658417, Validation Loss: 0.3490208685398102\n",
      "Epoch 530, Training Loss: 0.3527912497520447, Validation Loss: 0.34902065992355347\n",
      "Epoch 531, Training Loss: 0.35278981924057007, Validation Loss: 0.349020391702652\n",
      "Epoch 532, Training Loss: 0.35278841853141785, Validation Loss: 0.3490201532840729\n",
      "Epoch 533, Training Loss: 0.3527870178222656, Validation Loss: 0.3490199148654938\n",
      "Epoch 534, Training Loss: 0.352785587310791, Validation Loss: 0.34901970624923706\n",
      "Epoch 535, Training Loss: 0.3527841567993164, Validation Loss: 0.34901949763298035\n",
      "Epoch 536, Training Loss: 0.3527827858924866, Validation Loss: 0.34901928901672363\n",
      "Epoch 537, Training Loss: 0.35278135538101196, Validation Loss: 0.3490191102027893\n",
      "Epoch 538, Training Loss: 0.35277998447418213, Validation Loss: 0.349018931388855\n",
      "Epoch 539, Training Loss: 0.3527785837650299, Validation Loss: 0.34901875257492065\n",
      "Epoch 540, Training Loss: 0.3527772128582001, Validation Loss: 0.34901857376098633\n",
      "Epoch 541, Training Loss: 0.35277584195137024, Validation Loss: 0.3490183651447296\n",
      "Epoch 542, Training Loss: 0.3527745008468628, Validation Loss: 0.3490181565284729\n",
      "Epoch 543, Training Loss: 0.35277312994003296, Validation Loss: 0.3490179479122162\n",
      "Epoch 544, Training Loss: 0.3527717888355255, Validation Loss: 0.34901776909828186\n",
      "Epoch 545, Training Loss: 0.3527704179286957, Validation Loss: 0.34901759028434753\n",
      "Epoch 546, Training Loss: 0.3527691066265106, Validation Loss: 0.3490174114704132\n",
      "Epoch 547, Training Loss: 0.3527677655220032, Validation Loss: 0.3490172326564789\n",
      "Epoch 548, Training Loss: 0.3527664542198181, Validation Loss: 0.34901705384254456\n",
      "Epoch 549, Training Loss: 0.35276511311531067, Validation Loss: 0.34901684522628784\n",
      "Epoch 550, Training Loss: 0.3527637720108032, Validation Loss: 0.3490166664123535\n",
      "Epoch 551, Training Loss: 0.35276252031326294, Validation Loss: 0.3490164577960968\n",
      "Epoch 552, Training Loss: 0.3527612090110779, Validation Loss: 0.3490162491798401\n",
      "Epoch 553, Training Loss: 0.3527599275112152, Validation Loss: 0.34901610016822815\n",
      "Epoch 554, Training Loss: 0.35275861620903015, Validation Loss: 0.3490159511566162\n",
      "Epoch 555, Training Loss: 0.3527573347091675, Validation Loss: 0.34901583194732666\n",
      "Epoch 556, Training Loss: 0.3527560532093048, Validation Loss: 0.34901565313339233\n",
      "Epoch 557, Training Loss: 0.3527548015117645, Validation Loss: 0.3490154445171356\n",
      "Epoch 558, Training Loss: 0.35275352001190186, Validation Loss: 0.3490152955055237\n",
      "Epoch 559, Training Loss: 0.3527522683143616, Validation Loss: 0.34901514649391174\n",
      "Epoch 560, Training Loss: 0.35275107622146606, Validation Loss: 0.3490149676799774\n",
      "Epoch 561, Training Loss: 0.3527497947216034, Validation Loss: 0.34901484847068787\n",
      "Epoch 562, Training Loss: 0.3527485728263855, Validation Loss: 0.3490147292613983\n",
      "Epoch 563, Training Loss: 0.3527473509311676, Validation Loss: 0.349014550447464\n",
      "Epoch 564, Training Loss: 0.3527461290359497, Validation Loss: 0.34901443123817444\n",
      "Epoch 565, Training Loss: 0.3527449071407318, Validation Loss: 0.3490143120288849\n",
      "Epoch 566, Training Loss: 0.3527437150478363, Validation Loss: 0.3490141034126282\n",
      "Epoch 567, Training Loss: 0.3527425229549408, Validation Loss: 0.34901392459869385\n",
      "Epoch 568, Training Loss: 0.3527413308620453, Validation Loss: 0.3490138053894043\n",
      "Epoch 569, Training Loss: 0.3527401089668274, Validation Loss: 0.34901362657546997\n",
      "Epoch 570, Training Loss: 0.3527389466762543, Validation Loss: 0.34901344776153564\n",
      "Epoch 571, Training Loss: 0.35273775458335876, Validation Loss: 0.3490133285522461\n",
      "Epoch 572, Training Loss: 0.35273656249046326, Validation Loss: 0.34901317954063416\n",
      "Epoch 573, Training Loss: 0.35273540019989014, Validation Loss: 0.3490130305290222\n",
      "Epoch 574, Training Loss: 0.3527342677116394, Validation Loss: 0.3490128219127655\n",
      "Epoch 575, Training Loss: 0.3527331054210663, Validation Loss: 0.34901267290115356\n",
      "Epoch 576, Training Loss: 0.35273194313049316, Validation Loss: 0.34901246428489685\n",
      "Epoch 577, Training Loss: 0.35273081064224243, Validation Loss: 0.3490123152732849\n",
      "Epoch 578, Training Loss: 0.3527296483516693, Validation Loss: 0.349012166261673\n",
      "Epoch 579, Training Loss: 0.35272854566574097, Validation Loss: 0.34901198744773865\n",
      "Epoch 580, Training Loss: 0.3527274429798126, Validation Loss: 0.3490118384361267\n",
      "Epoch 581, Training Loss: 0.3527262806892395, Validation Loss: 0.3490116000175476\n",
      "Epoch 582, Training Loss: 0.35272520780563354, Validation Loss: 0.3490114212036133\n",
      "Epoch 583, Training Loss: 0.3527240753173828, Validation Loss: 0.34901127219200134\n",
      "Epoch 584, Training Loss: 0.35272297263145447, Validation Loss: 0.3490111231803894\n",
      "Epoch 585, Training Loss: 0.3527218997478485, Validation Loss: 0.34901097416877747\n",
      "Epoch 586, Training Loss: 0.35272082686424255, Validation Loss: 0.3490108251571655\n",
      "Epoch 587, Training Loss: 0.3527197241783142, Validation Loss: 0.3490106463432312\n",
      "Epoch 588, Training Loss: 0.35271865129470825, Validation Loss: 0.3490104675292969\n",
      "Epoch 589, Training Loss: 0.3527175784111023, Validation Loss: 0.3490102291107178\n",
      "Epoch 590, Training Loss: 0.35271650552749634, Validation Loss: 0.34901008009910583\n",
      "Epoch 591, Training Loss: 0.35271546244621277, Validation Loss: 0.3490099608898163\n",
      "Epoch 592, Training Loss: 0.3527144193649292, Validation Loss: 0.34900984168052673\n",
      "Epoch 593, Training Loss: 0.35271337628364563, Validation Loss: 0.3490097224712372\n",
      "Epoch 594, Training Loss: 0.35271236300468445, Validation Loss: 0.34900960326194763\n",
      "Epoch 595, Training Loss: 0.3527113199234009, Validation Loss: 0.3490094542503357\n",
      "Epoch 596, Training Loss: 0.3527102768421173, Validation Loss: 0.34900930523872375\n",
      "Epoch 597, Training Loss: 0.35270926356315613, Validation Loss: 0.3490091562271118\n",
      "Epoch 598, Training Loss: 0.35270822048187256, Validation Loss: 0.34900903701782227\n",
      "Epoch 599, Training Loss: 0.35270726680755615, Validation Loss: 0.3490088880062103\n",
      "Epoch 600, Training Loss: 0.35270625352859497, Validation Loss: 0.34900879859924316\n",
      "Epoch 601, Training Loss: 0.3527052104473114, Validation Loss: 0.3490086793899536\n",
      "Epoch 602, Training Loss: 0.3527042269706726, Validation Loss: 0.34900856018066406\n",
      "Epoch 603, Training Loss: 0.3527032732963562, Validation Loss: 0.34900838136672974\n",
      "Epoch 604, Training Loss: 0.352702260017395, Validation Loss: 0.3490082323551178\n",
      "Epoch 605, Training Loss: 0.3527012765407562, Validation Loss: 0.34900811314582825\n",
      "Epoch 606, Training Loss: 0.35270029306411743, Validation Loss: 0.3490079939365387\n",
      "Epoch 607, Training Loss: 0.3526993691921234, Validation Loss: 0.3490079343318939\n",
      "Epoch 608, Training Loss: 0.352698415517807, Validation Loss: 0.349007785320282\n",
      "Epoch 609, Training Loss: 0.3526974618434906, Validation Loss: 0.3490076959133148\n",
      "Epoch 610, Training Loss: 0.3526965081691742, Validation Loss: 0.3490075469017029\n",
      "Epoch 611, Training Loss: 0.3526955544948578, Validation Loss: 0.34900739789009094\n",
      "Epoch 612, Training Loss: 0.3526946008205414, Validation Loss: 0.349007248878479\n",
      "Epoch 613, Training Loss: 0.35269367694854736, Validation Loss: 0.34900712966918945\n",
      "Epoch 614, Training Loss: 0.35269275307655334, Validation Loss: 0.3490070402622223\n",
      "Epoch 615, Training Loss: 0.3526918590068817, Validation Loss: 0.34900692105293274\n",
      "Epoch 616, Training Loss: 0.3526909351348877, Validation Loss: 0.3490068018436432\n",
      "Epoch 617, Training Loss: 0.35269004106521606, Validation Loss: 0.34900668263435364\n",
      "Epoch 618, Training Loss: 0.35268911719322205, Validation Loss: 0.3490064740180969\n",
      "Epoch 619, Training Loss: 0.3526882231235504, Validation Loss: 0.3490063548088074\n",
      "Epoch 620, Training Loss: 0.3526872992515564, Validation Loss: 0.3490062355995178\n",
      "Epoch 621, Training Loss: 0.35268640518188477, Validation Loss: 0.34900611639022827\n",
      "Epoch 622, Training Loss: 0.3526855409145355, Validation Loss: 0.3490059971809387\n",
      "Epoch 623, Training Loss: 0.3526846468448639, Validation Loss: 0.3490058481693268\n",
      "Epoch 624, Training Loss: 0.35268375277519226, Validation Loss: 0.34900569915771484\n",
      "Epoch 625, Training Loss: 0.3526829183101654, Validation Loss: 0.3490055203437805\n",
      "Epoch 626, Training Loss: 0.35268205404281616, Validation Loss: 0.34900540113449097\n",
      "Epoch 627, Training Loss: 0.3526812195777893, Validation Loss: 0.34900525212287903\n",
      "Epoch 628, Training Loss: 0.35268035531044006, Validation Loss: 0.3490051329135895\n",
      "Epoch 629, Training Loss: 0.35267946124076843, Validation Loss: 0.34900498390197754\n",
      "Epoch 630, Training Loss: 0.3526786267757416, Validation Loss: 0.3490048348903656\n",
      "Epoch 631, Training Loss: 0.3526777923107147, Validation Loss: 0.34900468587875366\n",
      "Epoch 632, Training Loss: 0.35267698764801025, Validation Loss: 0.3490045368671417\n",
      "Epoch 633, Training Loss: 0.352676123380661, Validation Loss: 0.3490044176578522\n",
      "Epoch 634, Training Loss: 0.35267528891563416, Validation Loss: 0.34900423884391785\n",
      "Epoch 635, Training Loss: 0.3526744842529297, Validation Loss: 0.34900417923927307\n",
      "Epoch 636, Training Loss: 0.3526736795902252, Validation Loss: 0.34900403022766113\n",
      "Epoch 637, Training Loss: 0.35267284512519836, Validation Loss: 0.3490039110183716\n",
      "Epoch 638, Training Loss: 0.3526720702648163, Validation Loss: 0.34900379180908203\n",
      "Epoch 639, Training Loss: 0.35267123579978943, Validation Loss: 0.3490036725997925\n",
      "Epoch 640, Training Loss: 0.35267046093940735, Validation Loss: 0.34900355339050293\n",
      "Epoch 641, Training Loss: 0.3526696562767029, Validation Loss: 0.34900346398353577\n",
      "Epoch 642, Training Loss: 0.3526688814163208, Validation Loss: 0.3490033745765686\n",
      "Epoch 643, Training Loss: 0.35266807675361633, Validation Loss: 0.34900325536727905\n",
      "Epoch 644, Training Loss: 0.35266730189323425, Validation Loss: 0.3490031957626343\n",
      "Epoch 645, Training Loss: 0.3526665270328522, Validation Loss: 0.3490031063556671\n",
      "Epoch 646, Training Loss: 0.3526657521724701, Validation Loss: 0.3490029573440552\n",
      "Epoch 647, Training Loss: 0.352664977312088, Validation Loss: 0.3490028381347656\n",
      "Epoch 648, Training Loss: 0.3526642322540283, Validation Loss: 0.3490026295185089\n",
      "Epoch 649, Training Loss: 0.35266345739364624, Validation Loss: 0.34900251030921936\n",
      "Epoch 650, Training Loss: 0.35266271233558655, Validation Loss: 0.3490024209022522\n",
      "Epoch 651, Training Loss: 0.35266193747520447, Validation Loss: 0.34900233149528503\n",
      "Epoch 652, Training Loss: 0.35266125202178955, Validation Loss: 0.3490022122859955\n",
      "Epoch 653, Training Loss: 0.35266047716140747, Validation Loss: 0.34900209307670593\n",
      "Epoch 654, Training Loss: 0.35265976190567017, Validation Loss: 0.349001944065094\n",
      "Epoch 655, Training Loss: 0.3526589870452881, Validation Loss: 0.34900179505348206\n",
      "Epoch 656, Training Loss: 0.35265830159187317, Validation Loss: 0.3490016758441925\n",
      "Epoch 657, Training Loss: 0.3526575565338135, Validation Loss: 0.34900158643722534\n",
      "Epoch 658, Training Loss: 0.3526568114757538, Validation Loss: 0.3490014672279358\n",
      "Epoch 659, Training Loss: 0.35265612602233887, Validation Loss: 0.34900134801864624\n",
      "Epoch 660, Training Loss: 0.3526553809642792, Validation Loss: 0.3490011990070343\n",
      "Epoch 661, Training Loss: 0.35265466570854187, Validation Loss: 0.34900104999542236\n",
      "Epoch 662, Training Loss: 0.35265398025512695, Validation Loss: 0.3490009605884552\n",
      "Epoch 663, Training Loss: 0.35265326499938965, Validation Loss: 0.34900087118148804\n",
      "Epoch 664, Training Loss: 0.35265257954597473, Validation Loss: 0.34900081157684326\n",
      "Epoch 665, Training Loss: 0.3526518940925598, Validation Loss: 0.3490007221698761\n",
      "Epoch 666, Training Loss: 0.3526512086391449, Validation Loss: 0.34900063276290894\n",
      "Epoch 667, Training Loss: 0.3526504933834076, Validation Loss: 0.349000483751297\n",
      "Epoch 668, Training Loss: 0.35264983773231506, Validation Loss: 0.34900033473968506\n",
      "Epoch 669, Training Loss: 0.35264915227890015, Validation Loss: 0.3490002155303955\n",
      "Epoch 670, Training Loss: 0.3526484966278076, Validation Loss: 0.34900012612342834\n",
      "Epoch 671, Training Loss: 0.3526477813720703, Validation Loss: 0.3490000367164612\n",
      "Epoch 672, Training Loss: 0.3526471257209778, Validation Loss: 0.348999947309494\n",
      "Epoch 673, Training Loss: 0.35264647006988525, Validation Loss: 0.34899982810020447\n",
      "Epoch 674, Training Loss: 0.3526458144187927, Validation Loss: 0.3489997088909149\n",
      "Epoch 675, Training Loss: 0.3526451289653778, Validation Loss: 0.34899958968162537\n",
      "Epoch 676, Training Loss: 0.3526444733142853, Validation Loss: 0.34899941086769104\n",
      "Epoch 677, Training Loss: 0.35264381766319275, Validation Loss: 0.3489992916584015\n",
      "Epoch 678, Training Loss: 0.3526431918144226, Validation Loss: 0.34899917244911194\n",
      "Epoch 679, Training Loss: 0.3526425361633301, Validation Loss: 0.34899911284446716\n",
      "Epoch 680, Training Loss: 0.35264188051223755, Validation Loss: 0.3489989936351776\n",
      "Epoch 681, Training Loss: 0.3526412546634674, Validation Loss: 0.34899887442588806\n",
      "Epoch 682, Training Loss: 0.35264062881469727, Validation Loss: 0.3489987254142761\n",
      "Epoch 683, Training Loss: 0.35263997316360474, Validation Loss: 0.3489986062049866\n",
      "Epoch 684, Training Loss: 0.3526393473148346, Validation Loss: 0.34899845719337463\n",
      "Epoch 685, Training Loss: 0.35263872146606445, Validation Loss: 0.34899836778640747\n",
      "Epoch 686, Training Loss: 0.3526380956172943, Validation Loss: 0.3489982485771179\n",
      "Epoch 687, Training Loss: 0.35263746976852417, Validation Loss: 0.34899818897247314\n",
      "Epoch 688, Training Loss: 0.35263684391975403, Validation Loss: 0.3489980697631836\n",
      "Epoch 689, Training Loss: 0.3526362478733063, Validation Loss: 0.34899795055389404\n",
      "Epoch 690, Training Loss: 0.3526356518268585, Validation Loss: 0.3489978313446045\n",
      "Epoch 691, Training Loss: 0.3526350259780884, Validation Loss: 0.34899765253067017\n",
      "Epoch 692, Training Loss: 0.35263440012931824, Validation Loss: 0.348997563123703\n",
      "Epoch 693, Training Loss: 0.3526338040828705, Validation Loss: 0.34899747371673584\n",
      "Epoch 694, Training Loss: 0.35263320803642273, Validation Loss: 0.34899744391441345\n",
      "Epoch 695, Training Loss: 0.352632611989975, Validation Loss: 0.3489972949028015\n",
      "Epoch 696, Training Loss: 0.3526320159435272, Validation Loss: 0.34899717569351196\n",
      "Epoch 697, Training Loss: 0.35263141989707947, Validation Loss: 0.3489970862865448\n",
      "Epoch 698, Training Loss: 0.3526308238506317, Validation Loss: 0.34899693727493286\n",
      "Epoch 699, Training Loss: 0.35263022780418396, Validation Loss: 0.3489968478679657\n",
      "Epoch 700, Training Loss: 0.3526296317577362, Validation Loss: 0.34899675846099854\n",
      "Epoch 701, Training Loss: 0.35262906551361084, Validation Loss: 0.348996639251709\n",
      "Epoch 702, Training Loss: 0.3526284694671631, Validation Loss: 0.3489965498447418\n",
      "Epoch 703, Training Loss: 0.3526279032230377, Validation Loss: 0.3489964008331299\n",
      "Epoch 704, Training Loss: 0.35262733697891235, Validation Loss: 0.34899628162384033\n",
      "Epoch 705, Training Loss: 0.352626770734787, Validation Loss: 0.34899619221687317\n",
      "Epoch 706, Training Loss: 0.3526262044906616, Validation Loss: 0.348996102809906\n",
      "Epoch 707, Training Loss: 0.35262563824653625, Validation Loss: 0.34899601340293884\n",
      "Epoch 708, Training Loss: 0.3526250720024109, Validation Loss: 0.3489958941936493\n",
      "Epoch 709, Training Loss: 0.3526245057582855, Validation Loss: 0.34899580478668213\n",
      "Epoch 710, Training Loss: 0.35262393951416016, Validation Loss: 0.3489956855773926\n",
      "Epoch 711, Training Loss: 0.3526233732700348, Validation Loss: 0.3489956259727478\n",
      "Epoch 712, Training Loss: 0.3526228368282318, Validation Loss: 0.34899550676345825\n",
      "Epoch 713, Training Loss: 0.35262230038642883, Validation Loss: 0.3489953875541687\n",
      "Epoch 714, Training Loss: 0.35262173414230347, Validation Loss: 0.3489953279495239\n",
      "Epoch 715, Training Loss: 0.3526211380958557, Validation Loss: 0.3489952087402344\n",
      "Epoch 716, Training Loss: 0.3526206314563751, Validation Loss: 0.3489950895309448\n",
      "Epoch 717, Training Loss: 0.35262009501457214, Validation Loss: 0.34899502992630005\n",
      "Epoch 718, Training Loss: 0.3526195287704468, Validation Loss: 0.3489949107170105\n",
      "Epoch 719, Training Loss: 0.3526189923286438, Validation Loss: 0.34899482131004333\n",
      "Epoch 720, Training Loss: 0.35261842608451843, Validation Loss: 0.34899476170539856\n",
      "Epoch 721, Training Loss: 0.35261794924736023, Validation Loss: 0.3489946722984314\n",
      "Epoch 722, Training Loss: 0.35261741280555725, Validation Loss: 0.3489946126937866\n",
      "Epoch 723, Training Loss: 0.3526168763637543, Validation Loss: 0.34899452328681946\n",
      "Epoch 724, Training Loss: 0.3526163101196289, Validation Loss: 0.3489944338798523\n",
      "Epoch 725, Training Loss: 0.3526158034801483, Validation Loss: 0.3489943742752075\n",
      "Epoch 726, Training Loss: 0.3526152968406677, Validation Loss: 0.34899431467056274\n",
      "Epoch 727, Training Loss: 0.35261476039886475, Validation Loss: 0.3489942252635956\n",
      "Epoch 728, Training Loss: 0.35261425375938416, Validation Loss: 0.3489941656589508\n",
      "Epoch 729, Training Loss: 0.3526137173175812, Validation Loss: 0.34899410605430603\n",
      "Epoch 730, Training Loss: 0.3526131808757782, Validation Loss: 0.34899404644966125\n",
      "Epoch 731, Training Loss: 0.3526126742362976, Validation Loss: 0.3489939272403717\n",
      "Epoch 732, Training Loss: 0.35261213779449463, Validation Loss: 0.34899383783340454\n",
      "Epoch 733, Training Loss: 0.3526116609573364, Validation Loss: 0.34899377822875977\n",
      "Epoch 734, Training Loss: 0.3526111841201782, Validation Loss: 0.3489936888217926\n",
      "Epoch 735, Training Loss: 0.35261064767837524, Validation Loss: 0.3489936590194702\n",
      "Epoch 736, Training Loss: 0.35261017084121704, Validation Loss: 0.34899356961250305\n",
      "Epoch 737, Training Loss: 0.35260966420173645, Validation Loss: 0.3489934802055359\n",
      "Epoch 738, Training Loss: 0.35260912775993347, Validation Loss: 0.3489934206008911\n",
      "Epoch 739, Training Loss: 0.3526086211204529, Validation Loss: 0.34899330139160156\n",
      "Epoch 740, Training Loss: 0.3526081442832947, Validation Loss: 0.34899330139160156\n",
      "Epoch 741, Training Loss: 0.3526076376438141, Validation Loss: 0.348993182182312\n",
      "Epoch 742, Training Loss: 0.3526071608066559, Validation Loss: 0.34899312257766724\n",
      "Epoch 743, Training Loss: 0.3526066243648529, Validation Loss: 0.34899309277534485\n",
      "Epoch 744, Training Loss: 0.3526061475276947, Validation Loss: 0.3489930033683777\n",
      "Epoch 745, Training Loss: 0.3526056706905365, Validation Loss: 0.3489929437637329\n",
      "Epoch 746, Training Loss: 0.3526051342487335, Validation Loss: 0.34899285435676575\n",
      "Epoch 747, Training Loss: 0.3526046872138977, Validation Loss: 0.34899285435676575\n",
      "Epoch 748, Training Loss: 0.3526042103767395, Validation Loss: 0.34899279475212097\n",
      "Epoch 749, Training Loss: 0.3526037037372589, Validation Loss: 0.3489927649497986\n",
      "Epoch 750, Training Loss: 0.3526032567024231, Validation Loss: 0.3489927351474762\n",
      "Epoch 751, Training Loss: 0.3526027500629425, Validation Loss: 0.3489926755428314\n",
      "Epoch 752, Training Loss: 0.3526022732257843, Validation Loss: 0.34899258613586426\n",
      "Epoch 753, Training Loss: 0.3526017963886261, Validation Loss: 0.34899255633354187\n",
      "Epoch 754, Training Loss: 0.3526013195514679, Validation Loss: 0.34899255633354187\n",
      "Epoch 755, Training Loss: 0.3526008725166321, Validation Loss: 0.3489925265312195\n",
      "Epoch 756, Training Loss: 0.3526003658771515, Validation Loss: 0.3489924967288971\n",
      "Epoch 757, Training Loss: 0.3525999188423157, Validation Loss: 0.3489924669265747\n",
      "Epoch 758, Training Loss: 0.3525994122028351, Validation Loss: 0.34899240732192993\n",
      "Epoch 759, Training Loss: 0.35259896516799927, Validation Loss: 0.34899237751960754\n",
      "Epoch 760, Training Loss: 0.35259848833084106, Validation Loss: 0.34899234771728516\n",
      "Epoch 761, Training Loss: 0.35259804129600525, Validation Loss: 0.34899234771728516\n",
      "Epoch 762, Training Loss: 0.35259756445884705, Validation Loss: 0.34899234771728516\n",
      "Epoch 763, Training Loss: 0.35259711742401123, Validation Loss: 0.34899231791496277\n",
      "Epoch 764, Training Loss: 0.352596640586853, Validation Loss: 0.348992258310318\n",
      "Epoch 765, Training Loss: 0.3525961637496948, Validation Loss: 0.3489922285079956\n",
      "Epoch 766, Training Loss: 0.352595716714859, Validation Loss: 0.3489921987056732\n",
      "Epoch 767, Training Loss: 0.3525952696800232, Validation Loss: 0.34899216890335083\n",
      "Epoch 768, Training Loss: 0.35259485244750977, Validation Loss: 0.34899216890335083\n",
      "Epoch 769, Training Loss: 0.3525943458080292, Validation Loss: 0.34899216890335083\n",
      "Epoch 770, Training Loss: 0.35259389877319336, Validation Loss: 0.34899207949638367\n",
      "Epoch 771, Training Loss: 0.35259342193603516, Validation Loss: 0.3489920496940613\n",
      "Epoch 772, Training Loss: 0.35259300470352173, Validation Loss: 0.3489919900894165\n",
      "Epoch 773, Training Loss: 0.3525925576686859, Validation Loss: 0.3489919602870941\n",
      "Epoch 774, Training Loss: 0.3525920808315277, Validation Loss: 0.34899193048477173\n",
      "Epoch 775, Training Loss: 0.3525916337966919, Validation Loss: 0.34899190068244934\n",
      "Epoch 776, Training Loss: 0.35259121656417847, Validation Loss: 0.34899184107780457\n",
      "Epoch 777, Training Loss: 0.35259076952934265, Validation Loss: 0.3489917516708374\n",
      "Epoch 778, Training Loss: 0.35259032249450684, Validation Loss: 0.348991721868515\n",
      "Epoch 779, Training Loss: 0.352589875459671, Validation Loss: 0.348991721868515\n",
      "Epoch 780, Training Loss: 0.3525894284248352, Validation Loss: 0.34899166226387024\n",
      "Epoch 781, Training Loss: 0.3525889813899994, Validation Loss: 0.34899166226387024\n",
      "Epoch 782, Training Loss: 0.35258856415748596, Validation Loss: 0.34899160265922546\n",
      "Epoch 783, Training Loss: 0.35258814692497253, Validation Loss: 0.3489915728569031\n",
      "Epoch 784, Training Loss: 0.35258767008781433, Validation Loss: 0.3489915430545807\n",
      "Epoch 785, Training Loss: 0.3525872528553009, Validation Loss: 0.3489915430545807\n",
      "Epoch 786, Training Loss: 0.3525868058204651, Validation Loss: 0.3489915132522583\n",
      "Epoch 787, Training Loss: 0.35258638858795166, Validation Loss: 0.3489914834499359\n",
      "Epoch 788, Training Loss: 0.35258597135543823, Validation Loss: 0.3489914536476135\n",
      "Epoch 789, Training Loss: 0.35258549451828003, Validation Loss: 0.34899142384529114\n",
      "Epoch 790, Training Loss: 0.352585107088089, Validation Loss: 0.34899139404296875\n",
      "Epoch 791, Training Loss: 0.35258468985557556, Validation Loss: 0.34899139404296875\n",
      "Epoch 792, Training Loss: 0.35258427262306213, Validation Loss: 0.34899136424064636\n",
      "Epoch 793, Training Loss: 0.3525838255882263, Validation Loss: 0.3489913046360016\n",
      "Epoch 794, Training Loss: 0.3525833785533905, Validation Loss: 0.3489912748336792\n",
      "Epoch 795, Training Loss: 0.35258299112319946, Validation Loss: 0.3489912748336792\n",
      "Epoch 796, Training Loss: 0.35258254408836365, Validation Loss: 0.3489912152290344\n",
      "Epoch 797, Training Loss: 0.3525821566581726, Validation Loss: 0.34899118542671204\n",
      "Epoch 798, Training Loss: 0.3525817096233368, Validation Loss: 0.34899115562438965\n",
      "Epoch 799, Training Loss: 0.352581262588501, Validation Loss: 0.34899112582206726\n",
      "Epoch 800, Training Loss: 0.35258087515830994, Validation Loss: 0.3489910960197449\n",
      "Epoch 801, Training Loss: 0.3525804579257965, Validation Loss: 0.3489910960197449\n",
      "Epoch 802, Training Loss: 0.3525800406932831, Validation Loss: 0.3489910364151001\n",
      "Epoch 803, Training Loss: 0.35257962346076965, Validation Loss: 0.3489910066127777\n",
      "Epoch 804, Training Loss: 0.3525792062282562, Validation Loss: 0.3489909768104553\n",
      "Epoch 805, Training Loss: 0.3525787889957428, Validation Loss: 0.3489909768104553\n",
      "Epoch 806, Training Loss: 0.35257840156555176, Validation Loss: 0.3489909768104553\n",
      "Epoch 807, Training Loss: 0.35257795453071594, Validation Loss: 0.34899091720581055\n",
      "Epoch 808, Training Loss: 0.3525775969028473, Validation Loss: 0.34899085760116577\n",
      "Epoch 809, Training Loss: 0.35257717967033386, Validation Loss: 0.34899085760116577\n",
      "Epoch 810, Training Loss: 0.35257676243782043, Validation Loss: 0.3489908277988434\n",
      "Epoch 811, Training Loss: 0.352576345205307, Validation Loss: 0.3489908277988434\n",
      "Epoch 812, Training Loss: 0.35257595777511597, Validation Loss: 0.348990797996521\n",
      "Epoch 813, Training Loss: 0.35257551074028015, Validation Loss: 0.3489907383918762\n",
      "Epoch 814, Training Loss: 0.3525751531124115, Validation Loss: 0.3489907383918762\n",
      "Epoch 815, Training Loss: 0.35257473587989807, Validation Loss: 0.34899070858955383\n",
      "Epoch 816, Training Loss: 0.35257431864738464, Validation Loss: 0.34899067878723145\n",
      "Epoch 817, Training Loss: 0.352573961019516, Validation Loss: 0.34899064898490906\n",
      "Epoch 818, Training Loss: 0.35257354378700256, Validation Loss: 0.34899064898490906\n",
      "Epoch 819, Training Loss: 0.35257312655448914, Validation Loss: 0.34899061918258667\n",
      "Epoch 820, Training Loss: 0.3525727391242981, Validation Loss: 0.34899061918258667\n",
      "Epoch 821, Training Loss: 0.35257232189178467, Validation Loss: 0.3489905595779419\n",
      "Epoch 822, Training Loss: 0.35257193446159363, Validation Loss: 0.3489904999732971\n",
      "Epoch 823, Training Loss: 0.3525715470314026, Validation Loss: 0.3489904999732971\n",
      "Epoch 824, Training Loss: 0.35257112979888916, Validation Loss: 0.3489905595779419\n",
      "Epoch 825, Training Loss: 0.3525707423686981, Validation Loss: 0.3489905297756195\n",
      "Epoch 826, Training Loss: 0.3525703549385071, Validation Loss: 0.3489904999732971\n",
      "Epoch 827, Training Loss: 0.35256996750831604, Validation Loss: 0.34899047017097473\n",
      "Epoch 828, Training Loss: 0.3525695502758026, Validation Loss: 0.34899047017097473\n",
      "Epoch 829, Training Loss: 0.3525691628456116, Validation Loss: 0.3489904999732971\n",
      "Epoch 830, Training Loss: 0.3525688052177429, Validation Loss: 0.34899044036865234\n",
      "Epoch 831, Training Loss: 0.3525684177875519, Validation Loss: 0.34899044036865234\n",
      "Epoch 832, Training Loss: 0.35256803035736084, Validation Loss: 0.34899044036865234\n",
      "Epoch 833, Training Loss: 0.3525676429271698, Validation Loss: 0.34899044036865234\n",
      "Epoch 834, Training Loss: 0.35256725549697876, Validation Loss: 0.34899038076400757\n",
      "Epoch 835, Training Loss: 0.3525668680667877, Validation Loss: 0.34899041056632996\n",
      "Epoch 836, Training Loss: 0.3525664508342743, Validation Loss: 0.34899038076400757\n",
      "Epoch 837, Training Loss: 0.35256609320640564, Validation Loss: 0.34899038076400757\n",
      "Epoch 838, Training Loss: 0.3525657057762146, Validation Loss: 0.3489903509616852\n",
      "Epoch 839, Training Loss: 0.35256528854370117, Validation Loss: 0.34899038076400757\n",
      "Epoch 840, Training Loss: 0.3525649309158325, Validation Loss: 0.3489903211593628\n",
      "Epoch 841, Training Loss: 0.35256457328796387, Validation Loss: 0.3489903211593628\n",
      "Epoch 842, Training Loss: 0.3525641858577728, Validation Loss: 0.3489903211593628\n",
      "Epoch 843, Training Loss: 0.3525637984275818, Validation Loss: 0.3489902913570404\n",
      "Epoch 844, Training Loss: 0.35256344079971313, Validation Loss: 0.348990261554718\n",
      "Epoch 845, Training Loss: 0.3525630533695221, Validation Loss: 0.348990261554718\n",
      "Epoch 846, Training Loss: 0.35256266593933105, Validation Loss: 0.348990261554718\n",
      "Epoch 847, Training Loss: 0.35256227850914, Validation Loss: 0.34899023175239563\n",
      "Epoch 848, Training Loss: 0.35256192088127136, Validation Loss: 0.34899020195007324\n",
      "Epoch 849, Training Loss: 0.3525615632534027, Validation Loss: 0.34899020195007324\n",
      "Epoch 850, Training Loss: 0.3525611460208893, Validation Loss: 0.34899020195007324\n",
      "Epoch 851, Training Loss: 0.352560818195343, Validation Loss: 0.34899020195007324\n",
      "Epoch 852, Training Loss: 0.3525604009628296, Validation Loss: 0.34899020195007324\n",
      "Epoch 853, Training Loss: 0.3525600731372833, Validation Loss: 0.34899020195007324\n",
      "Epoch 854, Training Loss: 0.3525596857070923, Validation Loss: 0.34899020195007324\n",
      "Epoch 855, Training Loss: 0.35255929827690125, Validation Loss: 0.34899017214775085\n",
      "Epoch 856, Training Loss: 0.3525589406490326, Validation Loss: 0.34899017214775085\n",
      "Epoch 857, Training Loss: 0.35255858302116394, Validation Loss: 0.34899014234542847\n",
      "Epoch 858, Training Loss: 0.3525582253932953, Validation Loss: 0.34899017214775085\n",
      "Epoch 859, Training Loss: 0.35255783796310425, Validation Loss: 0.34899014234542847\n",
      "Epoch 860, Training Loss: 0.3525574803352356, Validation Loss: 0.34899014234542847\n",
      "Epoch 861, Training Loss: 0.35255712270736694, Validation Loss: 0.34899014234542847\n",
      "Epoch 862, Training Loss: 0.3525567650794983, Validation Loss: 0.3489901125431061\n",
      "Epoch 863, Training Loss: 0.35255640745162964, Validation Loss: 0.34899014234542847\n",
      "Epoch 864, Training Loss: 0.3525560200214386, Validation Loss: 0.34899014234542847\n",
      "Epoch 865, Training Loss: 0.35255566239356995, Validation Loss: 0.34899014234542847\n",
      "Epoch 866, Training Loss: 0.3525552749633789, Validation Loss: 0.34899014234542847\n",
      "Epoch 867, Training Loss: 0.35255494713783264, Validation Loss: 0.3489901125431061\n",
      "Epoch 868, Training Loss: 0.352554589509964, Validation Loss: 0.34899014234542847\n",
      "Epoch 869, Training Loss: 0.35255420207977295, Validation Loss: 0.34899014234542847\n",
      "Epoch 870, Training Loss: 0.3525538742542267, Validation Loss: 0.3489901125431061\n",
      "Epoch 871, Training Loss: 0.35255348682403564, Validation Loss: 0.34899014234542847\n",
      "Epoch 872, Training Loss: 0.352553129196167, Validation Loss: 0.34899014234542847\n",
      "Epoch 873, Training Loss: 0.3525528013706207, Validation Loss: 0.34899017214775085\n",
      "Epoch 874, Training Loss: 0.3525524437427521, Validation Loss: 0.34899014234542847\n",
      "Epoch 875, Training Loss: 0.3525520861148834, Validation Loss: 0.34899017214775085\n",
      "Epoch 876, Training Loss: 0.35255172848701477, Validation Loss: 0.34899017214775085\n",
      "Epoch 877, Training Loss: 0.3525513708591461, Validation Loss: 0.34899020195007324\n",
      "Epoch 878, Training Loss: 0.35255101323127747, Validation Loss: 0.34899017214775085\n",
      "Epoch 879, Training Loss: 0.3525506854057312, Validation Loss: 0.34899020195007324\n",
      "Epoch 880, Training Loss: 0.35255029797554016, Validation Loss: 0.34899017214775085\n",
      "Epoch 881, Training Loss: 0.3525499701499939, Validation Loss: 0.34899017214775085\n",
      "Epoch 882, Training Loss: 0.35254958271980286, Validation Loss: 0.34899017214775085\n",
      "Epoch 883, Training Loss: 0.3525492548942566, Validation Loss: 0.34899017214775085\n",
      "Epoch 884, Training Loss: 0.3525489270687103, Validation Loss: 0.34899020195007324\n",
      "Epoch 885, Training Loss: 0.3525485694408417, Validation Loss: 0.34899020195007324\n",
      "Epoch 886, Training Loss: 0.352548211812973, Validation Loss: 0.34899023175239563\n",
      "Epoch 887, Training Loss: 0.35254788398742676, Validation Loss: 0.34899020195007324\n",
      "Epoch 888, Training Loss: 0.3525474965572357, Validation Loss: 0.34899020195007324\n",
      "Epoch 889, Training Loss: 0.35254713892936707, Validation Loss: 0.34899023175239563\n",
      "Epoch 890, Training Loss: 0.3525468409061432, Validation Loss: 0.34899023175239563\n",
      "Epoch 891, Training Loss: 0.3525465130805969, Validation Loss: 0.34899023175239563\n",
      "Epoch 892, Training Loss: 0.35254615545272827, Validation Loss: 0.348990261554718\n",
      "Epoch 893, Training Loss: 0.3525457978248596, Validation Loss: 0.348990261554718\n",
      "Epoch 894, Training Loss: 0.35254544019699097, Validation Loss: 0.348990261554718\n",
      "Epoch 895, Training Loss: 0.3525451421737671, Validation Loss: 0.3489902913570404\n",
      "Epoch 896, Training Loss: 0.35254475474357605, Validation Loss: 0.3489903211593628\n",
      "Epoch 897, Training Loss: 0.3525444269180298, Validation Loss: 0.3489903509616852\n",
      "Epoch 898, Training Loss: 0.3525440990924835, Validation Loss: 0.3489903509616852\n",
      "Epoch 899, Training Loss: 0.35254380106925964, Validation Loss: 0.3489903211593628\n",
      "Epoch 900, Training Loss: 0.3525434136390686, Validation Loss: 0.3489903509616852\n",
      "Epoch 901, Training Loss: 0.35254308581352234, Validation Loss: 0.34899041056632996\n",
      "Epoch 902, Training Loss: 0.3525427579879761, Validation Loss: 0.34899044036865234\n",
      "Epoch 903, Training Loss: 0.3525424301624298, Validation Loss: 0.34899044036865234\n",
      "Epoch 904, Training Loss: 0.35254204273223877, Validation Loss: 0.34899044036865234\n",
      "Epoch 905, Training Loss: 0.3525417447090149, Validation Loss: 0.34899044036865234\n",
      "Epoch 906, Training Loss: 0.35254138708114624, Validation Loss: 0.34899044036865234\n",
      "Epoch 907, Training Loss: 0.3525410592556, Validation Loss: 0.34899047017097473\n",
      "Epoch 908, Training Loss: 0.3525407612323761, Validation Loss: 0.3489904999732971\n",
      "Epoch 909, Training Loss: 0.35254043340682983, Validation Loss: 0.34899047017097473\n",
      "Epoch 910, Training Loss: 0.3525400757789612, Validation Loss: 0.3489904999732971\n",
      "Epoch 911, Training Loss: 0.35253971815109253, Validation Loss: 0.3489905297756195\n",
      "Epoch 912, Training Loss: 0.35253942012786865, Validation Loss: 0.3489904999732971\n",
      "Epoch 913, Training Loss: 0.3525390625, Validation Loss: 0.3489905595779419\n",
      "Epoch 914, Training Loss: 0.35253873467445374, Validation Loss: 0.3489905297756195\n",
      "Epoch 915, Training Loss: 0.35253840684890747, Validation Loss: 0.3489905297756195\n",
      "Epoch 916, Training Loss: 0.3525380790233612, Validation Loss: 0.3489905297756195\n",
      "Epoch 917, Training Loss: 0.35253775119781494, Validation Loss: 0.3489905297756195\n",
      "Epoch 918, Training Loss: 0.35253745317459106, Validation Loss: 0.3489905595779419\n",
      "Epoch 919, Training Loss: 0.3525371253490448, Validation Loss: 0.3489905595779419\n",
      "Epoch 920, Training Loss: 0.35253679752349854, Validation Loss: 0.3489905595779419\n",
      "Epoch 921, Training Loss: 0.35253646969795227, Validation Loss: 0.3489905893802643\n",
      "Epoch 922, Training Loss: 0.352536141872406, Validation Loss: 0.3489905893802643\n",
      "Epoch 923, Training Loss: 0.35253581404685974, Validation Loss: 0.34899061918258667\n",
      "Epoch 924, Training Loss: 0.3525354862213135, Validation Loss: 0.34899061918258667\n",
      "Epoch 925, Training Loss: 0.3525351583957672, Validation Loss: 0.34899061918258667\n",
      "Epoch 926, Training Loss: 0.35253483057022095, Validation Loss: 0.34899067878723145\n",
      "Epoch 927, Training Loss: 0.3525345027446747, Validation Loss: 0.34899067878723145\n",
      "Epoch 928, Training Loss: 0.3525341749191284, Validation Loss: 0.34899061918258667\n",
      "Epoch 929, Training Loss: 0.35253387689590454, Validation Loss: 0.34899064898490906\n",
      "Epoch 930, Training Loss: 0.3525335490703583, Validation Loss: 0.34899067878723145\n",
      "Epoch 931, Training Loss: 0.352533221244812, Validation Loss: 0.34899067878723145\n",
      "Epoch 932, Training Loss: 0.35253292322158813, Validation Loss: 0.3489907383918762\n",
      "Epoch 933, Training Loss: 0.35253259539604187, Validation Loss: 0.3489907383918762\n",
      "Epoch 934, Training Loss: 0.352532297372818, Validation Loss: 0.3489907383918762\n",
      "Epoch 935, Training Loss: 0.35253196954727173, Validation Loss: 0.3489907383918762\n",
      "Epoch 936, Training Loss: 0.35253167152404785, Validation Loss: 0.3489907681941986\n",
      "Epoch 937, Training Loss: 0.352531373500824, Validation Loss: 0.348990797996521\n",
      "Epoch 938, Training Loss: 0.3525310456752777, Validation Loss: 0.348990797996521\n",
      "Epoch 939, Training Loss: 0.35253071784973145, Validation Loss: 0.348990797996521\n",
      "Epoch 940, Training Loss: 0.3525303900241852, Validation Loss: 0.348990797996521\n",
      "Epoch 941, Training Loss: 0.3525300621986389, Validation Loss: 0.3489908277988434\n",
      "Epoch 942, Training Loss: 0.3525297939777374, Validation Loss: 0.34899088740348816\n",
      "Epoch 943, Training Loss: 0.35252946615219116, Validation Loss: 0.34899085760116577\n",
      "Epoch 944, Training Loss: 0.3525291383266449, Validation Loss: 0.34899088740348816\n",
      "Epoch 945, Training Loss: 0.352528840303421, Validation Loss: 0.34899091720581055\n",
      "Epoch 946, Training Loss: 0.35252854228019714, Validation Loss: 0.34899088740348816\n",
      "Epoch 947, Training Loss: 0.3525282144546509, Validation Loss: 0.34899091720581055\n",
      "Epoch 948, Training Loss: 0.352527916431427, Validation Loss: 0.3489909768104553\n",
      "Epoch 949, Training Loss: 0.35252758860588074, Validation Loss: 0.3489909768104553\n",
      "Epoch 950, Training Loss: 0.35252729058265686, Validation Loss: 0.3489909768104553\n",
      "Epoch 951, Training Loss: 0.3525269627571106, Validation Loss: 0.3489910066127777\n",
      "Epoch 952, Training Loss: 0.3525266647338867, Validation Loss: 0.3489910364151001\n",
      "Epoch 953, Training Loss: 0.35252636671066284, Validation Loss: 0.3489910364151001\n",
      "Epoch 954, Training Loss: 0.35252606868743896, Validation Loss: 0.3489910364151001\n",
      "Epoch 955, Training Loss: 0.3525257110595703, Validation Loss: 0.3489910662174225\n",
      "Epoch 956, Training Loss: 0.3525254428386688, Validation Loss: 0.3489910960197449\n",
      "Epoch 957, Training Loss: 0.35252514481544495, Validation Loss: 0.3489910960197449\n",
      "Epoch 958, Training Loss: 0.35252484679222107, Validation Loss: 0.34899112582206726\n",
      "Epoch 959, Training Loss: 0.3525245189666748, Validation Loss: 0.34899115562438965\n",
      "Epoch 960, Training Loss: 0.3525242209434509, Validation Loss: 0.34899115562438965\n",
      "Epoch 961, Training Loss: 0.35252392292022705, Validation Loss: 0.34899115562438965\n",
      "Epoch 962, Training Loss: 0.3525236248970032, Validation Loss: 0.34899115562438965\n",
      "Epoch 963, Training Loss: 0.3525233268737793, Validation Loss: 0.3489912152290344\n",
      "Epoch 964, Training Loss: 0.3525230586528778, Validation Loss: 0.3489912152290344\n",
      "Epoch 965, Training Loss: 0.35252273082733154, Validation Loss: 0.3489912748336792\n",
      "Epoch 966, Training Loss: 0.35252243280410767, Validation Loss: 0.3489912450313568\n",
      "Epoch 967, Training Loss: 0.3525221347808838, Validation Loss: 0.3489912748336792\n",
      "Epoch 968, Training Loss: 0.3525218367576599, Validation Loss: 0.3489912748336792\n",
      "Epoch 969, Training Loss: 0.35252153873443604, Validation Loss: 0.3489913046360016\n",
      "Epoch 970, Training Loss: 0.35252124071121216, Validation Loss: 0.34899136424064636\n",
      "Epoch 971, Training Loss: 0.3525209426879883, Validation Loss: 0.34899136424064636\n",
      "Epoch 972, Training Loss: 0.352520614862442, Validation Loss: 0.34899136424064636\n",
      "Epoch 973, Training Loss: 0.3525203466415405, Validation Loss: 0.34899142384529114\n",
      "Epoch 974, Training Loss: 0.35252004861831665, Validation Loss: 0.34899142384529114\n",
      "Epoch 975, Training Loss: 0.35251978039741516, Validation Loss: 0.3489914834499359\n",
      "Epoch 976, Training Loss: 0.3525194823741913, Validation Loss: 0.3489914834499359\n",
      "Epoch 977, Training Loss: 0.3525191843509674, Validation Loss: 0.3489915430545807\n",
      "Epoch 978, Training Loss: 0.35251888632774353, Validation Loss: 0.3489915728569031\n",
      "Epoch 979, Training Loss: 0.35251861810684204, Validation Loss: 0.3489915728569031\n",
      "Epoch 980, Training Loss: 0.3525182902812958, Validation Loss: 0.34899163246154785\n",
      "Epoch 981, Training Loss: 0.3525180220603943, Validation Loss: 0.34899166226387024\n",
      "Epoch 982, Training Loss: 0.3525177240371704, Validation Loss: 0.34899166226387024\n",
      "Epoch 983, Training Loss: 0.35251742601394653, Validation Loss: 0.3489916920661926\n",
      "Epoch 984, Training Loss: 0.35251715779304504, Validation Loss: 0.348991721868515\n",
      "Epoch 985, Training Loss: 0.35251688957214355, Validation Loss: 0.348991721868515\n",
      "Epoch 986, Training Loss: 0.3525165617465973, Validation Loss: 0.348991721868515\n",
      "Epoch 987, Training Loss: 0.3525162935256958, Validation Loss: 0.3489917814731598\n",
      "Epoch 988, Training Loss: 0.3525159955024719, Validation Loss: 0.3489917814731598\n",
      "Epoch 989, Training Loss: 0.35251569747924805, Validation Loss: 0.3489917814731598\n",
      "Epoch 990, Training Loss: 0.35251539945602417, Validation Loss: 0.3489918112754822\n",
      "Epoch 991, Training Loss: 0.3525151312351227, Validation Loss: 0.34899184107780457\n",
      "Epoch 992, Training Loss: 0.3525148630142212, Validation Loss: 0.34899184107780457\n",
      "Epoch 993, Training Loss: 0.3525145649909973, Validation Loss: 0.34899184107780457\n",
      "Epoch 994, Training Loss: 0.35251426696777344, Validation Loss: 0.34899187088012695\n",
      "Epoch 995, Training Loss: 0.35251399874687195, Validation Loss: 0.34899187088012695\n",
      "Epoch 996, Training Loss: 0.35251370072364807, Validation Loss: 0.34899190068244934\n",
      "Epoch 997, Training Loss: 0.3525134325027466, Validation Loss: 0.34899190068244934\n",
      "Epoch 998, Training Loss: 0.3525131344795227, Validation Loss: 0.34899193048477173\n",
      "Epoch 999, Training Loss: 0.3525128662586212, Validation Loss: 0.3489919602870941\n",
      "Epoch 1000, Training Loss: 0.3525125980377197, Validation Loss: 0.3489919602870941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(pdlmodel.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pdlmodel.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    outputs = pdlmodel(user_features, all_x_included_products, all_x_other_products,all_bundle_prices)\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    pdlmodel.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = pdlmodel(X_user_val,  all_x_included_products, all_x_other_products ,all_bundle_prices)\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b5374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for complementarity model\n",
    "def calculate_expected_revenue(model, user_features, all_x_included_products, bundle_prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Calculate utilities for all bundles\n",
    "        utilities = model(user_features, all_x_included_products, all_x_other_products,bundle_prices)\n",
    "        probabilities = F.softmax(utilities, dim=1)\n",
    "\n",
    "        # Calculate total expected revenue\n",
    "        total_expected_revenue = (probabilities * bundle_prices.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c915a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $13088.15\n",
      "Expected Revenue all treated: $2650.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-10437.966796875"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data_bundle(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products,all_bundle_prices = control_prepared_data\n",
    "expected_revenue_all_control = calculate_expected_revenue(pdlmodel, user_features, all_x_included_products, all_bundle_prices, )\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data_bundle(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(pdlmodel, user_features, all_x_included_products, all_bundle_prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")\n",
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17cb1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = (expected_revenue_all_treated-expected_revenue_all_control)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a4df632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of PDL:  -0.42%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(pdl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c900cf",
   "metadata": {
    "id": "63c900cf"
   },
   "source": [
    "# use dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d442cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa2df020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityEstimator(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(UtilityEstimator, self).__init__()\n",
    "        \n",
    "        # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, 1)\n",
    "        )\n",
    "\n",
    "        self.theta0 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim +product_feature_dim*(NUM_Product)+1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "       \n",
    "        self.theta1 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + product_feature_dim*(NUM_Product)+1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_user, x_product, x_other_products,price):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features_theta =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                               x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                               aggregated_other_features.unsqueeze(0).expand(N, -1, -1)),\n",
    "                                                 dim=2)\n",
    "        theta0_output = self.theta0(combined_features_theta).squeeze(-1)\n",
    "        theta1_output = self.theta1(combined_features_theta).squeeze(-1)\n",
    "        \n",
    "        price = price.unsqueeze(-1)  \n",
    "        utility = theta0_output + theta1_output * price.squeeze(-1)\n",
    "\n",
    "\n",
    "        \n",
    "        return utility,theta0_output,theta1_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf77eeb6",
   "metadata": {
    "id": "cf77eeb6"
   },
   "outputs": [],
   "source": [
    "dml_model = UtilityEstimator(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "QT_wrrh3rIws",
   "metadata": {
    "id": "QT_wrrh3rIws"
   },
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31b522ed-0c36-4199-a309-74f71aece365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_bundle(user_features, product_features, prices):\n",
    "    # distinct device reference\n",
    "    device = product_features.device\n",
    "    \n",
    "    num_products = product_features.shape[0]\n",
    "    num_bundles = 2 ** num_products\n",
    "    bundle_choices = torch.tensor(\n",
    "        [[int(bit) for bit in np.binary_repr(i, width=num_products)] for i in range(num_bundles)],\n",
    "        dtype=torch.bool,\n",
    "        device=device \n",
    "    )\n",
    "    \n",
    "    # Calculate bundle prices\n",
    "    bundle_prices = torch.tensor([prices[bundle_mask].sum() for bundle_mask in bundle_choices], device=device)\n",
    "\n",
    "    # Initialize lists\n",
    "    all_x_included_products = []\n",
    "    all_x_other_products = []\n",
    "    all_bundle_prices = []\n",
    "    \n",
    "    # Iterate through each bundle\n",
    "    for i, bundle_mask in enumerate(bundle_choices):\n",
    "        # Get included product indices\n",
    "        included_indices = torch.where(bundle_mask)[0]\n",
    "        excluded_indices = torch.where(~bundle_mask)[0]\n",
    "\n",
    "        if included_indices.nelement() > 0:\n",
    "            included_products = product_features[included_indices].reshape(-1)\n",
    "        else:\n",
    "            included_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Features of excluded products\n",
    "        if excluded_indices.nelement() > 0:\n",
    "            other_products = product_features[excluded_indices].reshape(-1)\n",
    "        else:\n",
    "            other_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Price of the current bundle\n",
    "        current_bundle_price = bundle_prices[i]\n",
    "\n",
    "        # Append to lists\n",
    "        all_x_included_products.append(included_products)\n",
    "        all_x_other_products.append(other_products)\n",
    "        all_bundle_prices.append(current_bundle_price)\n",
    "        \n",
    "\n",
    "    max_included_len = max([x.numel() for x in all_x_included_products])\n",
    "    max_other_len = max([x.numel() for x in all_x_other_products])\n",
    "\n",
    "  \n",
    "    all_x_included_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_included_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_included_products\n",
    "    ])\n",
    "\n",
    "    all_x_other_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_other_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_other_products\n",
    "    ])\n",
    "    \n",
    "    all_bundle_prices = torch.stack(all_bundle_prices)\n",
    "\n",
    "    return user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0ff32e5-5c64-49ad-bbf0-f6aa2a53d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)\n",
    "X_user_train1 = X_user_train1.to(device)\n",
    "#for complementarity model\n",
    "prepared_data = prepare_data_bundle(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices= prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b386e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b386e12",
    "outputId": "aab71afc-5217-4c73-eba6-26cb31bed335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.6987287998199463, Validation Loss: 2.628040075302124\n",
      "Epoch 2, Training Loss: 2.628347158432007, Validation Loss: 2.5478994846343994\n",
      "Epoch 3, Training Loss: 2.548065662384033, Validation Loss: 2.456634998321533\n",
      "Epoch 4, Training Loss: 2.456768751144409, Validation Loss: 2.353860378265381\n",
      "Epoch 5, Training Loss: 2.353990077972412, Validation Loss: 2.2380032539367676\n",
      "Epoch 6, Training Loss: 2.2382149696350098, Validation Loss: 2.1090176105499268\n",
      "Epoch 7, Training Loss: 2.1092326641082764, Validation Loss: 1.9679954051971436\n",
      "Epoch 8, Training Loss: 1.9681450128555298, Validation Loss: 1.8174197673797607\n",
      "Epoch 9, Training Loss: 1.8177021741867065, Validation Loss: 1.6592581272125244\n",
      "Epoch 10, Training Loss: 1.6593964099884033, Validation Loss: 1.4959105253219604\n",
      "Epoch 11, Training Loss: 1.4958596229553223, Validation Loss: 1.3351292610168457\n",
      "Epoch 12, Training Loss: 1.3349157571792603, Validation Loss: 1.1828086376190186\n",
      "Epoch 13, Training Loss: 1.1826839447021484, Validation Loss: 1.0445828437805176\n",
      "Epoch 14, Training Loss: 1.0443954467773438, Validation Loss: 0.9258860349655151\n",
      "Epoch 15, Training Loss: 0.9255296587944031, Validation Loss: 0.8310474157333374\n",
      "Epoch 16, Training Loss: 0.8305789232254028, Validation Loss: 0.762591540813446\n",
      "Epoch 17, Training Loss: 0.7619615793228149, Validation Loss: 0.7190492749214172\n",
      "Epoch 18, Training Loss: 0.7184018492698669, Validation Loss: 0.695171058177948\n",
      "Epoch 19, Training Loss: 0.6946176290512085, Validation Loss: 0.6839818954467773\n",
      "Epoch 20, Training Loss: 0.6834414005279541, Validation Loss: 0.678952157497406\n",
      "Epoch 21, Training Loss: 0.6783431768417358, Validation Loss: 0.675238311290741\n",
      "Epoch 22, Training Loss: 0.6746528148651123, Validation Loss: 0.6699212789535522\n",
      "Epoch 23, Training Loss: 0.669433057308197, Validation Loss: 0.6615734100341797\n",
      "Epoch 24, Training Loss: 0.6610639095306396, Validation Loss: 0.6491111516952515\n",
      "Epoch 25, Training Loss: 0.6487311124801636, Validation Loss: 0.6329683065414429\n",
      "Epoch 26, Training Loss: 0.6327833533287048, Validation Loss: 0.6137539148330688\n",
      "Epoch 27, Training Loss: 0.6138097643852234, Validation Loss: 0.5924643278121948\n",
      "Epoch 28, Training Loss: 0.5926880240440369, Validation Loss: 0.5705735087394714\n",
      "Epoch 29, Training Loss: 0.5708411335945129, Validation Loss: 0.5502536296844482\n",
      "Epoch 30, Training Loss: 0.5503266453742981, Validation Loss: 0.5326460003852844\n",
      "Epoch 31, Training Loss: 0.5320208668708801, Validation Loss: 0.5177303552627563\n",
      "Epoch 32, Training Loss: 0.517241358757019, Validation Loss: 0.5065111517906189\n",
      "Epoch 33, Training Loss: 0.5068607926368713, Validation Loss: 0.49780336022377014\n",
      "Epoch 34, Training Loss: 0.49849480390548706, Validation Loss: 0.4892774522304535\n",
      "Epoch 35, Training Loss: 0.49010375142097473, Validation Loss: 0.48009711503982544\n",
      "Epoch 36, Training Loss: 0.48100754618644714, Validation Loss: 0.4703246057033539\n",
      "Epoch 37, Training Loss: 0.47130274772644043, Validation Loss: 0.4601106643676758\n",
      "Epoch 38, Training Loss: 0.46114447712898254, Validation Loss: 0.44957777857780457\n",
      "Epoch 39, Training Loss: 0.45066165924072266, Validation Loss: 0.4389302432537079\n",
      "Epoch 40, Training Loss: 0.4400893747806549, Validation Loss: 0.4283972978591919\n",
      "Epoch 41, Training Loss: 0.4296503961086273, Validation Loss: 0.4182254374027252\n",
      "Epoch 42, Training Loss: 0.4195830523967743, Validation Loss: 0.40876755118370056\n",
      "Epoch 43, Training Loss: 0.41022250056266785, Validation Loss: 0.40028029680252075\n",
      "Epoch 44, Training Loss: 0.40183722972869873, Validation Loss: 0.39303621649742126\n",
      "Epoch 45, Training Loss: 0.3946931064128876, Validation Loss: 0.3873180150985718\n",
      "Epoch 46, Training Loss: 0.38907158374786377, Validation Loss: 0.38333892822265625\n",
      "Epoch 47, Training Loss: 0.3851601481437683, Validation Loss: 0.38093122839927673\n",
      "Epoch 48, Training Loss: 0.38281089067459106, Validation Loss: 0.3794534504413605\n",
      "Epoch 49, Training Loss: 0.3813866972923279, Validation Loss: 0.37816575169563293\n",
      "Epoch 50, Training Loss: 0.3801579773426056, Validation Loss: 0.3764594793319702\n",
      "Epoch 51, Training Loss: 0.3785867989063263, Validation Loss: 0.37409865856170654\n",
      "Epoch 52, Training Loss: 0.3764401078224182, Validation Loss: 0.3715817928314209\n",
      "Epoch 53, Training Loss: 0.3739407956600189, Validation Loss: 0.3692341446876526\n",
      "Epoch 54, Training Loss: 0.37130922079086304, Validation Loss: 0.3672804832458496\n",
      "Epoch 55, Training Loss: 0.36887651681900024, Validation Loss: 0.3658697307109833\n",
      "Epoch 56, Training Loss: 0.36679425835609436, Validation Loss: 0.36483994126319885\n",
      "Epoch 57, Training Loss: 0.36517438292503357, Validation Loss: 0.36458662152290344\n",
      "Epoch 58, Training Loss: 0.3643645942211151, Validation Loss: 0.3647623360157013\n",
      "Epoch 59, Training Loss: 0.3644389808177948, Validation Loss: 0.3647618889808655\n",
      "Epoch 60, Training Loss: 0.36481034755706787, Validation Loss: 0.36419478058815\n",
      "Epoch 61, Training Loss: 0.36461442708969116, Validation Loss: 0.3629578948020935\n",
      "Epoch 62, Training Loss: 0.36351650953292847, Validation Loss: 0.36129096150398254\n",
      "Epoch 63, Training Loss: 0.361836701631546, Validation Loss: 0.35963794589042664\n",
      "Epoch 64, Training Loss: 0.3602186441421509, Validation Loss: 0.35848042368888855\n",
      "Epoch 65, Training Loss: 0.35924002528190613, Validation Loss: 0.3579161465167999\n",
      "Epoch 66, Training Loss: 0.35888540744781494, Validation Loss: 0.35753411054611206\n",
      "Epoch 67, Training Loss: 0.3586808443069458, Validation Loss: 0.3569367229938507\n",
      "Epoch 68, Training Loss: 0.358209490776062, Validation Loss: 0.35611918568611145\n",
      "Epoch 69, Training Loss: 0.35727229714393616, Validation Loss: 0.35519635677337646\n",
      "Epoch 70, Training Loss: 0.3560786843299866, Validation Loss: 0.3543911874294281\n",
      "Epoch 71, Training Loss: 0.35523152351379395, Validation Loss: 0.35394948720932007\n",
      "Epoch 72, Training Loss: 0.35508477687835693, Validation Loss: 0.353893518447876\n",
      "Epoch 73, Training Loss: 0.3552025258541107, Validation Loss: 0.3536430299282074\n",
      "Epoch 74, Training Loss: 0.354900598526001, Validation Loss: 0.3529770076274872\n",
      "Epoch 75, Training Loss: 0.3541150689125061, Validation Loss: 0.35237815976142883\n",
      "Epoch 76, Training Loss: 0.3534102439880371, Validation Loss: 0.35232388973236084\n",
      "Epoch 77, Training Loss: 0.35325413942337036, Validation Loss: 0.3524931073188782\n",
      "Epoch 78, Training Loss: 0.35337886214256287, Validation Loss: 0.3524234890937805\n",
      "Epoch 79, Training Loss: 0.3532940149307251, Validation Loss: 0.3520413637161255\n",
      "Epoch 80, Training Loss: 0.35287755727767944, Validation Loss: 0.3517743945121765\n",
      "Epoch 81, Training Loss: 0.35252588987350464, Validation Loss: 0.35198286175727844\n",
      "Epoch 82, Training Loss: 0.3525240123271942, Validation Loss: 0.35216400027275085\n",
      "Epoch 83, Training Loss: 0.35261768102645874, Validation Loss: 0.35201770067214966\n",
      "Epoch 84, Training Loss: 0.3525184094905853, Validation Loss: 0.3516237437725067\n",
      "Epoch 85, Training Loss: 0.3522547781467438, Validation Loss: 0.3512474000453949\n",
      "Epoch 86, Training Loss: 0.3520817160606384, Validation Loss: 0.35106274485588074\n",
      "Epoch 87, Training Loss: 0.35208046436309814, Validation Loss: 0.3509613871574402\n",
      "Epoch 88, Training Loss: 0.352103590965271, Validation Loss: 0.3507358431816101\n",
      "Epoch 89, Training Loss: 0.3519766628742218, Validation Loss: 0.3504630923271179\n",
      "Epoch 90, Training Loss: 0.3518006205558777, Validation Loss: 0.350274920463562\n",
      "Epoch 91, Training Loss: 0.3517206609249115, Validation Loss: 0.35016801953315735\n",
      "Epoch 92, Training Loss: 0.3517594635486603, Validation Loss: 0.3499893844127655\n",
      "Epoch 93, Training Loss: 0.35172876715660095, Validation Loss: 0.34973159432411194\n",
      "Epoch 94, Training Loss: 0.3515978157520294, Validation Loss: 0.3495354652404785\n",
      "Epoch 95, Training Loss: 0.35151588916778564, Validation Loss: 0.34944862127304077\n",
      "Epoch 96, Training Loss: 0.35154780745506287, Validation Loss: 0.3493734896183014\n",
      "Epoch 97, Training Loss: 0.3515620529651642, Validation Loss: 0.3492518365383148\n",
      "Epoch 98, Training Loss: 0.3514891564846039, Validation Loss: 0.349144846200943\n",
      "Epoch 99, Training Loss: 0.35141536593437195, Validation Loss: 0.34909287095069885\n",
      "Epoch 100, Training Loss: 0.3514257073402405, Validation Loss: 0.3490488827228546\n",
      "Epoch 101, Training Loss: 0.351435124874115, Validation Loss: 0.348994642496109\n",
      "Epoch 102, Training Loss: 0.35138773918151855, Validation Loss: 0.3489757180213928\n",
      "Epoch 103, Training Loss: 0.3513360023498535, Validation Loss: 0.34902241826057434\n",
      "Epoch 104, Training Loss: 0.35133278369903564, Validation Loss: 0.3490450084209442\n",
      "Epoch 105, Training Loss: 0.3513319790363312, Validation Loss: 0.34900662302970886\n",
      "Epoch 106, Training Loss: 0.35129183530807495, Validation Loss: 0.3489491641521454\n",
      "Epoch 107, Training Loss: 0.3512466847896576, Validation Loss: 0.34893032908439636\n",
      "Epoch 108, Training Loss: 0.35122570395469666, Validation Loss: 0.3489499092102051\n",
      "Epoch 109, Training Loss: 0.35121214389801025, Validation Loss: 0.3489675521850586\n",
      "Epoch 110, Training Loss: 0.3511722981929779, Validation Loss: 0.34900546073913574\n",
      "Epoch 111, Training Loss: 0.35113194584846497, Validation Loss: 0.34907060861587524\n",
      "Epoch 112, Training Loss: 0.3511222004890442, Validation Loss: 0.34911036491394043\n",
      "Epoch 113, Training Loss: 0.3511112928390503, Validation Loss: 0.3491101562976837\n",
      "Epoch 114, Training Loss: 0.3510829508304596, Validation Loss: 0.349103718996048\n",
      "Epoch 115, Training Loss: 0.3510638177394867, Validation Loss: 0.3491121530532837\n",
      "Epoch 116, Training Loss: 0.35106298327445984, Validation Loss: 0.349123477935791\n",
      "Epoch 117, Training Loss: 0.35105443000793457, Validation Loss: 0.3491414487361908\n",
      "Epoch 118, Training Loss: 0.3510357141494751, Validation Loss: 0.3491668403148651\n",
      "Epoch 119, Training Loss: 0.3510236144065857, Validation Loss: 0.34918782114982605\n",
      "Epoch 120, Training Loss: 0.3510169982910156, Validation Loss: 0.3491901457309723\n",
      "Epoch 121, Training Loss: 0.35100245475769043, Validation Loss: 0.34917324781417847\n",
      "Epoch 122, Training Loss: 0.35098445415496826, Validation Loss: 0.3491526246070862\n",
      "Epoch 123, Training Loss: 0.3509739637374878, Validation Loss: 0.349125474691391\n",
      "Epoch 124, Training Loss: 0.3509654998779297, Validation Loss: 0.34909358620643616\n",
      "Epoch 125, Training Loss: 0.3509514033794403, Validation Loss: 0.3490656614303589\n",
      "Epoch 126, Training Loss: 0.3509378433227539, Validation Loss: 0.3490336835384369\n",
      "Epoch 127, Training Loss: 0.3509264290332794, Validation Loss: 0.34898653626441956\n",
      "Epoch 128, Training Loss: 0.3509134352207184, Validation Loss: 0.34892603754997253\n",
      "Epoch 129, Training Loss: 0.3508972227573395, Validation Loss: 0.34886011481285095\n",
      "Epoch 130, Training Loss: 0.3508816659450531, Validation Loss: 0.3488047420978546\n",
      "Epoch 131, Training Loss: 0.3508714437484741, Validation Loss: 0.34876716136932373\n",
      "Epoch 132, Training Loss: 0.3508625328540802, Validation Loss: 0.34874457120895386\n",
      "Epoch 133, Training Loss: 0.350852370262146, Validation Loss: 0.34874260425567627\n",
      "Epoch 134, Training Loss: 0.35084256529808044, Validation Loss: 0.34875476360321045\n",
      "Epoch 135, Training Loss: 0.3508319854736328, Validation Loss: 0.34877675771713257\n",
      "Epoch 136, Training Loss: 0.3508186936378479, Validation Loss: 0.3488059639930725\n",
      "Epoch 137, Training Loss: 0.3508070707321167, Validation Loss: 0.3488445281982422\n",
      "Epoch 138, Training Loss: 0.3507968783378601, Validation Loss: 0.348882257938385\n",
      "Epoch 139, Training Loss: 0.3507877588272095, Validation Loss: 0.3489115834236145\n",
      "Epoch 140, Training Loss: 0.35077929496765137, Validation Loss: 0.3489251434803009\n",
      "Epoch 141, Training Loss: 0.35077038407325745, Validation Loss: 0.34892523288726807\n",
      "Epoch 142, Training Loss: 0.3507609963417053, Validation Loss: 0.3489112854003906\n",
      "Epoch 143, Training Loss: 0.350750207901001, Validation Loss: 0.34888574481010437\n",
      "Epoch 144, Training Loss: 0.35073933005332947, Validation Loss: 0.3488498330116272\n",
      "Epoch 145, Training Loss: 0.3507287800312042, Validation Loss: 0.34880977869033813\n",
      "Epoch 146, Training Loss: 0.35071811079978943, Validation Loss: 0.34877461194992065\n",
      "Epoch 147, Training Loss: 0.35070914030075073, Validation Loss: 0.34874796867370605\n",
      "Epoch 148, Training Loss: 0.3506997525691986, Validation Loss: 0.34872880578041077\n",
      "Epoch 149, Training Loss: 0.35069021582603455, Validation Loss: 0.3487115204334259\n",
      "Epoch 150, Training Loss: 0.3506799042224884, Validation Loss: 0.34869974851608276\n",
      "Epoch 151, Training Loss: 0.350671648979187, Validation Loss: 0.34869301319122314\n",
      "Epoch 152, Training Loss: 0.35065868496894836, Validation Loss: 0.3486888110637665\n",
      "Epoch 153, Training Loss: 0.3506484031677246, Validation Loss: 0.3486913740634918\n",
      "Epoch 154, Training Loss: 0.3506386876106262, Validation Loss: 0.3486936092376709\n",
      "Epoch 155, Training Loss: 0.3506280481815338, Validation Loss: 0.3486982583999634\n",
      "Epoch 156, Training Loss: 0.35061660408973694, Validation Loss: 0.34870386123657227\n",
      "Epoch 157, Training Loss: 0.35060474276542664, Validation Loss: 0.3487129807472229\n",
      "Epoch 158, Training Loss: 0.3505924642086029, Validation Loss: 0.3487188220024109\n",
      "Epoch 159, Training Loss: 0.3505796492099762, Validation Loss: 0.3487204313278198\n",
      "Epoch 160, Training Loss: 0.350566565990448, Validation Loss: 0.3487159013748169\n",
      "Epoch 161, Training Loss: 0.3505530059337616, Validation Loss: 0.3487064838409424\n",
      "Epoch 162, Training Loss: 0.35053911805152893, Validation Loss: 0.3486931324005127\n",
      "Epoch 163, Training Loss: 0.35052525997161865, Validation Loss: 0.3486815392971039\n",
      "Epoch 164, Training Loss: 0.35051390528678894, Validation Loss: 0.34866762161254883\n",
      "Epoch 165, Training Loss: 0.3504978120326996, Validation Loss: 0.34864962100982666\n",
      "Epoch 166, Training Loss: 0.3504839241504669, Validation Loss: 0.34863123297691345\n",
      "Epoch 167, Training Loss: 0.3504696488380432, Validation Loss: 0.34861287474632263\n",
      "Epoch 168, Training Loss: 0.35045456886291504, Validation Loss: 0.3485877811908722\n",
      "Epoch 169, Training Loss: 0.35043859481811523, Validation Loss: 0.3485560417175293\n",
      "Epoch 170, Training Loss: 0.3504234254360199, Validation Loss: 0.34851405024528503\n",
      "Epoch 171, Training Loss: 0.35040777921676636, Validation Loss: 0.34846797585487366\n",
      "Epoch 172, Training Loss: 0.35039275884628296, Validation Loss: 0.3484295904636383\n",
      "Epoch 173, Training Loss: 0.35037854313850403, Validation Loss: 0.3483969271183014\n",
      "Epoch 174, Training Loss: 0.3503630757331848, Validation Loss: 0.34835782647132874\n",
      "Epoch 175, Training Loss: 0.35034623742103577, Validation Loss: 0.3483004868030548\n",
      "Epoch 176, Training Loss: 0.3503280282020569, Validation Loss: 0.34823694825172424\n",
      "Epoch 177, Training Loss: 0.35031434893608093, Validation Loss: 0.3481817841529846\n",
      "Epoch 178, Training Loss: 0.35029518604278564, Validation Loss: 0.34815138578414917\n",
      "Epoch 179, Training Loss: 0.3502819538116455, Validation Loss: 0.3481459319591522\n",
      "Epoch 180, Training Loss: 0.35026824474334717, Validation Loss: 0.34817078709602356\n",
      "Epoch 181, Training Loss: 0.35025104880332947, Validation Loss: 0.3482239544391632\n",
      "Epoch 182, Training Loss: 0.3502328395843506, Validation Loss: 0.3482893407344818\n",
      "Epoch 183, Training Loss: 0.35021618008613586, Validation Loss: 0.34834811091423035\n",
      "Epoch 184, Training Loss: 0.3502010703086853, Validation Loss: 0.34838977456092834\n",
      "Epoch 185, Training Loss: 0.35018661618232727, Validation Loss: 0.34840822219848633\n",
      "Epoch 186, Training Loss: 0.3501713275909424, Validation Loss: 0.34841156005859375\n",
      "Epoch 187, Training Loss: 0.35015538334846497, Validation Loss: 0.3483993709087372\n",
      "Epoch 188, Training Loss: 0.35013872385025024, Validation Loss: 0.3483786880970001\n",
      "Epoch 189, Training Loss: 0.3501225709915161, Validation Loss: 0.34835758805274963\n",
      "Epoch 190, Training Loss: 0.35011088848114014, Validation Loss: 0.3483380973339081\n",
      "Epoch 191, Training Loss: 0.35009366273880005, Validation Loss: 0.348321795463562\n",
      "Epoch 192, Training Loss: 0.35008057951927185, Validation Loss: 0.34831562638282776\n",
      "Epoch 193, Training Loss: 0.3500676453113556, Validation Loss: 0.34831875562667847\n",
      "Epoch 194, Training Loss: 0.35005366802215576, Validation Loss: 0.34832656383514404\n",
      "Epoch 195, Training Loss: 0.35003775358200073, Validation Loss: 0.3483375906944275\n",
      "Epoch 196, Training Loss: 0.3500216007232666, Validation Loss: 0.3483532965183258\n",
      "Epoch 197, Training Loss: 0.3500053286552429, Validation Loss: 0.34836849570274353\n",
      "Epoch 198, Training Loss: 0.3499892055988312, Validation Loss: 0.34838125109672546\n",
      "Epoch 199, Training Loss: 0.34997299313545227, Validation Loss: 0.3483929932117462\n",
      "Epoch 200, Training Loss: 0.34995681047439575, Validation Loss: 0.34840455651283264\n",
      "Epoch 201, Training Loss: 0.3499404191970825, Validation Loss: 0.34841597080230713\n",
      "Epoch 202, Training Loss: 0.3499239385128021, Validation Loss: 0.3484247922897339\n",
      "Epoch 203, Training Loss: 0.3499131500720978, Validation Loss: 0.348432332277298\n",
      "Epoch 204, Training Loss: 0.3498941957950592, Validation Loss: 0.34843510389328003\n",
      "Epoch 205, Training Loss: 0.34988078474998474, Validation Loss: 0.348431259393692\n",
      "Epoch 206, Training Loss: 0.3498672544956207, Validation Loss: 0.34843093156814575\n",
      "Epoch 207, Training Loss: 0.3498537540435791, Validation Loss: 0.34843477606773376\n",
      "Epoch 208, Training Loss: 0.3498399257659912, Validation Loss: 0.34843921661376953\n",
      "Epoch 209, Training Loss: 0.3498259484767914, Validation Loss: 0.3484411835670471\n",
      "Epoch 210, Training Loss: 0.34981170296669006, Validation Loss: 0.34843915700912476\n",
      "Epoch 211, Training Loss: 0.3497971296310425, Validation Loss: 0.34843263030052185\n",
      "Epoch 212, Training Loss: 0.3497820794582367, Validation Loss: 0.34842145442962646\n",
      "Epoch 213, Training Loss: 0.3497665524482727, Validation Loss: 0.3484121561050415\n",
      "Epoch 214, Training Loss: 0.34975090622901917, Validation Loss: 0.3484055995941162\n",
      "Epoch 215, Training Loss: 0.3497467339038849, Validation Loss: 0.3484087586402893\n",
      "Epoch 216, Training Loss: 0.349723756313324, Validation Loss: 0.3484096825122833\n",
      "Epoch 217, Training Loss: 0.3497120141983032, Validation Loss: 0.34841376543045044\n",
      "Epoch 218, Training Loss: 0.34969964623451233, Validation Loss: 0.3484136462211609\n",
      "Epoch 219, Training Loss: 0.34968680143356323, Validation Loss: 0.3484070301055908\n",
      "Epoch 220, Training Loss: 0.34967291355133057, Validation Loss: 0.34839919209480286\n",
      "Epoch 221, Training Loss: 0.34965890645980835, Validation Loss: 0.3483966588973999\n",
      "Epoch 222, Training Loss: 0.34964415431022644, Validation Loss: 0.3483930826187134\n",
      "Epoch 223, Training Loss: 0.3496290147304535, Validation Loss: 0.3483854830265045\n",
      "Epoch 224, Training Loss: 0.3496130406856537, Validation Loss: 0.34837475419044495\n",
      "Epoch 225, Training Loss: 0.34960663318634033, Validation Loss: 0.3483722507953644\n",
      "Epoch 226, Training Loss: 0.34958669543266296, Validation Loss: 0.34837010502815247\n",
      "Epoch 227, Training Loss: 0.3495761454105377, Validation Loss: 0.34837105870246887\n",
      "Epoch 228, Training Loss: 0.34956493973731995, Validation Loss: 0.34837567806243896\n",
      "Epoch 229, Training Loss: 0.34955257177352905, Validation Loss: 0.3483918607234955\n",
      "Epoch 230, Training Loss: 0.3495391607284546, Validation Loss: 0.34841394424438477\n",
      "Epoch 231, Training Loss: 0.3495252728462219, Validation Loss: 0.3484308421611786\n",
      "Epoch 232, Training Loss: 0.3495103716850281, Validation Loss: 0.34844183921813965\n",
      "Epoch 233, Training Loss: 0.34949490427970886, Validation Loss: 0.3484308421611786\n",
      "Epoch 234, Training Loss: 0.34948834776878357, Validation Loss: 0.34842583537101746\n",
      "Epoch 235, Training Loss: 0.3494691848754883, Validation Loss: 0.3484273850917816\n",
      "Epoch 236, Training Loss: 0.3494599461555481, Validation Loss: 0.3484273850917816\n",
      "Epoch 237, Training Loss: 0.34944918751716614, Validation Loss: 0.3484264016151428\n",
      "Epoch 238, Training Loss: 0.34943705797195435, Validation Loss: 0.3484315276145935\n",
      "Epoch 239, Training Loss: 0.34942343831062317, Validation Loss: 0.3484457731246948\n",
      "Epoch 240, Training Loss: 0.349408358335495, Validation Loss: 0.34846022725105286\n",
      "Epoch 241, Training Loss: 0.3493918180465698, Validation Loss: 0.34847310185432434\n",
      "Epoch 242, Training Loss: 0.34939664602279663, Validation Loss: 0.3484937846660614\n",
      "Epoch 243, Training Loss: 0.34936654567718506, Validation Loss: 0.34851589798927307\n",
      "Epoch 244, Training Loss: 0.3493569493293762, Validation Loss: 0.3485444486141205\n",
      "Epoch 245, Training Loss: 0.34934550523757935, Validation Loss: 0.3485690951347351\n",
      "Epoch 246, Training Loss: 0.3493320941925049, Validation Loss: 0.3485877811908722\n",
      "Epoch 247, Training Loss: 0.34931716322898865, Validation Loss: 0.34859564900398254\n",
      "Epoch 248, Training Loss: 0.34930095076560974, Validation Loss: 0.34860920906066895\n",
      "Epoch 249, Training Loss: 0.349317342042923, Validation Loss: 0.3486206829547882\n",
      "Epoch 250, Training Loss: 0.3492765426635742, Validation Loss: 0.34863319993019104\n",
      "Epoch 251, Training Loss: 0.34926801919937134, Validation Loss: 0.3486478328704834\n",
      "Epoch 252, Training Loss: 0.34925633668899536, Validation Loss: 0.34866514801979065\n",
      "Epoch 253, Training Loss: 0.3492421507835388, Validation Loss: 0.34868741035461426\n",
      "Epoch 254, Training Loss: 0.3492254316806793, Validation Loss: 0.3487086296081543\n",
      "Epoch 255, Training Loss: 0.34923747181892395, Validation Loss: 0.34873515367507935\n",
      "Epoch 256, Training Loss: 0.3492010235786438, Validation Loss: 0.34874996542930603\n",
      "Epoch 257, Training Loss: 0.3491930067539215, Validation Loss: 0.3487643301486969\n",
      "Epoch 258, Training Loss: 0.34918129444122314, Validation Loss: 0.348782479763031\n",
      "Epoch 259, Training Loss: 0.3491668999195099, Validation Loss: 0.3487919569015503\n",
      "Epoch 260, Training Loss: 0.349148690700531, Validation Loss: 0.3487996459007263\n",
      "Epoch 261, Training Loss: 0.34917008876800537, Validation Loss: 0.348831444978714\n",
      "Epoch 262, Training Loss: 0.3491227924823761, Validation Loss: 0.3488730490207672\n",
      "Epoch 263, Training Loss: 0.34912818670272827, Validation Loss: 0.34889188408851624\n",
      "Epoch 264, Training Loss: 0.34913212060928345, Validation Loss: 0.348924458026886\n",
      "Epoch 265, Training Loss: 0.34912964701652527, Validation Loss: 0.34895405173301697\n",
      "Epoch 266, Training Loss: 0.3491216003894806, Validation Loss: 0.348982036113739\n",
      "Epoch 267, Training Loss: 0.34910932183265686, Validation Loss: 0.3489912748336792\n",
      "Epoch 268, Training Loss: 0.34909379482269287, Validation Loss: 0.34898456931114197\n",
      "Epoch 269, Training Loss: 0.3490752875804901, Validation Loss: 0.3489735722541809\n",
      "Epoch 270, Training Loss: 0.3490549921989441, Validation Loss: 0.348966509103775\n",
      "Epoch 271, Training Loss: 0.34903255105018616, Validation Loss: 0.3489682972431183\n",
      "Epoch 272, Training Loss: 0.3490082621574402, Validation Loss: 0.348967969417572\n",
      "Epoch 273, Training Loss: 0.34898868203163147, Validation Loss: 0.34898123145103455\n",
      "Epoch 274, Training Loss: 0.3489746153354645, Validation Loss: 0.3489825427532196\n",
      "Epoch 275, Training Loss: 0.3489612638950348, Validation Loss: 0.3489769995212555\n",
      "Epoch 276, Training Loss: 0.34895333647727966, Validation Loss: 0.34898141026496887\n",
      "Epoch 277, Training Loss: 0.3489430844783783, Validation Loss: 0.3489772379398346\n",
      "Epoch 278, Training Loss: 0.34894147515296936, Validation Loss: 0.3490046560764313\n",
      "Epoch 279, Training Loss: 0.34893155097961426, Validation Loss: 0.3490482270717621\n",
      "Epoch 280, Training Loss: 0.3489198684692383, Validation Loss: 0.34906867146492004\n",
      "Epoch 281, Training Loss: 0.348900705575943, Validation Loss: 0.3490979075431824\n",
      "Epoch 282, Training Loss: 0.3488788604736328, Validation Loss: 0.34912335872650146\n",
      "Epoch 283, Training Loss: 0.3488667607307434, Validation Loss: 0.3491482734680176\n",
      "Epoch 284, Training Loss: 0.3488481938838959, Validation Loss: 0.34915390610694885\n",
      "Epoch 285, Training Loss: 0.34883826971054077, Validation Loss: 0.3491564989089966\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(dml_model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dml_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    outputs = dml_model(user_features, all_x_included_products, all_x_other_products,all_bundle_prices)[0]\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    dml_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dml_model(X_user_val,  all_x_included_products, all_x_other_products ,all_bundle_prices)[0]\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b27e55e",
   "metadata": {
    "id": "1b27e55e"
   },
   "outputs": [],
   "source": [
    "#for complementarity model\n",
    "def calculate_expected_revenue(model, user_features, all_x_included_products, bundle_prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Calculate utilities for all bundles\n",
    "        utilities = model(user_features, all_x_included_products, all_x_other_products,bundle_prices)[0]\n",
    "        probabilities = F.softmax(utilities, dim=1)\n",
    "\n",
    "        # Calculate total expected revenue\n",
    "        total_expected_revenue = (probabilities * bundle_prices.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5efd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $13086.84\n",
      "Expected Revenue all treated: $2433.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-10653.0048828125"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data_bundle(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = control_prepared_data\n",
    "expected_revenue_all_control = calculate_expected_revenue(dml_model, user_features, all_x_included_products, all_bundle_prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data_bundle(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(dml_model, user_features, all_x_included_products, all_bundle_prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")\n",
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "QGABODM51OV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGABODM51OV4",
    "outputId": "3769a33f-2282-4787-e276-3f7d3b56c540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10653.0048828125"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121cd47",
   "metadata": {
    "id": "1121cd47"
   },
   "source": [
    "# debias the GTE estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "SMTzkngzuUls",
   "metadata": {
    "id": "SMTzkngzuUls"
   },
   "outputs": [],
   "source": [
    "test_prepared_data = prepare_data_bundle(X_user_test, X_product,  price*(1-(1-discount)*prod_randomization))\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = test_prepared_data\n",
    "\n",
    "# Compute Theta0 and Theta1\n",
    "_,theta0_output,theta1_output = dml_model(user_features, all_x_included_products, all_x_other_products,all_bundle_prices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rag3u55FWjiu",
   "metadata": {
    "id": "Rag3u55FWjiu"
   },
   "source": [
    "# use formulation debias for H_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "SSXrdFP4WnZL",
   "metadata": {
    "id": "SSXrdFP4WnZL"
   },
   "outputs": [],
   "source": [
    "def H_theta(theta0_output,theta1_output,all_treated_price,price):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = 2**NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = all_treated_price.unsqueeze(0).expand(N, M)\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_treated_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    all_treated_probabilities = F.softmax(all_treated_uti, dim=1)\n",
    "    all_control_probabilities = F.softmax(all_control_uti, dim=1)\n",
    "\n",
    "    price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "    treated_price_with_outside =  torch.cat((torch.zeros(1, device=all_treated_price.device),all_treated_price), dim=0)\n",
    "\n",
    "    H = torch.sum(all_treated_probabilities*treated_price_with_outside - all_control_probabilities*price_with_outside,dim=1)\n",
    "    expsum_treated = torch.sum(torch.exp(all_treated_uti),dim=1)\n",
    "    expsum_control = torch.sum(torch.exp(all_control_uti),dim=1)\n",
    "\n",
    "    expsum_treated_expanded = expsum_treated.unsqueeze(1).expand(-1, all_treated_uti.shape[1])  # Shape [N, M+1]\n",
    "    expsum_control_expanded = expsum_control.unsqueeze(1).expand(-1, all_control_uti.shape[1])  # Shape [N, M+1]\n",
    "\n",
    "    H_theta0 = torch.sum((torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded-\\\n",
    "                          torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded)\\\n",
    "                         *price_with_outside,dim=1)\n",
    "    H_theta1 = torch.sum(price_with_outside*(torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded*treated_price_with_outside-\\\n",
    "                                             torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded*price_with_outside),dim=1)\n",
    "\n",
    "\n",
    "    return H,H_theta0,H_theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "776a374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_treated_price = price*discount\n",
    "all_treated_bundle_price = torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    all_treated_bundle_price[b] = torch.sum(all_treated_price[bundle_mask.bool()])\n",
    "all_bundle_prices= torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    all_bundle_prices[b] = torch.sum(price[bundle_mask.bool()])\n",
    "H,H_theta0,H_theta1 = H_theta(theta0_output,theta1_output,all_treated_bundle_price,all_bundle_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "yivIC_MKad5x",
   "metadata": {
    "id": "yivIC_MKad5x"
   },
   "outputs": [],
   "source": [
    "def l_theta(theta0_output,theta1_output,adjusted_price,decision_test):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = 2**NUM_Product\n",
    "    expand_adjusted_price = adjusted_price.unsqueeze(0).expand(N, M)\n",
    "    uti = theta0_output + theta1_output * expand_adjusted_price\n",
    "    adjusted_price_with_outside =  torch.cat([torch.zeros(1, device=adjusted_price.device),adjusted_price])\n",
    "\n",
    "    probabilities = F.softmax(uti, dim=1)\n",
    "    prod_indices = torch.ones(2**NUM_Product, device=device)\n",
    "    ltheta0 = probabilities[torch.arange(decision_test.size(0)), decision_test] -prod_indices[decision_test]\n",
    "    ltheta1 = (probabilities[torch.arange(decision_test.size(0)), decision_test] * adjusted_price_with_outside[decision_test]) - adjusted_price_with_outside[decision_test]\n",
    "\n",
    "\n",
    "    return ltheta0,ltheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77bcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "O8c-tupIgHVu",
   "metadata": {
    "id": "O8c-tupIgHVu"
   },
   "outputs": [],
   "source": [
    "adjusted_price = price*(1-(1-discount)*prod_randomization).to(device)\n",
    "adjusted_bundle_price = torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    adjusted_bundle_price[b] = torch.sum(adjusted_price[bundle_mask.bool()])\n",
    "decision_test = decision_test.to(device)\n",
    "ltheta0,ltheta1= l_theta(theta0_output,theta1_output,adjusted_bundle_price,decision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UVRht78QaxSG",
   "metadata": {
    "id": "UVRht78QaxSG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lambdainv(theta0_output, theta1_output, price, decision_test,epsilon =10):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = 2**NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = discount*price.unsqueeze(0).expand(N, M)\n",
    "\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities_control = F.softmax(all_control_uti, dim=1)\n",
    "    probabilities_treated = F.softmax(all_treated_uti, dim=1)\n",
    "\n",
    "    # Extract probabilities of chosen products\n",
    "    chosen_prob_control = probabilities_control[torch.arange(N), decision_test]\n",
    "    chosen_prob_treated = probabilities_treated[torch.arange(N), decision_test]\n",
    "\n",
    "    # Calculate second derivatives\n",
    "    ltheta00 = chosen_prob_control * (1 - chosen_prob_control) + chosen_prob_treated * (1 - chosen_prob_treated)\n",
    "    ltheta01 = chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test] + \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])\n",
    "    ltheta11 = chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test]**2 + \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])**2\n",
    "    ltheta00=ltheta00/2\n",
    "    ltheta01=ltheta01/2\n",
    "    ltheta11=ltheta11/2\n",
    "\n",
    "    # Form the 2x2 Hessian matrices for each instance\n",
    "    ltheta00 = ltheta00.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta01 = ltheta01.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta11 = ltheta11.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    top_row = torch.cat((ltheta00, ltheta01), dim=2)\n",
    "    bottom_row = torch.cat((ltheta01, ltheta11), dim=2)\n",
    "\n",
    "    L_matrix = torch.cat((top_row, bottom_row), dim=1)\n",
    "\n",
    "    # Regularization and inversion\n",
    "\n",
    "    identity_matrix = torch.eye(2, dtype=L_matrix.dtype, device=L_matrix.device) * epsilon\n",
    "    L_matrix_reg = L_matrix + identity_matrix.unsqueeze(0).unsqueeze(0)\n",
    "    L_inv = torch.linalg.inv(L_matrix_reg)\n",
    "\n",
    "    return L_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6f7441c-eea7-4f98-8631-71cb75d14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.001,0.01,0.1,0.5,1,5,10]\n",
    "min_mape = float('inf')\n",
    "best_epsilon = None\n",
    "best_final_result = None\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    # Update L_inv for the current epsilon\n",
    "    try:\n",
    "        L_inv = lambdainv(theta0_output, theta1_output, all_bundle_prices, decision_test, epsilon).float()\n",
    "\n",
    "        # Calculate final_result with the given epsilon\n",
    "        H_theta_array = torch.stack((H_theta0, H_theta1), dim=-1).unsqueeze(1).float()\n",
    "        l_theta_array = torch.stack((ltheta0, ltheta1), dim=-1).unsqueeze(-1).float()\n",
    "\n",
    "        # Perform matrix multiplications\n",
    "        result_intermediate = torch.matmul(H_theta_array, L_inv.squeeze(0))\n",
    "        final_result = torch.matmul(result_intermediate, l_theta_array).squeeze(-1)\n",
    "        final_result[torch.isnan(final_result) | torch.isinf(final_result)] = 0\n",
    "\n",
    "        # Calculate sdl and dedl\n",
    "        sdl = H.sum().cpu().detach().numpy() * 2\n",
    "        dedl = (H.sum().cpu().detach().numpy() - final_result.sum().cpu().detach().numpy()) * 2\n",
    "\n",
    "        # Calculate MAPE of dedl with respect to true\n",
    "        mape_dedl = np.abs((dedl - true) / true)\n",
    "\n",
    "        # Update best_epsilon if the current epsilon yields a lower MAPE\n",
    "        if mape_dedl < min_mape:\n",
    "            min_mape = mape_dedl\n",
    "            best_epsilon = epsilon\n",
    "            best_final_result = final_result\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "q11HQu-goWM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11HQu-goWM0",
    "outputId": "ecf71629-8dbc-4d80-8d4a-de1f2534e3eb"
   },
   "outputs": [],
   "source": [
    "sdl = H.sum().cpu().detach().numpy()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3a445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedl = (H.sum().cpu().detach().numpy()-best_final_result.sum().cpu().detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c6d8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-21916.69140625, -20046.2109375, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl,dedl,best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d44fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of SDL:  -5.43%\n",
      "Absolute Percentage Estimation Error of SP MNL:  -3.57%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of SDL:  {100*np.abs(sdl-revenue_difference)/revenue_difference:.2f}%\")\n",
    "print(f\"Absolute Percentage Estimation Error of SP MNL:  {100*np.abs(dedl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eeea3d7d-d20e-48c8-b037-107fce171386",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pe = (naive - true) / true\n",
    "linear_pe = (linear - true) / true\n",
    "pdl_pe = (pdl - true) / true\n",
    "sdl_pe = (sdl - true) / true\n",
    "dedl_pe = (dedl - true) / true\n",
    "naive_mse = (naive - true)**2\n",
    "linear_mse =(linear - true)**2\n",
    "pdl_mse = (pdl - true)**2\n",
    "sdl_mse = (sdl - true)**2\n",
    "dedl_mse = (dedl - true)**2\n",
    "naive_e = (naive - true)\n",
    "linear_e =(linear - true)\n",
    "pdl_e = (pdl - true)\n",
    "sdl_e = (sdl - true)\n",
    "dedl_e = (dedl - true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1094fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5218808346824761 -1.0112543566720331 0.0042321133128392675 0.0542975349525513 -0.03568150936695984 -31636.792750656605 21021.91168397665 -87.97698789834976 -1128.7348003983498 741.7456683516502 1000886655.5479983 441920770.848914 7739.950399666381 1274042.2496303024 550186.6365184363\n"
     ]
    }
   ],
   "source": [
    "print(naive_pe,linear_pe,pdl_pe,sdl_pe,dedl_pe,naive_e,linear_e,pdl_e,sdl_e,dedl_e,naive_mse,linear_mse,pdl_mse,sdl_mse,dedl_mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "c2a5b9ca",
    "7e135e9e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
